[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 253: Statistical Machine Learning",
    "section": "",
    "text": "Welcome!\nThis is the course website for Statistical Machine Learning (STAT 253) at Macalester College for the Fall 2024 semester.\nThe site was built by Kelsey Grinde. It draws heavily upon our course textbook, the 2nd edition of An Introduction to Statistical Learning with Applications in R, as well as on materials prepared by fellow Macalester statistics faculty Brianna Heggeseth, Alicia Johnson, and Leslie Myint.\nThe structure of this site was inspired by an eCOTS 2024 workshop led by Devin Becker: see here for materials.\nNote that this page will be under active construction throughout the semester. If you find any typos or other issues, please click the Report an issue button above and/or email kgrinde@macalester.edu.\n \n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The schedule below is a tentative outline of our plans for the semester.\nBefore each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Goals.\nReadings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). The ISLR readings are highly encouraged and serve as a nice complement to the videos and in-class activities.\n\n\n\n\n\nWeek 1\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n9/3\nCANCELED\n\n\n\n\n\n\n9/5\nCANCELED\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n9/10\nUnit 0: Introductions & Overview\nBackground Survey\nISLR Reading: Chap 1 & Section 2.1 (Skip 2.1.2, 2.1.3 for now)\nIntroductions\n\nComplete CP1 (before next class)\nStart HW0 (9/13)\n\n\n\n9/12\nUnit 1: Model Evaluation\nConcept Video (script)\nCheckpoint 1\nR Tutorial Video (code)\nISLR: Section 2.2 (skip 2.2.3 for now.), Section 3.1\nEvaluating Regression Models\n(QMD)\nComplete CP2\nFinish HW0\nConcept Video: Evaluating Regression Models\nR Tutorial Video: Introduction to TidyModels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n9/17\nUnit 1: Overfitting\nCheckpoint 2\nR Tutorial Video (code)\nISLR: Section 2.1.2, 5.1\nOverfitting\n(Part 1 QMD) (Part 2 QMD)\nComplete CP3\nStart HW1\nContent Video: Overfitting\n\n\n9/19\nUnit 1: Cross Validation\nConcept Video 1 (script)\nConcept Video 2 (script)\nCheckpoint 3\nR Tutorial Video (code)\nISLR: 5.1\nCross-Validation\n(QMD)\nComplete CP4\nContinue HW1\nConcept Video: Cross-Validation\nR Tutorial: Training, Testing and Cross-Validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n9/24\nUnit 2: Model Selection\nVideo (script)\nCheckpoint #\nTopic\n(QMD)\nComplete CP#\nStart/Continue/Finish HW#\nISLR: Section #\nVideo: Topic\n\n\n9/26\nUnit 2: LASSO (Shrinkage/Regularization)\nVideo (script)\nCheckpoint #\nTopic\n(QMD)\nComplete CP#\nStart/Continue/Finish HW#\nISLR: Section #\nVideo: Topic\n\n\n\n \n\n--&gt;\nNote: this page is currently under construction! More dates and links coming soon.",
    "crumbs": [
      "Overview",
      "Schedule"
    ]
  },
  {
    "objectID": "learning-objectives.html",
    "href": "learning-objectives.html",
    "title": "Learning Goals",
    "section": "",
    "text": "General Skills\nComputational Thinking\nEthical Data Thinking\nData Communication\nCollaborative Learning",
    "crumbs": [
      "Overview",
      "Learning Goals"
    ]
  },
  {
    "objectID": "learning-objectives.html#general-skills",
    "href": "learning-objectives.html#general-skills",
    "title": "Learning Goals",
    "section": "",
    "text": "Be able to perform the following tasks:\n\nDecomposition: Break a task into smaller tasks to be able to explain the process to another person or computer\nPattern Recognition: Recognize patterns in tasks by noticing similarities and common differences\nAbstraction: Represent an idea or process in general terms so that you can use it to solve other projects that are similar in nature\nAlgorithmic Thinking: Develop a step-by-step strategy for solving a problem\n\n\n\n\n\nIdentify ethical issues associated with applications of statistical machine learning in a variety of settings\nAssess and critique the actions of individuals and organizations as it relates to ethical use of data\n\n\n\n\nIn written and oral formats:\n\nInform and justify data analysis and modeling process and the resulting conclusions with clear, organized, logical, and compelling details that adapt to the background, values, and motivations of the audience and context in which communication occurs.\n\n\n\n\nUnderstand and demonstrate characteristics of effective collaboration (team roles, interpersonal communication, self-reflection, awareness of social dynamics, advocating for yourself and others).\nDevelop a common purpose and agreement on goals.\nBe able to contribute questions or concerns in a respectful way.\nShare and contribute to the group’s learning in an equitable manner.",
    "crumbs": [
      "Overview",
      "Learning Goals"
    ]
  },
  {
    "objectID": "learning-objectives.html#course-topics",
    "href": "learning-objectives.html#course-topics",
    "title": "Learning Goals",
    "section": "Course Topics",
    "text": "Course Topics\nSpecific learning objectives for our course topics are listed below. Use these to guide your synthesis of video and reading material for specific topics.\nIntroduction to Statistical Machine Learning\n\nFormulate research questions that align with regression, classification, or unsupervised learning tasks.\nIdentify the appropriate task (regression, classification, unsupervised) for a given research question.\n\n\nUnit 1\nEvaluating Regression Models\n\nCreate and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns.\nCalculate and interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way.\n\n\nOverfitting and cross-validation\n\nExplain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance\nAccurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric\nExplain what role CV has in a predictive modeling analysis and its connection to overfitting\nExplain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time\n\n\nUnit 2\nModel selection\n\nExplain the difference between inferential models and predictive models and how the model building processes differ\nClearly describe the backward stepwise selection algorithm and why they are examples of greedy algorithms\nCompare best subset and stepwise algorithms in terms of optimality of output and computational time\n\n\nLASSO (shrinkage/regularization)\n\nExplain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection\nExplain how the lambda tuning parameter affects model performance and how this is related to overfitting\n\n\nUnit 3\nKNN Regression and the Bias-Variance Tradeoff\n\nClearly describe / implement by hand the KNN algorithm for making a regression prediction\nExplain how the number of neighbors relates to the bias-variance tradeoff\nExplain the difference between parametric and nonparametric methods\nExplain how the curse of dimensionality relates to the performance of KNN\n\n\nLocal Regression and Generalized Additive Models\n\nClearly describe the local regression algorithm for making a prediction\nExplain how bandwidth (span) relate to the bias-variance tradeoff\nDescribe some different formulations for a GAM (how the arbitrary functions are represented)\nExplain how to make a prediction from a GAM\nInterpret the output from a GAM\n\n\nUnit 4\nClassification via Logistic regression\n\nUse a logistic regression model to make hard (class) and soft (probability) predictions\nInterpret non-intercept coefficients from logistic regression models in the data context\n\n\nEvaluating classification models\n\nCalculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity\nConstruct and interpret plots of predicted probabilities across classes\nExplain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric\nAppropriately use and interpret the no-information rate to evaluate accuracy metrics\n\n\nDecision trees\n\nClearly describe the recursive binary splitting algorithm for tree building for both regression and classification\nCompute the weighted average Gini index to measure the quality of a classification tree split\nCompute the sum of squared residuals to measure the quality of a regression tree split\nExplain how recursive binary splitting is a greedy algorithm\nExplain how different tree parameters relate to the bias-variance tradeoff\n\n\nBagging and random forests\n\nExplain the rationale for bagging\nExplain the rationale for selecting a random subset of predictors at each split (random forests)\nExplain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff\nExplain the rationale for and implement out-of-bag error estimation for both regression and classification\nExplain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class)\n\n\nHierarchical clustering\n\nClearly describe / implement by hand the hierarchical clustering algorithm\nCompare and contrast k-means and hierarchical clustering in their outputs and algorithms\nInterpret cuts of the dendrogram for single and complete linkage\nDescribe the rationale for how clustering algorithms work in terms of within-cluster variation\nDescribe the tradeoff of more vs. less clusters in terms of interpretability\nImplement strategies for interpreting / contextualizing the clusters\n\n\nK-means clustering\n\nClearly describe / implement by hand the k-means algorithm\nDescribe the rationale for how clustering algorithms work in terms of within-cluster variation\nDescribe the tradeoff of more vs. less clusters in terms of interpretability\nImplement strategies for interpreting / contextualizing the clusters\n\n\nPrincipal Component Analysis\n\nExplain the goal of dimension reduction and how this can be useful in a supervised learning setting\nInterpret and use the information provided by principal component loadings and scores\nInterpret and use a scree plot to guide dimension reduction",
    "crumbs": [
      "Overview",
      "Learning Goals"
    ]
  },
  {
    "objectID": "L01-introductions.html",
    "href": "L01-introductions.html",
    "title": "1  Introductions",
    "section": "",
    "text": "Welcome!\nNote: everything you need for class today is on the course website: https://kegrinde.github.io/stat253_coursenotes/",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#whats-machine-learning",
    "href": "L01-introductions.html#whats-machine-learning",
    "title": "1  Introductions",
    "section": "What’s Machine Learning?",
    "text": "What’s Machine Learning?\n“Machine Learning” was coined back in 1959 by Arthur Samuel, an early contributor to AI.\nFrom Kohavi & Provost (1998): Machine Learning is the exploration & application of algorithms that can learn from existing patterns and make predictions using data. (NOTE: humans are in charge of the exploration & application!)\n\n\nIn STAT 253 we will…\n\nPick up where STAT 155 left off, acquiring tools that can be used to learn from data in greater depth and a wider variety of settings. (STAT 155 is a foundational subset of ML!)\nExplore universal ML concepts using tools and software common among statisticians (hence “statistical” machine learning).\nSurvey a breadth of modern ML tools and algorithms that fall into the workflow below. We’ll focus on concepts and applications over mathematical theory. Part of the cognitive load will be:\n\nkeeping all the tools in place (what are they and when to use them)\nunderstanding the connections between the tools\nadapting (not memorizing) code to implement each tool",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#supervised-learning",
    "href": "L01-introductions.html#supervised-learning",
    "title": "1  Introductions",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nWe want to model the relationship between some output variable \\(y\\) and input variables \\(x = (x_1, x_2,..., x_p)\\):\n\\[\\begin{split}\ny\n& = f(x) + \\varepsilon \\\\\n& = \\text{(trend in the relationship) } + \\text{ (residual deviation from the trend `epsilon`)} \\\\\n\\end{split}\\]\nTypes of supervised learning tasks:\n\nregression: \\(y\\) is quantitative\nexample:\n\\(y\\) = body mass index\n\\(x\\) = (number of live births, age, marital status, education, etc)\nclassification: \\(y\\) is categorical\nexample:\n\\(y\\) = whether a pair of crickets courted (yes, no) \\(x\\) = (species, pair of same species, CHC profile, etc)",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#unsupervised-learning",
    "href": "L01-introductions.html#unsupervised-learning",
    "title": "1  Introductions",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nWe have some input variables \\(x = (x_1, x_2,..., x_p)\\) but there’s no output variable \\(y\\). Thus the goal is to use \\(x\\) to understand and/or modify the structure of our data with respect to \\(x\\).\nTypes of unsupervised learning tasks:\n\nclustering\nIdentify and examine groups or clusters of data points that are similar with respect to their \\(x_i\\) values. example:\n\n\\(x\\) = (body mass index at 2 weeks, 1 month, 2 months, 4 months, 6 months, etc)\n\ndimension reduction\nTurn the original set of \\(p\\) input variables, which are potentially correlated, into a smaller set of \\(k &lt; p\\) variables which still preserve the majority of information in the originals. example:\n\n\\(x\\) = (cuticular hydrocarbon compounds concentrations based on gas chromatography analysis)",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#meet-your-classmates",
    "href": "L01-introductions.html#meet-your-classmates",
    "title": "1  Introductions",
    "section": "Meet Your Classmates!",
    "text": "Meet Your Classmates!\nI used a machine learning algorithm, one we’ll learn later this semester, to form groups based on your responses to the pre-course informational survey. BUT it didn’t provide any explanation of why these are the groups it picked. To that end, we need humans.\n\nGet into your assigned group.\nIntroduce yourselves in whatever way you feel appropriate (ideas: name, pronouns, how you’re feeling at the moment, things you’re looking forward to, best part of summer, why you are motivated to take this class)\nTry to figure out why the algorithm put you into a group together. (I don’t personally know the answer!)\nPrepare to introduce your group to the bigger class:\n\nEach person will introduce themself\nOne person will explain why they think the group was put together",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#meet-your-instructor",
    "href": "L01-introductions.html#meet-your-instructor",
    "title": "1  Introductions",
    "section": "Meet Your Instructor",
    "text": "Meet Your Instructor\nA few highlights from my answers to the Pre-Course Information Gathering Survey…\n\nPreferred name: “Kelsey” or “Professor Grinde”\nPronouns: she/her/hers\nHometown(s): Plymouth –&gt; Northfield –&gt; Seattle –&gt; St. Paul\nCan you tell me a bit about how you’ve been spending your time this summer? What’s been particularly important or meaningful to you? What brings you joy right now? What is on your mind? What do you do when you’re not in class?",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#instructions",
    "href": "L01-introductions.html#instructions",
    "title": "1  Introductions",
    "section": "Instructions",
    "text": "Instructions\n\nDiscuss the following scenarios as a group, talking through your ideas, questions, and reasoning as you go\nI’ll move around to groups to check in on your progress and see what questions you have\nYou can check your answers by clicking the drop-down “Solutions” button",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#questions",
    "href": "L01-introductions.html#questions",
    "title": "1  Introductions",
    "section": "Questions",
    "text": "Questions\nIndicate whether each scenario below represents a regression, classification, or clustering task.\n\nHow is the number of people that rent bikes on a given day in Washington, D.C. (\\(y\\)) explained by the temperature (\\(x_1\\)) and whether or not it’s a weekend (\\(x_2\\))?\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nregression. there’s a quantitative output variable \\(y\\).\n\n\n\nGiven the observed bill length (\\(x_1\\)) and bill depth (\\(x_2\\)) on a set of penguins, how many different penguin species might there be?\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nclustering. there’s no output variable \\(y\\).\n\n\n\nHow can we determine whether somebody has a certain infection (\\(y\\)) based on two different blood sample measurements, Measure A (\\(x_1\\)) and Measure B (\\(x_2\\))?\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nclassification. there’s a categorical output variable \\(y\\).\n\n\n\nMachine learn about each other! Scenario A.\nI collected some data on STAT 253 students (you!) and analyzed it using a machine learning algorithm. In your groups: (1) brainstorm what research question is being investigated; (2) determine whether this is a regression, classification, or clustering task; and (3) summarize what the output tells you about your classmates.\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nclassification (\\(y\\) = major is categorical)\n\n\n\nMachine learn about each other! Scenario B.\nSame directions as for Scenario A:\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nregression (\\(y\\) = time to mac is quantitative)\n\n\n\nMachine learn about each other! Scenario C.\nSame directions as for Scenario A:\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nclustering (no outcome \\(y\\)).\n\n\n\nUse Spotify users’ previous listening behavior to identify groups of similar users.\n\n\n\nSolution\n\nclustering\n\n \n\nPredict workers’ wages by their years of experience.\n\n\n\n\nSolution\n\nregression (\\(y\\) = wages)\n\n\n\nPredict workers’ wages by their college major.\n\n\n\n\nSolution\n\nregression (\\(y\\) = wages)\n\n\n\nUse a customer’s age to predict whether they’ve seen the Barbie movie.\n\n\n\n\nSolution\n\nclassification (\\(y\\) = whether or not watched the film)\n\n\n\nLook for similarities among genetic samples taken from a group of patients.\n\n\n\n\nSolution\n\nclustering (no outcome \\(y\\))",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#scavenger-hunt",
    "href": "L01-introductions.html#scavenger-hunt",
    "title": "1  Introductions",
    "section": "Scavenger Hunt",
    "text": "Scavenger Hunt\nTake a few minutes to make sure you know how to find all of the following:\n\ncourse website\nsyllabus\ntextbook\nSTAT 253 Slack\noffice hour times and locations\nassignment deadlines\ninformation on what you need to complete before class each day\nin-class activities\nassignment instructions / submission",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#whats-next",
    "href": "L01-introductions.html#whats-next",
    "title": "1  Introductions",
    "section": "What’s next?",
    "text": "What’s next?\nWhat to work on after class today:\n\ncarefully review the syllabus\n\nif time allows, we’ll discuss a few highlights now!\nmore to come in the next few class sessions\n\njoin Slack\nupdate your versions of R/RStudio (see R and RStudio Setup)\ncomplete the pre-class tasks for Thursday (videos/reading/checkpoint)\n\nreview the checkpoint recommendations/policies on Moodle before you start!\n\nstart HW0 (due Friday)",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "U01-motivation.html",
    "href": "U01-motivation.html",
    "title": "Motivating Question",
    "section": "",
    "text": "We are in the regression setting. We want to build a model of some quantitative output variable \\(y\\) by some predictors \\(x\\):\n\\[y = f(x) + \\epsilon\\]\nAfter building this model, it’s important to evaluate it: Is our regression model a “good” model?\n\nIs the model wrong?\nIs the model strong?\nDoes the model produce accurate predictions?\nIs the model fair?\n\n\n\nIs the model wrong? What assumptions does our model make and are these reasonable?\n\n\n\n\n\n\n\n\n\n\nTo check:\nExamine a residual plot, ie. a scatterplot of the residuals vs predictions for each case. Points should appear randomly scattered with no clear pattern. If you see any patterns in the residuals that suggest you are systematically over or underpredicting for different prediction values, this indicates that the assumption about the relationship with predictors could be wrong.\nExample: Model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\). What about the plots below reveals that this model is “wrong”?\n\n\n\n\n\n\n\n\n\n\nWhat about the plots below reveals that this model is “not wrong”?\n\n\n\n\n\n\n\n\n\n\n\nIs the model strong? How well does our model explain the variability in the response?\n\n\n\n\n\n\n\n\n\n\nCheck: \\(R^2\\), the proportion of variability in \\(y\\) that’s explained by the model. The closer to 1 the better.\n\\[R^2 = 1 - \\frac{\\text{Var}(\\text{residuals})}{\\text{Var}(y)} = 1 - \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n(y_i - \\overline{y})^2}\\]\n\n\nDoes the model produce accurate predictions?\n\n\n\n\n\n\n\n\n\n\n\nCheck: Summarize the combined size of the residuals, \\(y_1 - \\hat{y}_1\\), \\(y_2 - \\hat{y}_2\\), …, \\(y_n - \\hat{y}_n\\) where \\(n\\) is sample size. The closer to 0 the better!\n\\[\\begin{split}\n\\text{MSE}  & = \\text{ mean squared error } = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n\\text{RMSE} & = \\text{ root mean squared error } = \\sqrt{MSE}  \\\\\n\\text{MAE}  & = \\text{ mean absolute error } = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \\\\\n\\end{split}\\]\n\n\nIs the model fair?\n\n\nWho collected the data / who funded the data collection?\nHow did they collect the data?\nWhy did they collect the data?\nWhat are the implications of the analysis, ethical or otherwise?\n\nDig Deeper (optional)\nDigging deeper, there’s more theory behind our regression model assumptions, thus more to the question of “is our model wrong?”. Specifically, in applying the linear regression model\n\\[y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k} + \\epsilon\\]\nwe assume that at any given set of predictors \\(x = (x_1,x_2,...,x_n)\\),\n\\[\\epsilon \\stackrel{ind}{\\sim} N(0, \\sigma^2)\\]\nEquivalently, \\(y \\stackrel{ind}{\\sim} N(\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k}, \\; \\sigma^2)\\).\n\nWe can break this assumption and \\(N()\\) notation down into 4 pieces:\n\nIndependence: \\(\\epsilon \\stackrel{\\color{red}{ind}}{\\sim} N(0, \\sigma^2)\\) The observations on subject \\(i\\) are independent of the observations on any other subject.\n\nNOTE: If our data don’t meet this model assumption, our predictions and inference (eg: confidence intervals & hypothesis tests) might produce misleading results. Take Correlated Data to learn more about working with dependent data.\n\nTrend: \\(\\epsilon \\stackrel{ind}{\\sim} N(\\color{red}{0}, \\sigma^2)\\) At any \\(x\\), the residuals have mean 0. That is, responses are balanced above and below the model. Thus the model accurately captures the trend of the relationship.\n\nNOTE: If our data don’t meet this model assumption, our model is wrong. This issue might be corrected by transforming \\(y\\) or \\(x\\).\n\nHomoskedasticity: \\(\\epsilon \\stackrel{ind}{\\sim} N(0, \\color{red}{\\sigma}^2)\\) At any \\(x\\), the standard deviation among the residuals is \\(\\sigma\\). That is, deviations from the trend are no greater at any one “part” of the model than at another NOTE: If our data don’t meet this model assumption, our inference (eg: confidence intervals & hypothesis tests) might produce misleading results. This issue might be corrected by transforming \\(y\\).\nNormality: \\(\\epsilon \\stackrel{ind}{\\sim} \\color{red}{N}(0, \\sigma^2)\\) The residuals are normally distributed. Thus individual responses are normally distributed around the trend (closer to the trend and then tapering off).\n\nNOTE: If our data don’t meet this model assumption and the violation is extreme, our inference (eg: confidence intervals & hypothesis tests) might produce misleading results. This issue might be corrected by transforming \\(y\\).",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html",
    "href": "L02-evaluating-regression-models.html",
    "title": "2  Model Evaluation",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#directions",
    "href": "L02-evaluating-regression-models.html#directions",
    "title": "2  Model Evaluation",
    "section": "Directions",
    "text": "Directions\n\nIn small groups, please first introduce yourself (in whatever way you feel appropriate) and check in with each other as human beings.\nWhen everyone is ready, glance through the summary of concepts covered in the video (see “Video Recap” below) and discuss the following prompts:\n\nWhat vocabulary or notation was new to you?\nWhat concepts were new to you?\nWhat concepts are still unclear to you at this moment?\n\nPrepare to share a few highlights from your group discussion with the entire class",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#video-recap",
    "href": "L02-evaluating-regression-models.html#video-recap",
    "title": "2  Model Evaluation",
    "section": "Video Recap",
    "text": "Video Recap\n\n\n\nWe are in the regression setting. We want to build a model of some quantitative output variable \\(y\\) by some predictors \\(x\\):\n\\[y = f(x) + \\epsilon\\]\nThere are many regression tools that we might use to build this model. We’ll use a linear regression model which assumes that \\(y\\) is a linear combination of the \\(x\\)’s:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots \\beta_p x_p + \\epsilon\\]\nAfter building any model, it’s important to evaluate it: Is our regression model a “good” model?\n\nIs the model wrong?\nIs the model strong?\nDoes the model produce accurate predictions?\nIs the model fair?\n\nWe will review these concepts through today’s exercises. A detailed overview is provided in the “Motivating Question” section under “Regression: Model Evaluation (Unit 1)” on the course website.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#intro-to-tidymodels",
    "href": "L02-evaluating-regression-models.html#intro-to-tidymodels",
    "title": "2  Model Evaluation",
    "section": "Intro to tidymodels",
    "text": "Intro to tidymodels\nThroughout the semester, we are going to use the tidymodels package in R.\n\nSimilar flavor to tidyverse structure\nMore general structure that allows us to fit many other types of models\n\n. . .\nAt first, it will seem like a lot more code (perhaps even unnecessarily so).\n. . .\nFor example, what you did in STAT 155 with\n\nlm(y ~ x1 + x2, data = sample_data)\n\n. . .\nwill now look like\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% # we want a linear regression model\n  set_mode(\"regression\") %&gt;%  # this is a regression task (y is quantitative)\n  set_engine(\"lm\")# we'll estimate the model using the lm function\n\n# STEP 2: model estimation\nmodel_estimate &lt;- lm_spec %&gt;% \n  fit(y ~ x1 + x2, data = sample_data)\n\n\nBut you’ll need to trust me…",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#highlight-useful-model-evaluation-functions",
    "href": "L02-evaluating-regression-models.html#highlight-useful-model-evaluation-functions",
    "title": "2  Model Evaluation",
    "section": "Highlight: Useful Model Evaluation Functions",
    "text": "Highlight: Useful Model Evaluation Functions\nA few useful functions to use on model_estimate:\n\n. . .\n\nmodel_estimate %&gt;% \n  tidy() #gives you coefficients (and se, t-statistics)\n\n\n. . .\n\nmodel_estimate %&gt;% \n  augment(new_data = sample_data) # gives you predictions and residuals for sample_data\n\n\n. . .\n\nmodel_estimate %&gt;% \n  glance() #gives you some model evaluation metrics (is it strong?)\n\n\n. . .\n\nmodel_estimate %&gt;% \n  augment(new_data = sample_data) %&gt;% \n  mae(truth = y, estimate = .pred) # calculates MAE to measure accuracy of predictions\n\n. . .\nMore info, for future reference, below!",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#future-reference-r-code-for-building-and-evaluating-regression-models",
    "href": "L02-evaluating-regression-models.html#future-reference-r-code-for-building-and-evaluating-regression-models",
    "title": "2  Model Evaluation",
    "section": "Future Reference: R Code for Building and Evaluating Regression Models",
    "text": "Future Reference: R Code for Building and Evaluating Regression Models\nThis section is for future reference. It is a summary of code you’ll learn below for building and evaluating regression models. Throughout, suppose we wish to build and evaluate a linear regression model of y vs x1 and x2 using our sample_data.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nBuilding a linear regression model\n\n# STEP 1: specify the type of model to build\nlm_spec &lt;- linear_reg() %&gt;% # we want a linear regression model\n  set_mode(\"regression\") %&gt;%  # this is a regression task (y is quantitative)\n  set_engine(\"lm\") # we'll estimate the model using the lm function\n\n# STEP 2: estimate the specified model using sample data\nmodel_estimate &lt;- lm_spec %&gt;% \n  fit(y ~ x1 + x2, data = sample_data)\n\n# Get the model coefficients\nmodel_estimate %&gt;% \n  tidy()\n\nObtaining predictions (& residuals) for each observation\n\n# Obtain y predictions and residuals for each observation in our sample_data\n# (We can replace sample_data with any data frame that includes y, x1, and x2)\nmodel_estimate %&gt;% \n  augment(new_data = sample_data)\n\n# Obtain y predictions (but not residuals) for some given x1, x2 values, when we haven't yet observed y\n# (We can replace the data.frame with any data frame that includes x1 and x2)\nmodel_estimate %&gt;% \n  augment(new_data = data.frame(x1 = ___, x2 = ___))\n  \n# Another approach using predict()\nmodel_estimate %&gt;% \n  predict(new_data = data.frame(x1 = ___, x2 = ___))\n\nEvaluating the model\n\n# Is it strong? (R^2)\nmodel_estimate %&gt;% \n  glance()\n\n# Does it produce accurate predictions? (MAE)\nmodel_estimate %&gt;% \n  augment(new_data = sample_data) %&gt;% \n  mae(truth = y, estimate = .pred)\n\n# Is it wrong? (residual plot)\nmodel_estimate %&gt;% \n  augment(new_data = sample_data) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#instructions",
    "href": "L02-evaluating-regression-models.html#instructions",
    "title": "2  Model Evaluation",
    "section": "Instructions",
    "text": "Instructions\n\nWork through these exercises as a group, talking through your ideas, questions, and reasoning as you go and taking notes in your QMD\nBe kind to yourself/each other! You will be rusty and make mistakes, and that’s ok! Mistakes are important to learning.\nFocus on patterns in code. Review, but do not try to memorize any provided code. Focus on the general steps and patterns.\nIf you’re given some starter code with blanks (e.g. below), don’t type in those chunks. Instead, copy, paste, and modify the starter code in the chunk below it.\n\n\n# Start small: rides vs temp\nggplot(___, aes(y = ___, x = ___)) + \n  geom___()\n\n\nAsk questions! We will not have time to discuss all exercises at the end of class. Talk through your questions as a group, and ask me questions as I walk around the room!\nCollaborate. We’re sitting in groups for a reason. Collaboration improves higher-level thinking, confidence, communication, community, and more. I expect you to:\n\nActively contribute to discussion (don’t work on your own)\nActively include all group members in discussion\nCreate a space where others feel comfortable making mistakes & sharing their ideas (remember that we all come to this class with different experiences, both personal and academic)\nStay in sync while respecting that everybody has different learning strategies, work styles, note taking strategies, etc. If some people are working on exercise 10 and others are on exercise 2, that’s probably not a good collaboration.\nDon’t rush. You won’t hand anything in and can finish up outside of class.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#questions",
    "href": "L02-evaluating-regression-models.html#questions",
    "title": "2  Model Evaluation",
    "section": "Questions",
    "text": "Questions\n\nCapital Bikeshare provides a bike-sharing service in the Washington DC area. Customers can pick up and drop off bikes at any station around the city. Of primary interest to the company is:\nHow many registered riders can we expect today?\nTo this end, you will build, evaluate, and compare 2 different linear regression models of ridership using the following Capital Bikeshare dataset (originally from the UCI Machine Learning Repository):\n\n# Load packages we'll need to wrangle and plot the data\nlibrary(tidyverse)\n\n# Load the data\nbikes &lt;- read.csv(\"https://mac-stat.github.io/data/bike_share.csv\")\n\n# Only keep / select some variables\n# And round some variables (just for our demo purposes)\nbikes &lt;- bikes %&gt;% \n  rename(rides = riders_registered, temp = temp_feel) %&gt;% \n  mutate(windspeed = round(windspeed), temp = round(temp)) %&gt;% \n  select(rides, windspeed, temp, weekend)\n\n\n# Check out the dimensions\ndim(bikes)\n\n# Check out the first 3 rows\nhead(bikes, 3)\n\nThis dataset contains the following information for a sample of different dates:\n\n\n\nvariable\ndescription\n\n\n\n\nrides\ncount of daily rides by registered users\n\n\nwindspeed\nwind speed in miles per hour\n\n\ntemp\nwhat the temperature feels like in degrees Fahrenheit\n\n\nweekend\nwhether or not it falls on a weekend\n\n\n\n We’ll consider two linear regression models of ridership:\nrides ~ windspeed + temp and rides ~ windspeed + temp + weekend\n\n\nPlot the relationships. First, let’s plot these relationships. REMINDER: Don’t write in any chunk with starter code. Copy, paste, and modify the code in the chunk below it.\n\n\n# Start small: rides vs temp\nggplot(___, aes(y = ___, x = ___)) + \n  geom___()\n\n\n# rides vs temp & windspeed\nggplot(bikes, aes(y = ___, x = ___, ___ = windspeed)) + \n  geom_point()\n\n\n# rides vs temp & windspeed & weekend\nggplot(bikes, aes(y = ___, x = ___, ___ = windspeed)) + \n  geom_point() +  \n  facet_wrap(~ ___)\n\n\n\nSolution\n\n\n# Start small: rides vs temp\nggplot(bikes, aes(y = rides, x = temp)) + \n  geom_point()\n\n\n\n\n\n\n\n# rides vs temp & windspeed\nggplot(bikes, aes(y = rides, x = temp, color = windspeed)) + \n  geom_point()\n\n\n\n\n\n\n\n# rides vs temp & windspeed & weekend\nggplot(bikes, aes(y = rides, x = temp, color = windspeed)) + \n  geom_point() +  \n  facet_wrap(~ weekend)\n\n\n\n\n\n\n\n\n\n\n\ntidymodels STEP 1: model specification. We’ll build and evaluate our two models of ridership using the tidymodels package. This code is more complicated than the lm()function we used in STAT 155. BUT:\n\n\ntidymodels is part of the broader tidyverse (what we use to plot and wrangle data), thus the syntax is more consistent\ntidymodels generalizes to the other ML algorithms we’ll survey in this course, thus will eventually minimize the unique syntax we need to learn\n\n\n# Load package\nlibrary(tidymodels)\n\nThe first step is to specify what type of model we want to build. We’ll store this as lm_spec, our linear regression model (lm) specification (spec).\n\nlm_spec &lt;- linear_reg() %&gt;%   # we want a linear regression model\n  set_mode(\"regression\") %&gt;%  # this is a regression task (y is quantitative)\n  set_engine(\"lm\")# we'll estimate the model using the lm function\n\nThis code specifies but doesn’t build any model – we didn’t even give it any data or specify the variables of interest!\n\n# Check it out\nlm_spec\n\n\n\nSolution\n\n\n# Load package\nlibrary(tidymodels)\n\nlm_spec &lt;- linear_reg() %&gt;% # we want a linear regression model\n  set_mode(\"regression\") %&gt;%  # this is a regression task (y is quantitative)\n  set_engine(\"lm\")# we'll estimate the model using the lm function\n\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n\ntidymodels STEP 2: model estimation. We can now estimate or fit our two ridership models using the specified model structure (lm_spec) and our sample bikes data:\n\n\n# Fit bike_model_1\nbike_model_1 &lt;- lm_spec %&gt;% \n  fit(rides ~ windspeed + temp, data = bikes)\n\n# Check out the coefficients\nbike_model_1 %&gt;% \n  tidy()\n\n\n# YOUR TURN\n# Fit bike_model_2 & check out the coefficients\n\n\n\nSolution\n\n\nbike_model_1 &lt;- lm_spec %&gt;% \n  fit(rides ~ windspeed + temp, data = bikes)\n\nbike_model_2 &lt;- lm_spec %&gt;% \n  fit(rides ~ windspeed + temp + weekend, data = bikes)\n\n# Check out the results:\nbike_model_1 %&gt;% \n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -20.8    300.     -0.0694 9.45e- 1\n2 windspeed      -36.1      9.42   -3.83   1.37e- 4\n3 temp            55.4      3.33   16.6    7.58e-53\n\nbike_model_2 %&gt;% \n  tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    298.     289.        1.03 3.02e- 1\n2 windspeed      -35.6      9.00     -3.95 8.46e- 5\n3 temp            54.3      3.18     17.1  3.82e-55\n4 weekendTRUE   -858.     101.       -8.46 1.47e-16\n\n\n\n\n\n\nIs it fair? Now, let’s evaluate our two models. First, do you have any concerns about the context in which the data were collected and analyzed? About the potential impact of this analysis?\n\n\n\nSolution\n\nWhat do you think?\n\nWho might be harmed?\nWho benefits?\n\n\n\n\nIs it strong? We can measure and compare the strength of these models using \\(R^2\\), the proportion of variability in our response variable that’s explained by the model. Report which model is stronger and interpret its \\(R^2\\).\n\n\n# Obtain R^2 for bike_model_1\nbike_model_1 %&gt;% \n  glance()\n\n\n# YOUR TURN\n# Obtain R^2 for bike_model_2\n\n\n\nSolution\n\nModel 2 is stronger than model 1 (\\(R^2\\) of 0.372 vs 0.310). But it only explains 37% of the variability in ridership from day to day.\n\n# Obtain R^2 for bike_model_1\nbike_model_1 %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.310         0.308 1298.      163. 2.44e-59     2 -6276. 12560. 12578.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nbike_model_2 %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.372         0.369 1239.      143. 5.82e-73     3 -6242. 12493. 12516.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nPause: get the residuals and predictions. Our next model evaluation questions will focus on the models’ predictions and prediction errors, or residuals. We can obtain this information by augmenting our models with our original bikes data. For example:\n\n\n# Calculate predicted ridership (.pred) & corresponding residuals (.resid) using bike_model_1\n# Just look at first 6 days\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  head()\n\nWe can also predict outcomes for new observations using either augment() or predict(). Note the difference in the output:\n\n# Predict ridership on a 60 degree day with 20 mph winds\nbike_model_1 %&gt;% \n  augment(new_data = data.frame(windspeed = 20, temp = 60))\n\n\n# Predict ridership on a 60 degree day with 20 mph winds\nbike_model_1 %&gt;% \n  predict(new_data = data.frame(windspeed = 20, temp = 60))\n\n\n\nSolution\n\naugment() gives the predictions and residuals for all rows in the data. predict() only gives you predictions.\n\n# Obtain the predictions & residuals using bike_model_1\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  head()\n\n# A tibble: 6 × 6\n  .pred .resid rides windspeed  temp weekend\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;  \n1 3183. -2529.   654        11    65 TRUE   \n2 2911. -2241.   670        17    64 TRUE   \n3 2080.  -851.  1229        17    49 FALSE  \n4 2407.  -953.  1454        11    51 FALSE  \n5 2446.  -928.  1518        13    53 FALSE  \n6 2699. -1181.  1518         6    53 FALSE  \n\n# Predict ridership on a 60 degree day with 20 mph winds\nbike_model_1 %&gt;% \n  augment(new_data = data.frame(windspeed = 20, temp = 60))\n\n# A tibble: 1 × 3\n  .pred windspeed  temp\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 2581.        20    60\n\n# Predict ridership on a 60 degree day with 20 mph winds\nbike_model_1 %&gt;% \n  predict(new_data = data.frame(windspeed = 20, temp = 60))\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1 2581.\n\n\n\n\n\nDoes it produce accurate predictions? Recall that the mean absolute error (MAE) measures the typical prediction error. Specifically, it is the mean of the absolute values of the residual errors for the days in our dataset.\n\n\nUse the residuals to calculate the MAE for the 2 models. HINT: abs().\n\n\n# MAE for bike_model_1\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  summarize(mae = ___(___(___)))\n\n# MAE for bike_model_2\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  summarize(mae = ___(___(___)))\n\n\nDoing the calculation from scratch helps solidify your understanding of how MAE is calculated, thus interpreted. Check your calculations using a shortcut function.\n\n\n# Calculate MAE for the first model\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  mae(truth = rides, estimate = .pred)\n\n\n# YOUR TURN\n# Calculate MAE for the second model\n\n\nWhich model has more accurate predictions? Interpret the MAE for this model and comment on whether it’s “large” or “small”. NOTE: “large” or “small” is defined by the context (e.g. relative to the observed range of ridership, the consequences of a bad prediction, etc).\n\n\n\nSolution\n\nOn average, the model 1 predictions are off by ~1080 riders and the model 2 predictions are off by ~1038 riders. Is this a lot? Consider this error relative to the scale of the data: there are roughly 1000 - 7000 riders per day.\n\n# a\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  summarize(mae = mean(abs(.resid)))\n\n# A tibble: 1 × 1\n    mae\n  &lt;dbl&gt;\n1 1080.\n\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  summarize(mae = mean(abs(.resid)))\n\n# A tibble: 1 × 1\n    mae\n  &lt;dbl&gt;\n1 1038.\n\n# b\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  mae(truth = rides, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       1080.\n\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  mae(truth = rides, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       1038.\n\n\n\n\n\nIs it wrong? To determine whether the linear regression assumptions behind bike_model_1 and bike_model_2 are reasonable, we can review residual plots, i.e. plots of the residuals vs predictions for each observation in our dataset. Run the code below and summarize your assessment of whether our models are wrong. RECALL: We want the appoints to appear random and centered around 0 across the entire range of the model / predictions.\n\n\n# Residual plot for bike_model_1\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n# YOUR TURN\n# Residual plot for bike_model_2\n\n\n\nSolution\n\nBoth models look roughly “right” BUT there is a little downward slope at the extreme end of the residual plots. This corresponds to the observed phenomenon that when it’s really hot, ridership starts dipping. In a future model, we might incorporate a quadratic temperature term.\n\n# Residual plot for bike_model_1\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \ngeom_point() + \ngeom_hline(yintercept = 0)\n\n\n\n\n\n\n\n# Residual plot for bike_model_2\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \ngeom_point() + \ngeom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\n\n\n\nArt vs science Inspecting residual plots is more art than science.1 It requires a lot of practice. Consider another example using simulated data. First, build a model that assumes all predictors are roughly linearly related:\n\n\n# Import data\nsimulated_data &lt;- read.csv(\"https://ajohns24.github.io/data/simulated_data.csv\")\n\n# Model y by the 6 input variables\nnew_model &lt;- lm_spec %&gt;% \n  fit(y ~ x1 + x2 + x3 + x4 + x5 + x6, simulated_data)\n\nNext, check out a pairs plot. Is there anything here that makes you think that our model assumption is bad?\n\nlibrary(GGally)\nggpairs(simulated_data)\n\n\n\n\n\n\n\n\nFinally, check out a residual plot. Any concerns now?\n\nnew_model %&gt;% \n  ___(new_data = ___) %&gt;% \n  ggplot(aes(x = ___, y = ___)) + \n  geom_point(size = 0.1) + \n  geom_hline(yintercept = 0)\n\n\n\nSolution\n\nArt + Science!\n\nnew_model %&gt;% \n  augment(new_data = simulated_data) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \n  geom_point(size = 0.1) + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\n\n\n\nDetails: communication & code style Communication is a key machine learning skill, including written summaries, presentations, and code. Just like an essay, code must have structure, signposts, and grammar that will make it easier for others to follow. The below code runs, but it is “bad code”.\n\n\nFix this code and add comments so that it is easier for yourself and others to follow.\nAlso pay attention to what this code does.\n\n\nbikes%&gt;%group_by(weekend)%&gt;%summarize(median(rides))\n\nmynewdatasetissmallerthantheoriginal&lt;-bikes%&gt;%filter(rides&lt;=700,weekend==FALSE,temp&gt;60)\nmynewdatasetissmallerthantheoriginal\n\nmynewdatasetusescelsius&lt;-bikes%&gt;%mutate(temp=(temp-32)*5/9)\nhead(mynewdatasetusescelsius)\n\n\n\nSolution\n\n\n# Calculate the median ridership by weekend\n# Put each new thought or action on its own line! \n# This makes it easier to follow the steps.\nbikes %&gt;% \n  group_by(weekend) %&gt;% \n  summarize(median(rides))\n\n# A tibble: 2 × 2\n  weekend `median(rides)`\n  &lt;lgl&gt;             &lt;dbl&gt;\n1 FALSE              3848\n2 TRUE               2955\n\n# Obtain days on which there are at most 700 rides,\n# it's the weekend, and temps are above 60 degrees\n# Use a shorter name that's easier to read and type.\n# Add spaces to make things easier to read.\n# Add line breaks to make it easier to follow the steps.\nwarm_weekends &lt;- bikes %&gt;%\n  filter(rides &lt;= 700, weekend == FALSE, temp &gt; 60)\nwarm_weekends\n\n  rides windspeed temp weekend\n1   577        18   67   FALSE\n2   655        18   68   FALSE\n3    20        24   72   FALSE\n\n# Store temp in Celsius\nbikes_celsius &lt;- bikes %&gt;% \n  mutate(temp = (temp - 32)*5/9)\nhead(bikes_celsius)\n\n  rides windspeed      temp weekend\n1   654        11 18.333333    TRUE\n2   670        17 17.777778    TRUE\n3  1229        17  9.444444   FALSE\n4  1454        11 10.555556   FALSE\n5  1518        13 11.666667   FALSE\n6  1518         6 11.666667   FALSE\n\n\n\n\n\nSTAT 155 Review: model interpretation & application Let’s interpret and apply bike_model_2.\n\n\n___ %&gt;% \n  tidy()\n\n\nHow can we interpret the temp coefficient?\n\n\nWe expect roughly 54 more riders on warm days.\nWe expect roughly 54 more riders per every 1 degree increase in temperature.\nWhen controlling for windspeed and weekend status, we expect roughly 54 more riders on warm days.\nWhen controlling for windspeed and weekend status, we expect roughly 54 more riders per every 1 degree increase in temperature.\n\n\nHow can we interpret the weekendTRUE coefficient?\n\n\nWe expect roughly 858 fewer riders on weekends.\nWe expect roughly 858 fewer riders per every extra weekend.\nWhen controlling for windspeed and temperature, we expect roughly 858 fewer riders on weekends.\nWhen controlling for windspeed and temperature, we expect roughly 858 fewer riders per every extra weekend.\n\n\nReproduce the predicted ridership and corresponding residual for day 1 from scratch (how were these calculated?!):\n\n\nbike_model_2 %&gt;% \n  ___(new_data = bikes) %&gt;% \n  head(1)\n\n\n\nSolution\n\n\n# Get the coefficients\nbike_model_2 %&gt;% \n  tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    298.     289.        1.03 3.02e- 1\n2 windspeed      -35.6      9.00     -3.95 8.46e- 5\n3 temp            54.3      3.18     17.1  3.82e-55\n4 weekendTRUE   -858.     101.       -8.46 1.47e-16\n\n\n\nWhen controlling for windspeed and weekend status, we expect roughly 54 more riders per every 1 degree increase in temperature.\nWhen controlling for windspeed and temperature, we expect roughly 858 fewer riders on weekends (compared to weekdays).\n\n\n# Predict ridership on day 1\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  head(1)\n\n# A tibble: 1 × 6\n  .pred .resid rides windspeed  temp weekend\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;  \n1 2581. -1927.   654        11    65 TRUE   \n\n# This matches .pred\n298.45 - 35.57*11 + 54.33*65 - 857.76*1\n\n[1] 2580.87\n\n# Calculate the residual (observed - predicted)\n# This matches .resid\n654 - 2580.87\n\n[1] -1926.87\n\n\n\n\n\nSTAT 155 Review: data wrangling Through the “Details: communication & code style” and elsewhere, you’ve reviewed the use of various dplyr data wrangling verbs: filter(), mutate(), summarize(), group_by(), and select(). Use these to complete the following tasks.\n\n\nCalculate the mean temperature across all days in the data set.\nCalculate the mean temperature on weekends vs weekdays.\nPrint out the 3 days with the highest temperatures. HINT: arrange() or arrange(desc())\nName and store a new data set which: - only includes the days that fall on a weekend and have temps below 80 degrees - has a new variable, temp_above_freezing, which calculates how far the temperature is above (or below) freezing (32 degrees F) - only includes the windspeed, temp, and temp_above_freezing variables.\n\n\n\nSolution\n\n\n# a\nbikes %&gt;% \n  summarize(mean(temp))\n\n  mean(temp)\n1   74.69083\n\n# b\nbikes %&gt;% \n  group_by(weekend) %&gt;% \n  summarize(mean(temp))\n\n# A tibble: 2 × 2\n  weekend `mean(temp)`\n  &lt;lgl&gt;          &lt;dbl&gt;\n1 FALSE           75.1\n2 TRUE            73.7\n\n# c\nbikes %&gt;% \n  arrange(desc(temp)) %&gt;% \n  head(3)\n\n  rides windspeed temp weekend\n1  2825         9  108   FALSE\n2  3152        15  106   FALSE\n3  2298         9  104    TRUE\n\n# d\nnew_data &lt;- bikes %&gt;% \n  filter(weekend == TRUE, temp &lt; 80) %&gt;% \n  mutate(temp_above_freezing = temp - 32) %&gt;% \n  select(windspeed, temp, temp_above_freezing)\nhead(new_data)\n\n  windspeed temp temp_above_freezing\n1        11   65                  33\n2        17   64                  32\n3        18   47                  15\n4        24   42                  10\n5        11   54                  22\n6        13   53                  21\n\n\n\n\n\nSTAT 155 Review: plots\nConstruct plots of the following relationships:\n\n\nrides vs temp\nrides vs weekend\nrides vs temp and weekend\n\n\n\nSolution\n\n\n# a. rides vs temp\nggplot(bikes, aes(y = rides, x = temp)) + \n  geom_point()\n\n\n\n\n\n\n\n# b. rides vs weekend\nggplot(bikes, aes(y = rides, x = weekend)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(bikes, aes(x = rides, fill = weekend)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n# c. rides vs temp and weekend\n ggplot(bikes, aes(y = rides, x = temp, color = weekend)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nDone!\n\n\nKnit/render your notes.\nCheck the solutions on the course website.\nGet a head start on the wrap-up steps below.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#footnotes",
    "href": "L02-evaluating-regression-models.html#footnotes",
    "title": "2  Model Evaluation",
    "section": "",
    "text": "Stefanski, Leonard A. (2007). Residual (Sur)Realism. “The American Statistician,” 61, pp 163-177.↩︎",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html",
    "href": "L03-overfitting.html",
    "title": "3  Overfitting",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#directions",
    "href": "L03-overfitting.html#directions",
    "title": "3  Overfitting",
    "section": "Directions",
    "text": "Directions\nLet’s build and evaluate a predicted model of an adult’s height (\\(y\\)) using some predictors \\(x_i\\) (e.g., age, weight, etc.).\n\nIntroduce yourself in whatever way you feel appropriate and check in with each other as human beings\nCome up with a team name\nWork through the steps below as a group, after you are told your group number\n\nEach group will be given a different sample of 40 adults\nStart by predicting height (in) using hip circumference (cm)\nEvaluate the model on your sample.\n\nBe prepared to share your answers to:\n\nHow good is your simple model?\nWhat would happen if we added more predictors?",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#questions",
    "href": "L03-overfitting.html#questions",
    "title": "3  Overfitting",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\nGoal:\n\nLet’s build and evaluate a predictive model of an adult’s height (\\(y\\)) using some predictors \\(x_i\\) (eg: age, height, etc).\nSince \\(y\\) is quantitative this is a regression task.\nThere are countless possible models of \\(y\\) vs \\(x\\). We’ll utilize a linear regression model:\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p + \\varepsilon\\]\n\nAnd after building this model, we’ll evaluate it.\n\n\n\n\nData: Each group will be given a different sample of 40 adults.\n\n# Load packages needed for this analysis\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n# Load your data: fill in the blanks at end of url with your number\n# group 1 = 50\n# group 2 = 143\n# group 3 = 160\n# group 4 = 174\n# group 5 = 86\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat___.csv\") %&gt;% \n  filter(ankle &lt; 30) %&gt;% \n  rename(body_fat = fatSiri)\n\n\n# Check out a density plot of your y outcomes\nggplot(humans, aes(x = height)) + \n  geom_density()\n\n\n\nModel building: Build a linear regression model of height (in) by hip circumference (cm).\n\n# STEP 1: model specification\nlm_spec &lt;- ___() %&gt;% \n  set_mode(___) %&gt;% \n  set_engine(___)\n\n\n# STEP 2: model estimation\nmodel_1 &lt;- ___ %&gt;% \n  ___(height ~ hip, data = humans)\n\n\n# Check out the coefficients\n# Do all groups have the same coefficients? Should they?\n\n\n\nSolution\n\nEach group will have slightly different coefficients because they have different samples of data.\n\n#This is an example with one of the samples\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat50.csv\") %&gt;% \n  filter(ankle &lt; 30) %&gt;% \n  rename(body_fat = fatSiri)\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode('regression') %&gt;% \n  set_engine('lm')\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip, data = humans)\n\n# Check out the coefficients\nmodel_1  %&gt;% \n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)   52.5      7.68        6.84 0.0000000460\n2 hip            0.179    0.0778      2.30 0.0272      \n\n\n\n\nModel evaluation: How good is our model?\n\n# Calculate the R^2 for model_1\n\n\n# Use your model to predict height for your subjects\n# Just print the first 6 results\nmodel_1 %&gt;% \n  ___(new_data = ___) %&gt;% \n  head()\n\n\n# Calculate the MAE, i.e. typical prediction error, for your model\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  ___(truth = ___, estimate = ___)\n\n\n\nSolution\n\nAgain, each group will have slightly different answers here because they have different samples of data.\n\n# Calculate the R^2 for model_1\nmodel_1 %&gt;%\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.125         0.101  2.26      5.29  0.0272     1  -86.1  178.  183.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Use your model to predict height for your subjects\n# Just print the first 6 results\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  head()\n\n# A tibble: 6 × 21\n  .pred .resid fatBrozek body_fat density   age weight height adiposity\n  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  70.3  -1.09      24.7     25.4    1.04    43   177    69.2      26  \n2  70.8  -1.52      22       22.5    1.05    38   187.   69.2      27.5\n3  70.2  -1.19       9.4      8.8    1.08    29   161.   69        23.8\n4  68.9   4.58       7.1      6.3    1.08    49   153.   73.5      19.9\n5  69.3   2.91       9.9      9.4    1.08    23   160.   72.2      21.6\n6  70.2  -2.48      22.7     23.3    1.05    52   167    67.8      25.6\n# ℹ 12 more variables: fatFreeWeight &lt;dbl&gt;, neck &lt;dbl&gt;, chest &lt;dbl&gt;,\n#   abdomen &lt;dbl&gt;, hip &lt;dbl&gt;, thigh &lt;dbl&gt;, knee &lt;dbl&gt;, ankle &lt;dbl&gt;,\n#   biceps &lt;dbl&gt;, forearm &lt;dbl&gt;, wrist &lt;dbl&gt;, hipin &lt;dbl&gt;\n\n# Calculate the MAE, i.e. typical prediction error, for your model\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.88\n\n\n\n\nReflection\nIn addition to hip circumference, suppose we incorporated more predictors into our model of height. What would happen to \\(R^2\\)? To the MAE?\n\n\nSolution\n\n\\(R^2\\) would increase and MAE would decrease.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#directions-1",
    "href": "L03-overfitting.html#directions-1",
    "title": "3  Overfitting",
    "section": "Directions",
    "text": "Directions\n\nTake 5 minutes to complete exercises 1 and 2 (choosing one of three models).\nWe’ll pause for a few minutes to discuss each group’s answers to these exercises.\nThen, and only then, you can finish exercises 3 - 5.\n\nREMINDERS:\n\nBe kind to yourself/each other. You will make mistakes!\nCollaborate:\n\nactively contribute to discussion (don’t work on your own)\nactively include all group members in discussion\ncreate a space where others feel comfortable making mistakes and sharing their ideas\nstay in sync",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#questions-1",
    "href": "L03-overfitting.html#questions-1",
    "title": "3  Overfitting",
    "section": "Questions",
    "text": "Questions\n\nSelect a model\n\nConsider 3 different models of height, estimated below. As a group, use your data to choose which is the best predictive model of height. Calculate the MAE for this model.\n\n# height vs hip\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip, data = humans)\nmodel_1 %&gt;% \n  tidy()\n\n# height vs hip & weight\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight, data = humans)\nmodel_2 %&gt;% \n  tidy()\n\n# height vs a lot of predictors (AND some interaction terms)\nmodel_3 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight * body_fat * abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\nmodel_3 %&gt;% \n  tidy()\n\n\n# Calculate the MAE for your model\n___ %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n\n\nSolution\n\nWill vary by group. MAE is calculated here for each model.\n\n# Build the models\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip, data = humans)\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight, data = humans)\nmodel_3 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight * body_fat * abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\n\n# Evaluate the models\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.88\n\nmodel_2 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.67\n\nmodel_3 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard    1.53e-10\n\n\n\n\n\nShare your results\nOnly when you’re done with exercise 1:\n\nOpen this “Top Model Competition” Google Doc.\nRecord your team name.\nRecord which model you chose (1, 2, or 3).\nRecord the MAE for your model.\nWAIT. Don’t keep going.\n\n\n\nDon’t peak\nWhat do you know?! 40 new people just walked into the doctor’s office and the doctor wants to predict their height:\n\n# Import the new data\nnew_patients &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat182.csv\") %&gt;% \n  filter(ankle &lt; 30) %&gt;% \n  rename(body_fat = fatSiri)\n\n\n\n\n\nIntuition\nConsider using your model to predict height for these 40 new subjects. On average, do you think these predictions will be better or worse than for your original patients? Why?\n\n\n\n\n\nHow well does your model do in the real world?\nUse your model to predict height for the new patients and calculate the typical prediction error (MAE). Record this in the Google sheet.\n\n\n___ %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n\n\nSolution\n\n\n# Predict height (assume, for example, I choose model_1)\nmodel_1 %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  head()\n\n# A tibble: 6 × 21\n  .pred .resid fatBrozek body_fat density   age weight height adiposity\n  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  71.5 -1.96       27.1     28      1.04    62   201.   69.5      29.3\n2  70.4 -0.141      20.9     21.3    1.05    42   163    70.2      23.3\n3  69.7 -0.497      26.1     27      1.04    72   168    69.2      24.7\n4  69.1 -1.39        4.1      3      1.09    35   152.   67.8      23.4\n5  68.5 -2.99        1.9      0.7    1.1     35   126.   65.5      20.6\n6  71.9 -1.91       31       32.3    1.03    57   206.   70        29.5\n# ℹ 12 more variables: fatFreeWeight &lt;dbl&gt;, neck &lt;dbl&gt;, chest &lt;dbl&gt;,\n#   abdomen &lt;dbl&gt;, hip &lt;dbl&gt;, thigh &lt;dbl&gt;, knee &lt;dbl&gt;, ankle &lt;dbl&gt;,\n#   biceps &lt;dbl&gt;, forearm &lt;dbl&gt;, wrist &lt;dbl&gt;, hipin &lt;dbl&gt;\n\n\n\n# Calculate the MAE for model_1\nmodel_1 %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.73\n\n# Calculate the MAE for model_2\nmodel_2 %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.68\n\n# Calculate the MAE for model_3\nmodel_3 %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        105.\n\n\n\n\n\nReflection\nIn summary, which model seems best? What’s the central theme here?",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#overfitting",
    "href": "L03-overfitting.html#overfitting",
    "title": "3  Overfitting",
    "section": "Overfitting",
    "text": "Overfitting\nWhen we add more and more predictors into a model, it can become overfit to the noise in our sample data:\n\nour model loses the broader trend / big picture\nthus does not generalize to new data\nthus results in bad predictions and a bad understanding of the relationship among the new data points\n\nPreventing overfitting: training and testing\n\nIn-sample metrics, i.e. measures of how well the model performs on the same sample data that we used to build it, tend to be overly optimistic and lead to overfitting.\nInstead, we should build and evaluate, or train and test, our model using different data.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#r-code",
    "href": "L03-overfitting.html#r-code",
    "title": "3  Overfitting",
    "section": "R Code",
    "text": "R Code\nThis section is for future reference. It is a summary of code you’ll learn below for creating and applying training and testing data. Throughout, suppose we wish to build and evaluate a linear regression model of y vs x1 and x2 using our sample_data.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nSplit the sample data into training and test sets\n\n# Set the random number seed\nset.seed(___)\n\n# Split the sample_data\n# \"prop\" is the proportion of data assigned to the training set\n# it must be some number between 0 and 1\ndata_split &lt;- initial_split(sample_data, strata = y, prop = ___)\n\n# Get the training data from the split\ndata_train &lt;- data_split %&gt;% \n  training()\n\n# Get the testing data from the split\ndata_test &lt;- data_split %&gt;% \n  testing()\n\nBuild a training model\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: model estimation using the training data\nmodel_train &lt;- lm_spec %&gt;% \n  fit(y ~ x1 + x2, data = data_train)\n\nUse the training model to make predictions for the test data\n\n# Make predictions\nmodel_train %&gt;% \n  augment(new_data = data_test)\n\nEvaluate the training model using the test data\n\n# Calculate the test MAE\nmodel_train %&gt;% \n  augment(new_data = data_test) %&gt;% \n  mae(truth = y, estimate = .pred)",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#directions-2",
    "href": "L03-overfitting.html#directions-2",
    "title": "3  Overfitting",
    "section": "Directions",
    "text": "Directions\n\nOpen the Part 2 QMD file\nSame directions as before:\n\nBe kind to yourself/each other\nCollaborate\n\nWe will not discuss these exercises as a class. Be sure to ask questions as I walk around the room.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#questions-2",
    "href": "L03-overfitting.html#questions-2",
    "title": "3  Overfitting",
    "section": "Questions",
    "text": "Questions\nThe following exercises are inspired by Chapter 5.3.1 of ISLR.\n\n# Load packages & data\n# NOTE: You might first need to install the ISLR package\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR)\ndata(Auto)\ncars &lt;- Auto %&gt;% \n  dplyr::select(mpg, horsepower, year)\n\nLet’s use the cars data to compare three linear regression models of fuel efficiency in miles per gallon (mpg) by engine power (horsepower):\n\n# Raw data\ncars_plot &lt;- ggplot(cars, aes(x = horsepower, y = mpg)) + \n  geom_point()\ncars_plot\n\n\n# model 1: 1 predictor (y = b0 + b1 x)\ncars_plot + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n# model 2: 2 predictors (y = b0 + b1 x + b2 x^2)\ncars_plot + \n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2))\n\n\n# model 3: 19 predictors (y = b0 + b1 x + b2 x^2 + ... + b19 x^19)\ncars_plot + \n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 19))\n\nGoal\nLet’s evaluate and compare these models by training and testing them using different data.\n\n155 review: set.seed()\n\nRun the two chunks below multiple times each. Afterward, summarize what set.seed() does and why it’s important to being able to reproduce a random sample.\n\nsample_n(cars, 2)\n\n\nset.seed(253)\nsample_n(cars, 2)\n\n\n\nSolution\n\nset.seed() is used to create the same “random numbers” each time a random function is called.\nNote that is if you want to get exactly the same random result, set.seed() needs to be run right before the call to random function, every time.\nIt is important so that you can reproduce the same random sample every time you knit your work.\nThere might be different results across computers/platforms as they might be using different pseudo-random number generators. The most important thing is for your code to be consistent.\n\n\n\nTraining and test sets\n\nLet’s randomly split our original 392 sample cars into two separate pieces: select 80% of the cars to train (build) the model and the other 20% to test (evaluate) the model.\n\n# Set the random number seed\nset.seed(8)\n    \n# Split the cars data into 80% / 20%\n# Ensure that the sub-samples are similar with respect to mpg\ncars_split &lt;- initial_split(cars, strata = mpg, prop = 0.8)\n\n\n# Check it out\n# What do these numbers mean?\ncars_split\n\n\n# Get the training data from the split\ncars_train &lt;- cars_split %&gt;% \n  training()\n    \n# Get the testing data from the split\ncars_test &lt;- cars_split %&gt;% \n  testing()\n\n\n# The original data has 392 cars\nnrow(cars)\n    \n# How many cars are in cars_train?\n    \n# How many cars are in cars_test?\n\n\n\nSolution\n\n\n# Set the random number seed\nset.seed(8)\n\n# Split the cars data into 80% / 20%\n# Ensure that the sub-samples are similar with respect to mpg\ncars_split &lt;- initial_split(cars, strata = mpg, prop = 0.8)\ncars_split\n\n&lt;Training/Testing/Total&gt;\n&lt;312/80/392&gt;\n\n# Get the training data from the split\ncars_train &lt;- cars_split %&gt;% \n  training()\n\n# Get the testing data from the split\ncars_test &lt;- cars_split %&gt;% \n  testing()\n\n# The original data has 392 cars\nnrow(cars)\n\n[1] 392\n\n# How many cars are in cars_train?\nnrow(cars_train)\n\n[1] 312\n\n# How many cars are in cars_test?\nnrow(cars_test)\n\n[1] 80\n\n\n\n\n\nReflect on the above code\n\n\nWhy do we want the training and testing data to be similar with respect to mpg (strata = mpg)? What if they weren’t?\nWhy did we need all this new code instead of just using the first 80% of cars in the sample for training and the last 20% for testing?\n\n\n\nSolution\n\n\nSuppose, for example, the training cars all had higher mpg than the test cars. Then the training model likely would not perform well on the test cars, thus we’d get an overly pessimistic measure of model quality.\nIf the cars are ordered in some way (eg: from biggest to smallest) then our training and testing samples would have systematically different properties.\n\n\n\n\nBuild the training model\n\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: model estimation using the training data\n# Construct the 19th order polynomial model using the TRAINING data\nmodel_19_train &lt;- ___ %&gt;% \n  ___(mpg ~ poly(horsepower, 19), data = ___)\n\n\n\nSolution\n\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \nset_mode(\"regression\") %&gt;% \nset_engine(\"lm\")\n\n# STEP 2: model estimation using the training data\n# Construct the 19th order polynomial model using the TRAINING data\nmodel_19_train &lt;- lm_spec %&gt;% \nfit(mpg ~ poly(horsepower, 19), data = cars_train)\n\n\n\n\nEvaluate the training model\n\n\n# How well does the TRAINING model predict the TRAINING data?\n# Calculate the training (in-sample) MAE\nmodel_19_train %&gt;% \n  augment(new_data = ___) %&gt;% \n  mae(truth = mpg, estimate = .pred)\n\n\n# How well does the TRAINING model predict the TEST data?\n# Calculate the test MAE\nmodel_19_train %&gt;% \n  augment(new_data = ___) %&gt;% \n  mae(truth = mpg, estimate = .pred)\n\n\n\nSolution\n\n\n# How well does the training model predict the training data?\n# Calculate the training (in-sample) MAE\nmodel_19_train %&gt;% \naugment(new_data = cars_train) %&gt;% \nmae(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        2.99\n\n# How well does the training model predict the test data?\n# Calculate the test MAE\nmodel_19_train %&gt;% \naugment(new_data = cars_test) %&gt;% \nmae(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        6.59\n\n\n\n\n\nPunchline\nThe table below summarizes your results for train_model_19 as well as the other two models of interest. (You should confirm the other two model results outside of class!)\n\n\n\n\nModel\nTraining MAE\nTesting MAE\n\n\n\n\nmpg ~ horsepower\n3.78\n4.00\n\n\nmpg ~ poly(horsepower, 2)\n3.20\n3.49\n\n\nmpg ~ poly(horsepower, 19)\n2.99\n6.59\n\n\n\nAnswer the following and reflect on why each answer makes sense:\n\nWithin each model, how do the training errors compare to the testing errors? (This isn’t always the case, but is common.)\n\nWhy about the training and test errors for the third model suggest that it is overfit to our sample data?\nWhich model seems the best with respect to the training errors?\n\nWhich model is the best with respect to the testing errors?\n\nWhich model would you choose?\n\n\n\nSolution\n\n\nthe training errors are smaller\n\nthe test MAE is much larger than the training MAE\nthe 19th order polynomial\n\nthe quadratic\n\nthe quadratic\n\nCode for the curious\nI wrote a function calculate_MAE() to automate the calculations in the table. If you’re curious, pick through this code!\n\n# Write function to calculate MAEs\ncalculate_MAE &lt;- function(poly_order){\n  # Construct a training model\n  model &lt;- lm_spec %&gt;% \n    fit(mpg ~ poly(horsepower, poly_order), cars_train)\n  \n  # Calculate the training MAE\n  train_it &lt;- model %&gt;% \n    augment(new_data = cars_train) %&gt;% \n    mae(truth = mpg, estimate = .pred)\n      \n  # Calculate the testing MAE\n  test_it &lt;- model %&gt;% \n    augment(new_data = cars_test) %&gt;% \n    mae(truth = mpg, estimate = .pred)\n      \n  # Return the results\n  return(data.frame(train_MAE = train_it$.estimate, test_MAE = test_it$.estimate))\n}\n    \n# Calculate training and testing MSEs\ncalculate_MAE(poly_order = 1)\n\n  train_MAE test_MAE\n1  3.779331 4.004333\n\ncalculate_MAE(poly_order = 2)\n\n  train_MAE test_MAE\n1  3.199882 3.487022\n\ncalculate_MAE(poly_order = 19)\n\n  train_MAE test_MAE\n1  2.989305 6.592341\n\n\n\n# For those of you interested in trying all orders...\n\nresults &lt;- purrr::map_df(1:19,calculate_MAE) %&gt;% \n  mutate(order = 1:19) %&gt;%\n  pivot_longer(cols=1:2,names_to='Metric',values_to = 'MAE') \n\nresults %&gt;%\n  ggplot(aes(x = order, y = MAE, color = Metric)) + \n  geom_line() + \n  geom_point(data = results %&gt;% filter(Metric == 'test_MAE') %&gt;% slice_min(MAE)) + \n  geom_point(data = results %&gt;% filter(Metric == 'train_MAE') %&gt;% slice_min(MAE))\n\n\n\n\n\n\n\n\n\n\n\nFinal reflection\n\nThe training / testing procedure provided a more honest evaluation and comparison of our model predictions. How might we improve upon this procedure? What problems can you anticipate in splitting our data into 80% / 20%?\nSummarize the key themes from today in your own words.\n\n\n\n\n\n\nSolution\n\nThis will be discussed in the next video!\n\n\n\nSTAT 155 REVIEW: data drill\n\n\nConstruct and interpret a plot of mpg vs horsepower and year.\nCalculate the average mpg.\nCalculate the average mpg for each year. HINT: group_by()\nPlot the average mpg by year.\n\n\n\nSolution\n\n\n# a. One of many options\nggplot(cars, aes(x = horsepower, y = mpg, color = year)) + \n  geom_point()\n\n\n\n\n\n\n\n# b\ncars %&gt;% \nsummarize(mean(mpg))\n\n  mean(mpg)\n1  23.44592\n\n# c\ncars %&gt;% \n  group_by(year) %&gt;% \n  summarize(mean_mpg = mean(mpg))\n\n# A tibble: 13 × 2\n    year mean_mpg\n   &lt;dbl&gt;    &lt;dbl&gt;\n 1    70     17.7\n 2    71     21.1\n 3    72     18.7\n 4    73     17.1\n 5    74     22.8\n 6    75     20.3\n 7    76     21.6\n 8    77     23.4\n 9    78     24.1\n10    79     25.1\n11    80     33.8\n12    81     30.2\n13    82     32  \n\n# d\ncars %&gt;% \n  group_by(year) %&gt;% \n  summarize(mean_mpg = mean(mpg)) %&gt;% \n  ggplot(aes(y = mean_mpg, x = year)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nDigging deeper (optional)\n\nCheck out the online solutions for exercise 6. Instead of calculating MAE from scratch for 3 different models, I wrote a function calculate_MAE() to automate the process. After picking through this code, adapt the function so that it also returns the \\(R^2\\) value of each model.\nDone!\n\nKnit your notes.\nCheck the solutions in the course website.\nIf you finish all that during class, start your homework!",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#finishing-the-activity",
    "href": "L03-overfitting.html#finishing-the-activity",
    "title": "3  Overfitting",
    "section": "Finishing the Activity",
    "text": "Finishing the Activity\n\nIf you didn’t finish the activity, no problem! Be sure to complete the activity outside of class, review the solutions in the online manual, and ask any questions on Slack or in office hours.\nRe-organize and review your notes to help deepen your understanding, solidify your learning, and make homework go more smoothly!",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#after-class",
    "href": "L03-overfitting.html#after-class",
    "title": "3  Overfitting",
    "section": "After Class",
    "text": "After Class\n\nAn R code video, posted under the pre-course materials for today’s class (see the “Schedule” page on this website), talks through the new code. This video is OPTIONAL. Decide what’s right for you.\nContinue to check in on Slack. I’ll be posting announcements there from now on.\nUpcoming due dates:\n\nCP3: due 10 minutes before our next class.\n\nThere are two (short) videos to watch in advance.\n\nHW1 (Regression Model Evaluation): due next Tuesday at 11:59 pm\n\nStart today, even if you just review the directions and scan the exercises. Homework is not designed to be completed in one sitting!\nInvite others to work with you!\n\nStop by office hours (preceptors or mine) with any questions",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html",
    "href": "L04-cross-validation.html",
    "title": "4  Cross-Validation",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#context-evaluating-regression-models",
    "href": "L04-cross-validation.html#context-evaluating-regression-models",
    "title": "4  Cross-Validation",
    "section": "Context: Evaluating Regression Models",
    "text": "Context: Evaluating Regression Models\nA reminder of our current context:\n\n\n\n\nworld = supervised learning\nWe want to build a model some output variable \\(y\\) by some predictors \\(x\\).\ntask = regression\n\\(y\\) is quantitative\nmodel = linear regression model via least squares algorithm\nWe’ll assume that the relationship between \\(y\\) and \\(x\\) can be represented by\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\varepsilon\\]\n\n\nGOAL: model evaluation\nWe want more honest metrics of prediction quality that\n\nassess how well our model predicts new outcomes; and\nhelp prevent overfitting.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#why-is-overfitting-so-bad",
    "href": "L04-cross-validation.html#why-is-overfitting-so-bad",
    "title": "4  Cross-Validation",
    "section": "Why is overfitting so bad?",
    "text": "Why is overfitting so bad?\nNot only can overfitting produce misleading models, it can have serious societal impacts. Examples:\n\nA former Amazon algorithm built to help sift through resumes was overfit to its current employees in leadership positions (who weren’t representative of the general population or candidate pool).\nFacial recognition algorithms are often overfit to the people who build them (who are not broadly representative of society). As one example, this has led to disproportionate bias in policing. For more on this topic, you might check out Coded Bias, a documentary by Shalini Kantayya which features MIT Media Lab researcher Joy Buolamwini.\nPolygenic risk scores (PRSs), which aim to predict a person’s risk of developing a particular disease/trait based on their genetics, are often overfit to the data on which they are built (which, historically, has exclusively—or at least primarily—included individuals of European ancestry). As a result, PRS predictions tend to be more accurate in European populations and new research suggests that their continued use in clinical settings could exacerbate health disparities.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#k-fold-cross-validation",
    "href": "L04-cross-validation.html#k-fold-cross-validation",
    "title": "4  Cross-Validation",
    "section": "k-fold Cross Validation",
    "text": "k-fold Cross Validation\nWe can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\n\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\n\nFit a model using the data in the other \\(k-1\\) folds (training).\n\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\n\nCalculate the MAE for fold \\(j\\) (testing): \\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\).\n\nCombine this information into one measure of model quality: \\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#definitions",
    "href": "L04-cross-validation.html#definitions",
    "title": "4  Cross-Validation",
    "section": "Definitions",
    "text": "Definitions\n\nalgorithm = a step-by-step procedure for solving a problem (Merriam-Webster)\ntuning parameter = a parameter or quantity upon which an algorithm depends, that must be selected or tuned to “optimize” the algorithm\n\n1",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#prompts",
    "href": "L04-cross-validation.html#prompts",
    "title": "4  Cross-Validation",
    "section": "Prompts",
    "text": "Prompts\n\nAlgorithms\n\n\nWhy is \\(k\\)-fold cross-validation an algorithm?\nWhat is the tuning parameter of this algorithm and what values can this take?\n\n\n\nSolution\n\n\nYes. It follows a list of steps to get to its goal.\n\\(k\\), the number of folds, is a tuning parameter. \\(k\\) can be any integer from 1, 2, …, \\(n\\) where \\(n\\) is our sample size.\n\n\n\n\nTuning the k-fold Cross-Validation algorithm\n\n\n\n\n\nLet’s explore k-fold cross-validation with some personal experience. Our class has a representative sample of cards from a non-traditional population (no “face cards”, not equal numbers, etc). We want to use these to predict whether a new card will be odd or even (a classification task).\n\nBased on all of our cards, do we predict the next card will be odd or even?\nYou’ve been split into 2 groups. Use 2-fold cross-validation to estimate the possible error of using our sample of cards to predict whether a new card will be odd or even. How’s this different than validation?\nRepeat for 3-fold cross-validation. Why might this be better than 2-fold cross-validation?\nRepeat for LOOCV, i.e. n-fold cross-validation where n is the number of students in this room. Why might this be worse than 3-fold cross-validation?\nWhat value of k do you think practitioners typically use?\n\n\n\nSolution\n\n\nUse the percentage of odd and percentage of even among the sample of cards to help you make a prediction.\nWe use both groups as training and testing, in turn.\nWe have a larger dataset to train our model on. We are less likely to get an unrepresentative set as our training data.\nPrediction error for 1 person is highly variable.\nIn practice, \\(k = 10\\) and \\(k=7\\) are common choices for cross-validation. This has been shown to hit the ‘sweet spot’ between the extremes of \\(k=n\\) (LOOCV) and \\(k=2\\).\n\n\n\\(k=2\\) only utilizes 50% of the data for each training model, thus might result in overestimating the prediction error\n\\(k=n\\) leave-one-out cross-validation (LOOCV) requires us to build \\(n\\) training models, thus might be computationally expensive for larger sample sizes \\(n\\). Further, with only one data point in each test set, the training sets have a lot of overlap. This correlation among the training sets can make the ultimate corresponding estimate of prediction error less reliable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Code Preview\n\nWe’ve been doing a 2-step process to build linear regression models using the tidymodels package:\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n  \n# STEP 2: model estimation\nmy_model &lt;- lm_spec %&gt;% \n  fit(\n    y ~ x1 + x2,\n    data = sample_data\n  )\n\nFor k-fold cross-validation, we can tweak STEP 2.\n\nDiscuss the code below and why we need to set the seed.\n\n\n# k-fold cross-validation\nset.seed(___)\nmy_model_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    y ~ x1 + x2, \n    resamples = vfold_cv(sample_data, v = ___), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n\nSolution\n\nThe process of creating the folds is random, so we should set the seed to have reproducibility within our work.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#obtain-k-fold-cross-validated-estimates-of-mae-and-r2",
    "href": "L04-cross-validation.html#obtain-k-fold-cross-validated-estimates-of-mae-and-r2",
    "title": "4  Cross-Validation",
    "section": "Obtain k-fold cross-validated estimates of MAE and \\(R^2\\)",
    "text": "Obtain k-fold cross-validated estimates of MAE and \\(R^2\\)\n(Review above for discussion of these steps.)\n\n# model specification\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# k-fold cross-validation\n# For \"v\", put your number of folds k\nset.seed(___)\nmodel_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    y ~ x1 + x2,\n    resamples = vfold_cv(sample_data, v = ___), \n    metrics = metric_set(mae, rsq)\n)",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#obtain-the-cross-validated-metrics",
    "href": "L04-cross-validation.html#obtain-the-cross-validated-metrics",
    "title": "4  Cross-Validation",
    "section": "Obtain the cross-validated metrics",
    "text": "Obtain the cross-validated metrics\n\nmodel_cv %&gt;% \n  collect_metrics()",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#get-the-mae-and-r-squared-for-each-test-fold",
    "href": "L04-cross-validation.html#get-the-mae-and-r-squared-for-each-test-fold",
    "title": "4  Cross-Validation",
    "section": "Get the MAE and R-squared for each test fold",
    "text": "Get the MAE and R-squared for each test fold\n\n# MAE for each test fold: Model 1\nmodel_cv %&gt;% \n  unnest(.metrics)",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#instructions",
    "href": "L04-cross-validation.html#instructions",
    "title": "4  Cross-Validation",
    "section": "Instructions",
    "text": "Instructions\n\nGo to the Course Schedule and find the QMD template for today\n\nSave this in your STAT 253 Notes folder, NOT your downloads!\n\nWork through the exercises implementing CV to compare two possible models predicting height\nSame directions as before:\n\nBe kind to yourself/each other\nCollaborate\nDON’T edit starter code (i.e., code with blanks ___). Instead, copy-paste into a new code chunk below and edit from there.\n\nAsk me questions as I move around the room",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#questions",
    "href": "L04-cross-validation.html#questions",
    "title": "4  Cross-Validation",
    "section": "Questions",
    "text": "Questions\n\n# Load packages and data\nlibrary(tidyverse)\nlibrary(tidymodels)\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat50.csv\") %&gt;% \n  filter(ankle &lt; 30) %&gt;% \n  rename(body_fat = fatSiri)\n\n\n\n\n\nReview: In-sample metrics\n\nUse the humans data to build two separate models of height:\n\n# STEP 1: model specification\nlm_spec &lt;- ___() %&gt;% \n  set_mode(___) %&gt;% \n  set_engine(___)\n\n\n# STEP 2: model estimation\nmodel_1 &lt;- ___ %&gt;% \n  ___(height ~ hip + weight + thigh + knee + ankle, data = humans)\nmodel_2 &lt;- ___ %&gt;% \n  ___(height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\n\nCalculate the in-sample R-squared for both models:\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% \n  ___()\n\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% \n  ___()\n\nCalculate the in-sample MAE for both models:\n\n# IN-SAMPLE MAE for model_1 = ???\nmodel_1 %&gt;% \n  ___(new_data = ___) %&gt;% \n  mae(truth = ___, estimate = ___)\n\n\n# IN-SAMPLE MAE for model_2 = ???\nmodel_2 %&gt;% \n  ___(new_data = ___) %&gt;% \n  mae(truth = ___, estimate = ___)\n\n\n\nSolution\n\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight + thigh + knee + ankle, data = humans)\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\n\n# IN-SAMPLE R^2 for model_1 = 0.40\nmodel_1 %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.401         0.310  1.98      4.42 0.00345     5  -78.8  172.  183.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# IN-SAMPLE R^2 for model_2 = 0.87\nmodel_2 %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.680  1.35      4.51 0.00205    23  -48.4  147.  188.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# IN-SAMPLE MAE for model_1 = 1.55\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.55\n\n# IN-SAMPLE MAE for model_2 = 0.64\nmodel_2 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.646\n\n\n\n\n\nIn-sample model comparison\nWhich model seems “better” by the in-sample metrics you calculated above? Any concerns about either of these models?\n\n\n\nSolution\n\nThe in-sample metrics are better for model_2, but from experience in our previous class, we should expect this to be overfit.\n\n\n\n10-fold CV\nComplete the code to run 10-fold cross-validation for our two models.\nmodel_1: height ~ hip + weight\nmodel_2: height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist\n\n\n# 10-fold cross-validation for model_1\nset.seed(253)\nmodel_1_cv &lt;- ___ %&gt;% \n  ___(\n    ___,\n    ___ = vfold_cv(___, v = ___), \n    ___ = metric_set(mae, rsq)\n  )\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- ___ %&gt;% \n  ___(\n    ___,\n    ___ = vfold_cv(___, v = ___), \n    ___ = metric_set(mae, rsq)\n  )\n\n\n\nSolution\n\n\n# 10-fold cross-validation for model_1\nset.seed(253)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(humans, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n# STEP 2: 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(humans, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n\n\nCalculating the CV MAE\n\n\nUse collect_metrics() to obtain the cross-validated MAE and \\(R^2\\) for both models.\n\n\n# HINT\n___ %&gt;% \n  collect_metrics()\n\n\nInterpret the cross-validated MAE and \\(R^2\\) for model_1.\n\n\n\nSolution\n\n\n\n\n\n# model_1\n# CV MAE = 1.87, CV R-squared = 0.41\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   1.87     10   0.159 Preprocessor1_Model1\n2 rsq     standard   0.409    10   0.124 Preprocessor1_Model1\n\n# model_2\n# CV MAE = 2.47, CV R-squared = 0.53\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   2.47     10   0.396 Preprocessor1_Model1\n2 rsq     standard   0.526    10   0.122 Preprocessor1_Model1\n\n\n\nWe expect our first model to explain roughly 40% of variability in height among new adults, and to produce predictions of height that are off by 1.9 inches on average.\n\n\n\n\nDetails: fold-by-fold results\ncollect_metrics() gave the final CV MAE, or the average MAE across all 10 test folds. unnest(.metrics) provides the MAE from each test fold.\n\n\nObtain the fold-by-fold results for the model_1 cross-validation procedure using unnest(.metrics).\n\n\n# HINT\n___ %&gt;% \n  unnest(.metrics)\n\n\nWhich fold had the worst average prediction error and what was it?\nRecall that collect_metrics() reported a final CV MAE of 1.87 for model_1. Confirm this calculation by wrangling the fold-by-fold results from part a.\n\n\n\nSolution\n\n\n# a. model_1 MAE for each test fold\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;% \n  filter(.metric == \"mae\")\n\n# A tibble: 10 × 7\n   splits         id     .metric .estimator .estimate .config           .notes  \n   &lt;list&gt;         &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n 1 &lt;split [35/4]&gt; Fold01 mae     standard        2.22 Preprocessor1_Mo… &lt;tibble&gt;\n 2 &lt;split [35/4]&gt; Fold02 mae     standard        2.34 Preprocessor1_Mo… &lt;tibble&gt;\n 3 &lt;split [35/4]&gt; Fold03 mae     standard        2.56 Preprocessor1_Mo… &lt;tibble&gt;\n 4 &lt;split [35/4]&gt; Fold04 mae     standard        1.51 Preprocessor1_Mo… &lt;tibble&gt;\n 5 &lt;split [35/4]&gt; Fold05 mae     standard        1.81 Preprocessor1_Mo… &lt;tibble&gt;\n 6 &lt;split [35/4]&gt; Fold06 mae     standard        2.43 Preprocessor1_Mo… &lt;tibble&gt;\n 7 &lt;split [35/4]&gt; Fold07 mae     standard        1.61 Preprocessor1_Mo… &lt;tibble&gt;\n 8 &lt;split [35/4]&gt; Fold08 mae     standard        1.84 Preprocessor1_Mo… &lt;tibble&gt;\n 9 &lt;split [35/4]&gt; Fold09 mae     standard        1.28 Preprocessor1_Mo… &lt;tibble&gt;\n10 &lt;split [36/3]&gt; Fold10 mae     standard        1.10 Preprocessor1_Mo… &lt;tibble&gt;\n\n# b. fold 3 had the worst error (2.55)\n\n# c. use these metrics to confirm the 1.87 CV MAE for model_1\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;% \n  filter(.metric == \"mae\") %&gt;% \n  summarize(mean(.estimate))\n\n# A tibble: 1 × 1\n  `mean(.estimate)`\n              &lt;dbl&gt;\n1              1.87\n\n\n\n\n\n\n\n\nComparing models\nThe table below summarizes the in-sample and 10-fold CV MAE for both models.\n\n\n\n\n\nModel\nIN-SAMPLE MAE\n10-fold CV MAE\n\n\n\n\nmodel_1\n1.55\n1.87\n\n\nmodel_2\n0.64\n2.47\n\n\n\n\n\n\n\nBased on the in-sample MAE alone, which model appears better?\n\nBased on the CV MAE alone, which model appears better?\n\nBased on all of these results, which model would you pick?\nDo the in-sample and CV MAE suggest that model_1 is overfit to our humans sample data? What about model_2?\n\n\n\nSolution\n\n\nmodel_2\nmodel_1\nmodel_1 – model_2 produces bad predictions for new adults\nmodel_1 is NOT overfit – its predictions of height for new adults seem roughly as accurate as the predictions for the adults in our sample. model_2 IS overfit – its predictions of height for new adults are worse than the predictions for the adults in our sample.\n\n\n\n\nLOOCV\n\nReconsider model_1. Instead of estimating its prediction accuracy using the 10-fold CV MAE, use the LOOCV MAE. THINK: How many people are in our humans sample?\nHow does the LOOCV MAE compare to the 10-fold CV MAE of 1.87? NOTE: These are just two different approaches to estimating the same thing: the typical prediction error when applying our model to new data. Thus we should expect them to be similar.\n\nExplain why we technically don’t need to set.seed() for the LOOCV algorithm.\n\n\n\n\nSolution\n\n\nThere are 40 people in our sample, thus LOOCV is equivalent to 40-fold CV:\n\n\nnrow(humans)\n\n[1] 39\n\nmodel_1_loocv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(humans, v = nrow(humans)), \n    metrics = metric_set(mae)\n  )\n    \nmodel_1_loocv %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    1.82    39   0.179 Preprocessor1_Model1\n\n\n\nThe LOOCV MAE (1.82) is very similar to the 10-fold CV MAE (1.87).\nThere’s no randomness in the test folds. Each test fold is a single person.\n\n\n\n\nData drill\n\n\nCalculate the average height of people under 40 years old vs people 40+ years old.\nPlot height vs age among our subjects that are 30+ years old.\nFix this code:\n\n\nmodel_3&lt;-lm_spec%&gt;%fit(height~age,data=humans)\nmodel_3%&gt;%tidy()\n\n\n\nSolution\n\n\n# a (one of many solutions)\nhumans %&gt;% \n  mutate(younger_older = age &lt; 40) %&gt;% \n  group_by(younger_older) %&gt;% \n  summarize(mean(height))\n\n# A tibble: 2 × 2\n  younger_older `mean(height)`\n  &lt;lgl&gt;                  &lt;dbl&gt;\n1 FALSE                   70.4\n2 TRUE                    69.8\n\n# b\nhumans %&gt;% \n  filter(age &gt;= 30) %&gt;% \n  ggplot(aes(x = age, y = height)) + \n  geom_point()\n\n\n\n\n\n\n\n# c\nmodel_3 &lt;- lm_spec %&gt;%\n  fit(height ~ age, data = humans)\nmodel_3 %&gt;%\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  71.1       1.63      43.7   1.96e-33\n2 age          -0.0210    0.0363    -0.577 5.67e- 1\n\n\n\n\n\nReflection: Part 1\nThe “regular” exercises are over but class is not done! Your group should agree to either work on HW1 or the remaining reflection questions.\nThis is the end of Unit 1 on “Regression: Model Evaluation”! Let’s reflect on the technical content of this unit:\n\n\nWhat was the main motivation / goal behind this unit?\nWhat are the four main questions that were important to this unit?\nFor each of the following tools, describe how they work and what questions they help us address:\n\nR-squared\nresidual plots\nout-of-sample MAE\nin-sample MAE\nvalidation\ncross-validation\n\nIn your own words, define the following:\n\noverfitting\nalgorithm\ntuning parameter\n\nReview the new tidymodels syntax from this unit. Identify key themes and patterns.\n\n\n\n\nReflection: Part 2\nThe reflection above addresses your understanding of/progress toward our course learning goals. Consider the other components that have helped you worked toward this learning throughout Unit 1.\n\nWith respect to collaboration, reflect upon your strengths and what you might change in the next unit:\n\nHow actively did you contribute to group discussions?\nHow actively did you include all other group members in discussion?\nIn what ways did you (or did you not) help create a space where others feel comfortable making mistakes & sharing their ideas?\n\nWith respect to engagement, reflect upon your strengths and what you might change the next unit:\n\nDid you regularly attend, be on time for, & stay for the full class sessions?\nHave you not missed more than 3 in-person class sessions?\nWere you actively present during class (eg: not on your phone, not working on other courses, etc)?\nDid you stay updated on Slack?\nWhen you had questions, did you ask them on Slack or in OH?\n\nWith respect to preparation, how many of checkpoints 1–3 did you complete and pass?\nWith respect to exploration, did you complete and pass HW0? Are you on track to complete and pass HW1?\n\n\nDone!\n\nKnit/render your notes.\nCheck the solutions in the online manual.\nCheck out the wrap-up steps below.\nIf you finish all that during class, work on your homework!",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#todays-material",
    "href": "L04-cross-validation.html#todays-material",
    "title": "4  Cross-Validation",
    "section": "Today’s Material",
    "text": "Today’s Material\n\nIf you didn’t finish the activity, no problem! Be sure to complete the activity outside of class, review the solutions in the course site, and ask any questions on Slack or in office hours.\nThis is the end of Unit 1, so there are reflection questions at the bottom to help you organize the concepts in your mind.\nAn R Tutorial Video, talking through the new code, is posted under the materials for today’s class on the Course Schedule. This video is OPTIONAL. Decide what’s right for you.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#upcoming-deadlines",
    "href": "L04-cross-validation.html#upcoming-deadlines",
    "title": "4  Cross-Validation",
    "section": "Upcoming Deadlines",
    "text": "Upcoming Deadlines\n\nCP4:\n\ndue 10 minutes before our next class\ncovers one R code video\n\nHW1:\n\ndue next Tuesday at 11:59 pm\nstart today if you haven’t already!\nreview the homework and late work/extension policies on Moodle/Syllabus\nuniversal flexibility: pass/revise grading (as long as your original submission meets certain criteria including on-time submission), late work grace period\n\ndeadline is so we can get timely feedback to you; if you cannot make a deadline, please send me an email/Slack DM (in advance!) and let me know how much time you need",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#footnotes",
    "href": "L04-cross-validation.html#footnotes",
    "title": "4  Cross-Validation",
    "section": "",
    "text": "https://www.wallpaperflare.com/grayscale-photography-of-guitar-headstock-music-low-electric-bass-wallpaper-zzbyn↩︎",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "U02-motivation.html",
    "href": "U02-motivation.html",
    "title": "Motivating Question",
    "section": "",
    "text": "Question\nThe field of machine learning is most often associated with the building of predictive models, not inferential models. Specifically, the goal is to build a model which produces good predictions of our response variable \\(y\\), not one that necessarily lends itself to testing specific hypotheses about \\(y\\). In this case:",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "U02-motivation.html#question",
    "href": "U02-motivation.html#question",
    "title": "Motivating Question",
    "section": "",
    "text": "If we have access to a bunch of potential predictors \\(x\\), how can we decide which model to build?",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "U02-motivation.html#model-selection-methods",
    "href": "U02-motivation.html#model-selection-methods",
    "title": "Motivating Question",
    "section": "Model Selection Methods",
    "text": "Model Selection Methods\n\nVariable selection Identify a subset of predictors to use in our model of \\(y\\). Methods: best subset selection, backward stepwise selection, forward stepwise selection\nShrinkage / regularization Shrink / regularize the coefficients of all predictors toward or to 0. Methods: LASSO, ridge regression, elastic net (a combination of LASSO & ridge)\nDimension reduction Combine the predictors into a smaller set of new predictors. Methods: principal components regression",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "L05-model-selection.html",
    "href": "L05-model-selection.html",
    "title": "5  Model Selection",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#statistical-machine-learning-concepts",
    "href": "L05-model-selection.html#statistical-machine-learning-concepts",
    "title": "5  Model Selection",
    "section": "Statistical Machine Learning Concepts",
    "text": "Statistical Machine Learning Concepts\n\nGain intuition about different approaches to variable selection\nClearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms\nCompare best subset and stepwise algorithms in terms of optimality of output and computational time\nDescribe how selection algorithms can give a measure of variable importance",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#general-skills",
    "href": "L05-model-selection.html#general-skills",
    "title": "5  Model Selection",
    "section": "General Skills",
    "text": "General Skills\n\nHighlight: Collaborative Learning\n\nUnderstand and demonstrate characteristics of effective collaboration (team roles, interpersonal communication, self-reflection, awareness of social dynamics, advocating for yourself and others).\nDevelop a common purpose and agreement on goals.\nBe able to contribute questions or concerns in a respectful way.\nShare and contribute to the group’s learning in an equitable manner.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#collaborative-learning",
    "href": "L05-model-selection.html#collaborative-learning",
    "title": "5  Model Selection",
    "section": "Collaborative Learning",
    "text": "Collaborative Learning\nTake 5 minutes to reflect upon your work throughout Unit 1, particularly with respect to collaboration.\nReflect upon your strengths and what you might change in the next unit:\n\nHow actively did you contribute to group discussions?\nHow actively did you include ALL other group members in discussion?\nIn what ways did you (or did you not) help create a space where others feel comfortable making mistakes & sharing their ideas?",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#unit-1-reflection-continued",
    "href": "L05-model-selection.html#unit-1-reflection-continued",
    "title": "5  Model Selection",
    "section": "Unit 1 Reflection (continued)",
    "text": "Unit 1 Reflection (continued)\nIf you did not finish Exercises #9 and #10 from last class, please take time after class today to do so.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#context",
    "href": "L05-model-selection.html#context",
    "title": "5  Model Selection",
    "section": "Context",
    "text": "Context\n\n\n\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = regression\n\\(y\\) is quantitative\nmodel = linear regression\nWe’ll assume that the relationship between \\(y\\) and (\\(x_1, x_2, ..., x_p\\)) can be represented by\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\varepsilon\\]",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#inferential-v.-predictive-models",
    "href": "L05-model-selection.html#inferential-v.-predictive-models",
    "title": "5  Model Selection",
    "section": "Inferential v. Predictive Models",
    "text": "Inferential v. Predictive Models\nIn model building, the decision of which predictors to use depends upon our goal.\nInferential models\n\n\nGoal: Explore & test hypotheses about a specific relationship.\nPredictors: Defined by the goal.\nExample: An economist wants to understand how salaries (\\(y\\)) vary by age (\\(x_1\\)) while controlling for education level (\\(x_2\\)).\n\n\nPredictive models\n\n\nGoal: Produce the “best” possible predictions of \\(y\\).\nPredictors: Any combination of predictors that help us meet this goal.\nExample: A mapping app wants to provide users with quality estimates of arrival time (\\(y\\)) utilizing any useful predictors (eg: time of day, distance, route, speed limit, weather, day of week, traffic radar…)",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#model-selection-goals",
    "href": "L05-model-selection.html#model-selection-goals",
    "title": "5  Model Selection",
    "section": "Model Selection Goals",
    "text": "Model Selection Goals\nModel selection algorithms can help us build a predictive model of \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\nThere are 3 general approaches to this task:\n\n\nVariable selection (today)\nIdentify a subset of predictors to use in our model of \\(y\\).\nShrinkage / regularization (next class)\nShrink / regularize the coefficients of all predictors toward or to 0.\nDimension reduction (later in the semester)\nCombine the predictors into a smaller set of new predictors.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#instructions",
    "href": "L05-model-selection.html#instructions",
    "title": "5  Model Selection",
    "section": "Instructions",
    "text": "Instructions\nOpen the Part 1 QMD. Scroll down to the # Exercises section.\nAs a group, you’ll design a variable selection algorithm to pick which predictors to use in a predictive model of height. Specifically, you will:\n\n15 mins: come up with one algorithm, document it, and try it\n5 mins: try another group’s algorithm\n\nNOTE: This will NOT be perfect! Our goals are to:\n\nHave fun and work together!\nTap into your intuition for key questions and challenges in variable selection.\nDeepen your understanding of “algorithms” and “tuning parameters” by designing and communicating your own.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#questions",
    "href": "L05-model-selection.html#questions",
    "title": "5  Model Selection",
    "section": "Questions",
    "text": "Questions\nLet’s build a predictive model of height in inches using one or more of 12 possible predictors. Other than age and weight, these are circumferences measured in cm.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# Load data\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat1.csv\")\nnames(humans)\n\n [1] \"age\"     \"weight\"  \"neck\"    \"chest\"   \"abdomen\" \"hip\"     \"thigh\"  \n [8] \"knee\"    \"ankle\"   \"biceps\"  \"forearm\" \"wrist\"   \"height\" \n\n\nA heat map displays correlations for each pair of variables in our dataset. Not only is height correlated with multiple predictors, the predictors are correlated with one another (mulicollinear)! We don’t need all of them in our model.\n\n# Get the correlation matrix\nlibrary(reshape2)\ncor_matrix &lt;- cor(humans)\ncor_matrix[lower.tri(cor_matrix)] &lt;- NA\ncor_matrix &lt;- cor_matrix %&gt;% \n  melt() %&gt;% \n  na.omit() %&gt;% \n  rename(correlation = value)\n\n# Visualize the correlation for each pair of variables\nggplot(cor_matrix, aes(x = Var1, y = Var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\", \n    midpoint = 0, limit = c(-1,1)) +\n  labs(x = \"\", y = \"\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\nDesign your own algorithm (15 minutes)\n\nDo not use any materials from outside this class.\nDocument your algorithm in words (not code) in this google doc.\nYour algorithm must:\n\nbe clear to other humans\nbe clear to a machine (cannot utilize context)\nlead to a single model that uses 0-12 of our predictors\ndefine and provide directions for selecting any tuning parameters\n\nImplement as many steps of your algorithm as possible in the time allotted. You can modify the code below to build and evaluate the models in your algorithm:\n\n\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: model estimation\nheight_model_1 &lt;- lm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\n\n# Check it out\nheight_model_1 %&gt;% \n  tidy()\n\n# CV MAE\nset.seed(253)\nlm_spec %&gt;% \n  fit_resamples(\n    height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(humans, v = 10), \n    metrics = metric_set(mae)\n  ) %&gt;% \n  collect_metrics()\n\n\n\nTest another group’s algorithm (5 minutes)\nTry to implement the next algorithm below yours (or the first algorithm if your group’s is last). Think: Are the steps clear? What are the drawbacks to the algorithm?",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#goal",
    "href": "L05-model-selection.html#goal",
    "title": "5  Model Selection",
    "section": "Goal",
    "text": "Goal\nLet’s consider three existing variable selection algorithms.\nHeads up: these algorithms are important to building intuition for the questions and challenges in model selection, BUT have major drawbacks.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-1-best-subset-selection-algorithm",
    "href": "L05-model-selection.html#example-1-best-subset-selection-algorithm",
    "title": "5  Model Selection",
    "section": "Example 1: Best Subset Selection Algorithm",
    "text": "Example 1: Best Subset Selection Algorithm\n\n\nBuild all \\(2^p\\) possible models that use any combination of the available predictors \\((x_1, x_2,..., x_p)\\).\n\nIdentify the best model with respect to some chosen metric (eg: CV MAE) and context.\n\n\nSuppose we used this algorithm for our height model with 12 possible predictors. What’s the main drawback?\n\n\nSolution\n\nIt’s computationally expensive. For our humans example, we’d need to build 4096 models:\n\n2^12\n\n[1] 4096",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-2-backward-stepwise-selection-algorithm",
    "href": "L05-model-selection.html#example-2-backward-stepwise-selection-algorithm",
    "title": "5  Model Selection",
    "section": "Example 2: Backward Stepwise Selection Algorithm",
    "text": "Example 2: Backward Stepwise Selection Algorithm\n\n\nBuild a model with all \\(p\\) possible predictors, \\((x_1, x_2,..., x_p)\\).\n\nRepeat the following until only 1 predictor remains in the model:\n\nRemove the 1 predictor with the biggest p-value.\nBuild a model with the remaining predictors.\n\n\nYou now have \\(p\\) competing models: one with all \\(p\\) predictors, one with \\(p-1\\) predictors, …, and one with 1 predictor. Identify the “best” model with respect to some metric (eg: CV MAE) and context.\n\n\n. . .\nLet’s try out the first few steps!\n. . .\n\n# Load packages and data\nlibrary(tidyverse)\nlibrary(tidymodels)\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat1.csv\")\n\n\n# STEP 1: model specifications\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n\n# STEP 2: model estimate (using all 12 predictors to start)\n# Pick apart this code and make it easier to identify the least \"significant\" predictor!!!\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, \n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n\n# 11 predictors (tweak the code)\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n\n# 10 predictors (tweak the code)\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n\n\nSolution\n\n\n# All 12 predictors\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;%  # use tidy to get p-values for each coefficient\n  filter(term != \"(Intercept)\") %&gt;% # exclude the intercept\n  mutate(p.value = round(p.value, 4)) %&gt;% # round the p-values for easier viewing\n  arrange(desc(p.value)) # added this line to arrange from largest to smallest p-value\n\n# A tibble: 12 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 biceps   -0.0808     0.746    -0.108  0.915 \n 2 neck      0.139      1.17      0.119  0.906 \n 3 knee      0.151      0.941     0.160  0.874 \n 4 wrist     0.836      2.32      0.361  0.721 \n 5 ankle    -0.888      1.28     -0.693  0.494 \n 6 abdomen   0.283      0.354     0.798  0.432 \n 7 age      -0.112      0.132    -0.847  0.405 \n 8 chest    -0.459      0.473    -0.971  0.340 \n 9 forearm   2.25       1.80      1.25   0.223 \n10 weight    0.379      0.213     1.78   0.0864\n11 hip      -0.921      0.510    -1.81   0.0822\n12 thigh    -1.24       0.646    -1.92   0.066 \n\n\n\n# 11 predictors (got rid of biceps)\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4)) %&gt;% \n  arrange(desc(p.value))\n\n# A tibble: 11 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 neck       0.161     1.14      0.142  0.888 \n 2 knee       0.180     0.886     0.203  0.841 \n 3 wrist      0.907     2.18      0.416  0.681 \n 4 ankle     -0.878     1.26     -0.699  0.490 \n 5 abdomen    0.281     0.348     0.809  0.425 \n 6 age       -0.111     0.130    -0.858  0.398 \n 7 chest     -0.453     0.461    -0.982  0.334 \n 8 forearm    2.17      1.62      1.34   0.192 \n 9 hip       -0.902     0.470    -1.92   0.0652\n10 weight     0.369     0.190     1.94   0.0623\n11 thigh     -1.26      0.602    -2.09   0.0454\n\n\n\n# 10 predictors (got rid of neck)\nlm_spec %&gt;% \n  fit(height ~ age + weight  + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4)) %&gt;% \n  arrange(desc(p.value))\n\n# A tibble: 10 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 knee       0.166     0.866     0.191  0.850 \n 2 wrist      0.985     2.07      0.475  0.639 \n 3 ankle     -0.884     1.23     -0.716  0.480 \n 4 age       -0.111     0.127    -0.869  0.392 \n 5 abdomen    0.298     0.322     0.924  0.363 \n 6 chest     -0.460     0.451    -1.02   0.316 \n 7 forearm    2.29      1.37      1.66   0.107 \n 8 weight     0.377     0.179     2.11   0.0435\n 9 thigh     -1.26      0.591    -2.14   0.0409\n10 hip       -0.931     0.416    -2.24   0.0331",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-3-backward-stepwise-selection-step-by-step-results",
    "href": "L05-model-selection.html#example-3-backward-stepwise-selection-step-by-step-results",
    "title": "5  Model Selection",
    "section": "Example 3: Backward Stepwise Selection Step-by-Step Results",
    "text": "Example 3: Backward Stepwise Selection Step-by-Step Results\nBelow is the complete model sequence along with 10-fold CV MAE for each model (using set.seed(253)).\n\n\n\npred\nCV MAE\npredictor list\n\n\n\n\n12\n5.728\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck, biceps\n\n\n11\n5.523\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck\n\n\n10\n5.413\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee\n\n\n9\n5.368\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist\n\n\n8\n5.047\nweight, hip, forearm, thigh, chest, abdomen, age, ankle\n\n\n7\n5.013\nweight, hip, forearm, thigh, chest, abdomen, age\n\n\n6\n4.684\nweight, hip, forearm, thigh, chest, abdomen\n\n\n5\n4.460\nweight, hip, forearm, thigh, chest\n\n\n4\n4.386\nweight, hip, forearm, thigh\n\n\n3\n4.091\nweight, hip, forearm\n\n\n2\n3.733\nweight, hip\n\n\n1\n3.658\nweight\n\n\n\nDISCUSS:\n\n(Review) Interpret the CV MAE for the model of height by weight alone.\nIs this algorithm more or less computationally expensive than the best subset algorithm?\nThe predictors neck and wrist, in that order, are the most strongly correlated with height. Where do these appear in the backward sequence and what does this mean?\n\n\ncor(humans)[,'height'] %&gt;% \n  sort()\n\n      thigh         hip         age     abdomen        knee       chest \n-0.11301249 -0.10648937 -0.05853538 -0.02173587  0.02345904  0.05838830 \n     biceps       ankle      weight     forearm       wrist        neck \n 0.07441696  0.07920867  0.11228791  0.16968040  0.28967468  0.29147610 \n     height \n 1.00000000 \n\n\n\nWe deleted predictors one at a time. Why is this better than deleting a collection of multiple predictors at the same time (eg: kicking out all predictors with p-value &gt; 0.1)?\n\n\n\nSolution\n\n\nUsing a linear model with only weight to predict height, our prediction error would be on average 3.58 inches off from the truth on new data.\nLess. We only have to build 12 models.\nBoth neck and wrist are kicked out early! The 1-predictor model produced by this algorithm isn’t necessarily the best 1-predictor model (same for any number of predictors).\nThe value of the coefficient (and thus the p-value) is dependent on the other variables in the model as we are accounting for or conditioning on them.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-4-backward-stepwise-selection-final-model",
    "href": "L05-model-selection.html#example-4-backward-stepwise-selection-final-model",
    "title": "5  Model Selection",
    "section": "Example 4: Backward Stepwise Selection Final Model",
    "text": "Example 4: Backward Stepwise Selection Final Model\n\nWe have to pick just 1 of the 12 models as our final model.\nThat is, we have to pick a value for our tuning parameter, the number of predictors.\nIt helps to plot the CV MAE for each model in the sequence.\nHere’s what we saw above:\n\n\nCode\ndata.frame(\n    predictors = c(12:1), \n    mae = c(5.728, 5.523, 5.413, 5.368, 5.047, 5.013, 4.684, 4.460, 4.386, 4.091, 3.733, 3.658)) %&gt;% \n  ggplot(aes(x = predictors, y = mae)) + \n    geom_point() + \n    geom_line() + \n    scale_x_continuous(breaks = c(1:12))\n\n\n\n\n\n\n\n\n\nHere’s another example from a different subset of these data:\n\n\nIn the odd “Goldilocks” fairy tale, a kid comes upon a bear den – the first bear’s bed is too hard, the second bear’s is too soft, and the third bear’s is just right. Our plot illustrates a goldilocks problem in tuning the number of predictors in our backward stepwise model. Explain.\n\n\nWhen the number of predictors is too small, the MAE increases because the model is too….\nWhen the number of predictors is too large, the MAE increases because the model is too….\n\n\nWhich model do you pick?!?\n\n\n\nSolution\n\n\nToo few predictors: model is too simple. too many predictors: model is too overfit.\nBased on our data, I think the model with 1 predictor seems pretty reasonable! If I were looking at the other MAE plot, though, I might gravitate pick a model with 1 (the simplest), 2 (still simple, but better MAE than 1 predictor), or 5 predictors (the model with the best CV MAE).",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-5-machine-learning-vs-human-learning",
    "href": "L05-model-selection.html#example-5-machine-learning-vs-human-learning",
    "title": "5  Model Selection",
    "section": "Example 5: machine learning vs human learning",
    "text": "Example 5: machine learning vs human learning\n\nWhen tuning or finalizing a model building algorithm, we (humans!) have our own choices to make. For one, we need to decide what we prefer:\n\na model with the lowest prediction errors; or\na more parsimonious model: one with slightly higher prediction errors but fewer predictors\n\nIn deciding, here are some human considerations:\n\ngoal: How will the model be used? Should it be easy for humans to interpret and apply?\ncost: How many resources (time, money, computer memory, etc) do the model and data needed require?\nimpact: What are the consequences of a bad prediction?\n\nFor each scenario below, which model would you pick: (1) the model with the lowest prediction errors; or (2) a parsimonious model with slightly worse predictions?\n\nGoogle asks us to re-build their search algorithm.\nA small non-profit hires us to help them build a predictive model of the donation dollars they’ll receive throughout the year.\n\n\n\nSolution\n\n\n1\n2",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-6-forward-stepwise-selection-algorithm",
    "href": "L05-model-selection.html#example-6-forward-stepwise-selection-algorithm",
    "title": "5  Model Selection",
    "section": "Example 6: Forward Stepwise Selection Algorithm",
    "text": "Example 6: Forward Stepwise Selection Algorithm\n\n\nHow do you think this works?\nIs it more or less computationally expensive than backward stepwise?\n\n\n\nSolution\n\n\nStart with 0 predictors. Add the predictor with the smallest p-value. To this model, add a second predictor with the smallest p-value. Continue until all predictors are in the model.\nmore. For 12 predictors, we’d have to build 12 models in step 1, 11 models in step 2, etc. Thus 12 + 11 + … + 1 = 78 models total.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#warning",
    "href": "L05-model-selection.html#warning",
    "title": "5  Model Selection",
    "section": "WARNING",
    "text": "WARNING\n\nVariable selection algorithms are a nice, intuitive place to start our discussion of model selection techniques.\nBUT we will not use them.\nThey are frowned upon in the broader ML community, so much so that tidymodels doesn’t even implement them! Why?\n\nBest subset selection is computationally expensive.\n\nBackward stepwise selection:\n\nis greedy – it makes locally optimal decisions, thus often misses the globally optimal model\noverestimates the significance of remaining predictors, thus shouldn’t be used for inference\n\nForward stepwise selection:\n\nis computationally expensive\ncan produce odd combinations of predictors (eg: a new predictor may render previously included predictors non-significant).",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#instructions-1",
    "href": "L05-model-selection.html#instructions-1",
    "title": "5  Model Selection",
    "section": "Instructions",
    "text": "Instructions\n\nScroll down to the # Exercises section in the Part 2 QMD.\nGoal: become familiar with new code structures (recipes and workflows)\nAsk me questions as I move around the room.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#questions-1",
    "href": "L05-model-selection.html#questions-1",
    "title": "5  Model Selection",
    "section": "Questions",
    "text": "Questions\nThe video for today introduced the concepts of recipes and workflows in the tidymodels framework. These concepts will become important to our new modeling algorithms. Though they aren’t necessary to linear regression models, let’s explore them in this familiar setting.\nRun through the following discussion and code one step at a time. Take note of the general process, concepts, and questions you have.\nSTEP 1: model specification\nThis specifies the structure or general modeling algorithm we plan to use.\nIt does not specify anything about the variables of interest or our data.\n\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# Check it out\nlm_spec\n\nSTEP 2: recipe specification\nJust as a cooking recipe specifies the ingredients and how to prepare them, a tidymodels recipe specifies:\n\nthe variables in our relationship of interest (the ingredients)\nhow to pre-process or wrangle these variables (how to prepare the ingredients)\nthe data we’ll use to explore these variables (where to find the ingredients)\n\nIt does not specify anything about the model structure we’ll use to explore this relationship.\n\n# A simple recipe with NO pre-processing\ndata_recipe &lt;- recipe(height ~ wrist + ankle, data = humans)\n\n# Check it out\ndata_recipe\n\nSTEP 3: workflow creation (model + recipe)\nThis specifies the general workflow of our modeling process, including our model structure and our variable recipe.\n\nmodel_workflow &lt;- workflow() %&gt;%\n  add_recipe(data_recipe) %&gt;%\n  add_model(lm_spec)\n\n# Check it out\nmodel_workflow\n\nSTEP 4: Model estimation\nThis step estimates or fits our model of interest using our entire sample data.\nThe model (lm_spec) and variable details (here just height ~ wrist + ankle) are specified in the workflow, so we do not need to give that information again!\n\nmy_model &lt;- model_workflow %&gt;% \n  fit(data = humans)\n\nSTEPS 5: Model evaluation\nTo get in-sample metrics, use my_model like normal.\n\n# example: calculate of in-sample metrics\nmy_model %&gt;% \n  glance()\n\nTo get CV metrics, pass the workflow to fit_resamples along with information about how to randomly create folds.\n\nset.seed(253)\nmy_model_cv &lt;- model_workflow %&gt;% \n  fit_resamples(resamples = vfold_cv(humans, v = 10),\n                metrics = metric_set(rsq))\n\nThen, proceed as usual… (my_model_cv %&gt;% collect_metrics(), etc. )",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "r_rstudio.html",
    "href": "r_rstudio.html",
    "title": "R and RStudio Setup",
    "section": "",
    "text": "Troubleshooting\nHere’s how to fix it:",
    "crumbs": [
      "Appendices",
      "R and RStudio Setup"
    ]
  },
  {
    "objectID": "r_rstudio.html#troubleshooting",
    "href": "r_rstudio.html#troubleshooting",
    "title": "R and RStudio Setup",
    "section": "",
    "text": "Problem: You are on a Mac and getting the following error (or something similar):\n\n\n    Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n     there is no package called ‘rlang’\n\n\nFirst install the suite of Command Line Tools for Mac using the instructions here.\nNext enter install.packages(\"rlang\") in the Console.\nFinally check that entering library(ggplot2) gives no errors.",
    "crumbs": [
      "Appendices",
      "R and RStudio Setup"
    ]
  },
  {
    "objectID": "r_resources.html",
    "href": "r_resources.html",
    "title": "R Resources",
    "section": "",
    "text": "Tidymodels resources",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#tidymodels-resources",
    "href": "r_resources.html#tidymodels-resources",
    "title": "R Resources",
    "section": "",
    "text": "Tidymodels package documentation\nTidy Modeling with R textbook (Max Kuhn and Julia Silge)\nISLR Labs with Tidymodels (Emil Hvitfeldt)\nIntro to Tidymodels Presentation (Lucy D’Agostino McGowan)",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#tidyverse-resources",
    "href": "r_resources.html#tidyverse-resources",
    "title": "R Resources",
    "section": "Tidyverse resources",
    "text": "Tidyverse resources\n\nBrianna Heggeseth’s COMP/STAT 112 website (with code examples and videos)\nR for Data Science\nExploratory Data Analysis with R\nJohn’s Hopkins Tidyverse course text",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#visualization-resources",
    "href": "r_resources.html#visualization-resources",
    "title": "R Resources",
    "section": "Visualization resources",
    "text": "Visualization resources\n\nggplot2 reference\nColors in R",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#general-r-resources",
    "href": "r_resources.html#general-r-resources",
    "title": "R Resources",
    "section": "General R resources",
    "text": "General R resources\n\nRStudio cheatsheets\nAdvanced R\nR Programming Wikibook\nDebugging in R\n\nArticle\nVideo",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#some-example-code",
    "href": "r_resources.html#some-example-code",
    "title": "R Resources",
    "section": "Some example code",
    "text": "Some example code\nCreating new variables\ncase_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables.\n\n# Turn quant_var into a Low/Med/High version\ndata &lt;- data %&gt;%\n    mutate(cat_var = case_when(\n            quant_var &lt; 10 ~ \"Low\",\n            quant_var &gt;= 10 & quant_var &lt;= 20 ~ \"Med\",\n            quant_var &gt; 20 ~ \"High\"\n        )\n    )\n\n# Turn cat_var (A, B, C categories) into another categorical variable\n# (collapse A and B into one category)\ndata &lt;- data %&gt;%\n    mutate(new_cat_var = case_when(\n            cat_var %in% c(\"A\", \"B\") ~ \"A or B\"\n            cat_var==\"C\" ~ \"C\"\n        )\n    )\n\n# Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable\n# Doing this for multiple variables allows you to create an index\ndata &lt;- data %&gt;%\n    mutate(x1_score = case_when(\n            x1==0 ~ 10,\n            x1==1 ~ 20,\n            x1==2 ~ 50\n        )\n    )\n\n# Add together multiple variables with mutate\ndata &lt;- data %&gt;%\n    mutate(index = x1_score + x2_score + x3_score)",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "stat155.html",
    "href": "stat155.html",
    "title": "STAT 155 Review",
    "section": "",
    "text": "COMPREHENSIVE REVIEW\nA comprehensive STAT 155 review is provided by the Prof. Johnson’s Spring 2022 STAT 155 manual here and the STAT 155 Notes created by Profs. Grinde, Heggeseth, and Myint here.\n\n\n\nQUICK REVIEW\nLet \\(y\\) be a response variable with a set of \\(k\\) explanatory variables \\(x = (x_{1}, x_{2}, ..., x_{k})\\). Then the population linear regression model is\n\\[\\begin{split}\ny & = f(x) + \\varepsilon  = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k} + \\varepsilon \\\\\n\\end{split}\\]\nNOTES:\n\n\\(\\beta\\) is the Greek letter “beta”. \\(\\varepsilon\\) is the Greek letter “epsilon”.\n\n“Linear” regression is so named because it assumes that \\(y\\) is a linear combination of the \\(x\\)’s. It does not mean that the relationship itself is linear!! For example, one of the predictors might be a quadratic term: \\(x_2 = x_1^2\\).\n\\(f(x) = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k}\\) captures the trend of the relationship\n\n\\(\\beta_0\\) = intercept coefficient\nthe model value when \\(x_1=x_2=\\cdots=x_k=0\\)\n\\(\\beta_i\\) = \\(x_i\\) coefficient\nhow \\(x_i\\) is related to \\(y\\) when holding constant all other \\(x_i\\)\n\n\\(\\epsilon\\) reflects deviation from the trend (the residual)\n\n\n\n\n\nFitting the Model\nOnce we have a population model in mind, we can “fit the model” (i.e. estimate the \\(\\beta\\) population coefficients) using sample data:\n\\[\\begin{split}\ny & =  \\hat{f}(x) + \\varepsilon \\\\\n& = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1} + \\hat{\\beta}_2 x_{2} + \\cdots + \\hat{\\beta}_k x_{k} + \\varepsilon \\\\\n\\end{split}\\]\n\n\nTo this end, collect a sample of data on \\(n\\) subjects. Use subscripts to denote the data for subject \\(i\\): \\(y_i\\) and \\(x_{ij}\\). Then the predicted response and residual (prediction error) for subject \\(i\\) are\n\nprediction \\[\\hat{y}_i = \\hat{f}(x_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + \\cdots + \\hat{\\beta}_k x_{ik}\\]\nresidual / prediction error \\[y_i - \\hat{y}_i\\]\n\n\n\n\nLeast Squares Criterion\nEstimate (\\(\\beta_0, \\beta_1,..., \\beta_k\\)) by (\\(\\hat{\\beta}_0, \\hat{\\beta}_1,..., \\hat{\\beta}_k)\\) that minimize the sum of squared residuals: \\[\\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = (y_1-\\hat{y}_1)^2 + (y_2-\\hat{y}_2)^2 + \\cdots + (y_n-\\hat{y}_n)^2\\]",
    "crumbs": [
      "Appendices",
      "STAT 155 Review"
    ]
  }
]