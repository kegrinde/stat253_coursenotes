[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 253: Statistical Machine Learning",
    "section": "",
    "text": "Welcome!\nThis is the course website for Statistical Machine Learning (STAT 253) at Macalester College for the Fall 2024 semester.\nThe site was built by Kelsey Grinde. It draws heavily upon our course textbook, the 2nd edition of An Introduction to Statistical Learning with Applications in R, as well as on materials prepared by fellow Macalester statistics faculty Brianna Heggeseth, Alicia Johnson, and Leslie Myint.\nThe structure of this site was inspired by an eCOTS 2024 workshop led by Devin Becker: see here for materials.\nNote that this page will be under active construction throughout the semester. If you find any typos or other issues, please click the Report an issue button above and/or email kgrinde@macalester.edu.\n \n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The schedule below is a tentative outline of our plans for the semester.\nBefore each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Goals.\nReadings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). The ISLR readings are highly encouraged and serve as a nice complement to the videos and in-class activities.\n\n\n\n\n\nWeek 1\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n9/3\nCANCELED\n\n\n\n\n\n\n9/5\nCANCELED\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n9/10\nUnit 0: Introductions & Overview\nBackground Survey\nISLR Reading: Chap 1 & Section 2.1 (Skip 2.1.2, 2.1.3 for now)\nIntroductions\n\nComplete CP1 (before next class)\nStart HW0 (9/13)\n\n\n\n9/12\nUnit 1: Model Evaluation\nConcept Video (script)\nCheckpoint 1\nR Tutorial Video (code)\nISLR: Section 2.2 (skip 2.2.3 for now.), Section 3.1\nEvaluating Regression Models\n(QMD)\nComplete CP2\nFinish HW0\nConcept Video: Evaluating Regression Models\nR Tutorial Video: Introduction to TidyModels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n9/17\nUnit 1: Overfitting\nCheckpoint 2\nR Tutorial Video (code)\nISLR: Section 2.1.2, 5.1\nOverfitting\n(Part 1 QMD) (Part 2 QMD)\nComplete CP3\nStart HW1\nContent Video: Overfitting\n\n\n9/19\nUnit 1: Cross Validation\nConcept Video 1 (script)\nConcept Video 2 (script)\nCheckpoint 3\nR Tutorial Video (code)\nISLR: 5.1\nCross-Validation\n(QMD)\nComplete CP4\nContinue HW1\nConcept Video: Cross-Validation\nR Tutorial: Training, Testing and Cross-Validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n9/24\nUnit 2: Model Selection\nR Code Video (code)\nCheckpoint 4\nModel Selection\n(Part 1 QMD)\n(Part 2 QMD)\n- Complete CP5\n- Submit HW1\n- Start HW2\n- ISLR 6.1\n- Concept Video: Model Building / Selection\n- R Tutorial Video: Preprocessing and Recipes\n- R Tutorial Video: Subset Selection\n\n\n9/26\nUnit 2: LASSO (Shrinkage/ Regularization)\nConcept Video 1\nConcept Video 2 (script)\nCheckpoint 5\nLASSO\n(QMD)\nContinue HW2\n- ISLR 6.2\n- R code tutorial part 1& part 2 (code)\n- Concept Video: LASSO (Shrinkage/Regularization)\n- R Tutorial Video: Lasso and Regularization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n10/1\nUnit 3: Nonparametric Models\n(see optional resources)\nNonparametric Models\n(QMD)\n- Finish HW2 (due tonight!)\n- CP6\n- Start HW3\nISLR 2.1.2, 3.5\n\n\n10/3\nUnit 3: KNN Regression and the Bias-Variance Tradeoff\nConcept Video 1 (script)\nConcept Video 2 (script)\nCheckpoint 6\nKNN Regression\n(QMD)\n- CP7\n- Continue HW3\n- ISLR 2.2.2 (bias-variance), 3.5 (KNN)\n- R code tutorial (code)\n- Concept Video: KNN Regression and the Bias-Variance Tradeoff\n- R Tutorial: KNN Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n10/8\nUnit 3: LOESS & GAMs\nConcept Video 1 (script)\nConcept Video 2 (script)\nCheckpoint 7\nLOESS & GAMs\n(QMD)\n- Continue HW3\n- Study for Quiz 1\n- Start Group Assignment 1\n- ISLR 7.1-7.4, 7.6-7.7\n- Concept Video: Local Regression and Generalized Additive Models\n- R Tutorial Video: Local Regression and GAM\n- Concept Video: Modeling Nonlinearity: Polynomial Regression and Splines\n- R Tutorial Video: Nonlinearity: Polynomial Regression and Splines\n\n\n\n10/10\nUnits 1-3: Review\nStart Group Assignment 1\nRegression Review & Work Time\n- Finish HW3\n- Study for Quiz 1\n- Continue Midterm Learning Reflection\n- Continue Group Assignment 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n10/15\nQuiz 1\n(study!)\nQuiz 1\n- Finish Part 2 of Quiz\n- Finish Learning Reflection\n- Continue Group Assign\n- Prep for next class\n\n\n\n10/17\nFALL BREAK\n\n(no class today!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n10/22\nCatch-Up Day\n(nothing / enjoy Fall Break!)\nBoard Notes \n- Complete CP8\n- Continue: GA1 (10-24), HW3 Revisions (10-25), Quiz Revisions (10-29)\n\n\n\n10/24\nUnit 4: Classification via Logistic Regression\nConcept Video 1 (script)\nConcept Video 2 (script)\nCheckpoint 8\nLogistic Regression\n(QMD)\n- Complete CP9\n- Submit: GA1 (10-24), HW3 Revisions (10-25)\n- Continue: Quiz Revisions (10-29)\n- Start HW4 (10-31)\nISLR: 4.1-4.3\nConcept Video: Logistic Regression\nR Video: Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n10/29\nUnit 4: Evaluating Classification Models\nConcept Video 1 (script)\nConcept Video 2 (script)\nCheckpoint 9\nEvaluating (binary) Classification Models\n(QMD)\n- Submit Quiz Revisions (today)\n- Continue HW4 (10-31)\nISLR: 4.1-4.3\nConcept Video: Evaluating Classification Models\nR Video: Evaluating Classification\n\n\n10/31\nUnit 5: KNN & Decision Trees\n\nKNN & Decision Trees\n(QMD)\n- Complete CP10\n- Submit HW4 (today)\nISLR: 2.2.3, 8.1\nConcept Video: Decision Trees\nR Video: Decision Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n11/5\nUnit 5: More KNN & Decision Trees\nConcept Video 1 (script)\nConcept Video 2 (script)\nCheckpoint 10\nMore KNN & Decision Trees\n(QMD)\n- VOTE!!!\n- Start HW5 (11-3)\nISLR: 8.1\nConcept Video: Decision Trees\nR Video: Decision Trees\n\n\n11/7\nUnit 5: Bagging and Random Forests\n\nBagging & Random Forests\n(QMD)\n- Complete CP11\n- Continue HW5 (11-13)\n - Start Group Assignment 2 (11-26)\nISLR: 8.2\nConcept Video: Bagging and Random Forests\nR Video: Bagging and Random Forests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n11/12\nUnit 5: Classification Review\nConcept Video (script)\nCheckpoint 11\nUnits 4-5 Review\n- Complete CP12\n- Finish HW5 (11-13)\n\n\n\n11/14\nUnit 6: Hierarchical Clustering\nConcept Video 1 (script)\nConcept Video 2 (script)\nCheckpoint 12\nHierarchical Clustering\n(QMD)\n- Continue Group Assignment 2\n- Study for Quiz 2\nISLR: 10.3.2\nConcept Video: Hierarchical Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\n\nDate\nTopic\nBefore Class: Videos/Readings\nIn Class: Slides/Notes\nAfter Class: Assignments\nAdditional Resources (Optional)\n\n\n11/19\nUnits 4-5: Quiz\n(study!)\nQuiz 2\n- Complete CP13\n- Continue GA2 (11-26)\n\n\n\n11/21\nUnit 6: K-Means Clustering\nConcept Video (script)\nCheckpoint 13\nK-Means Clustering\n(QMD)\n- Complete CP14\n- Start HW6 (12-4)\nISLR: 10.3.1\nConcept Video: K-Means Clustering\n\n\n\n \n\n--&gt;\nNote: this page is currently under construction! More dates coming soon.",
    "crumbs": [
      "Overview",
      "Schedule"
    ]
  },
  {
    "objectID": "learning-objectives.html",
    "href": "learning-objectives.html",
    "title": "Learning Goals",
    "section": "",
    "text": "General Skills\nComputational Thinking\nEthical Data Thinking\nData Communication\nCollaborative Learning\nReflection",
    "crumbs": [
      "Overview",
      "Learning Goals"
    ]
  },
  {
    "objectID": "learning-objectives.html#general-skills",
    "href": "learning-objectives.html#general-skills",
    "title": "Learning Goals",
    "section": "",
    "text": "Be able to perform the following tasks:\n\nDecomposition: Break a task into smaller tasks to be able to explain the process to another person or computer\nPattern Recognition: Recognize patterns in tasks by noticing similarities and common differences\nAbstraction: Represent an idea or process in general terms so that you can use it to solve other projects that are similar in nature\nAlgorithmic Thinking: Develop a step-by-step strategy for solving a problem\n\n\n\n\n\nIdentify ethical issues associated with applications of statistical machine learning in a variety of settings\nAssess and critique the actions of individuals and organizations as it relates to ethical use of data\n\n\n\n\nIn written and oral formats:\n\nInform and justify data analysis and modeling process and the resulting conclusions with clear, organized, logical, and compelling details that adapt to the background, values, and motivations of the audience and context in which communication occurs.\n\n\n\n\nUnderstand and demonstrate characteristics of effective collaboration (team roles, interpersonal communication, self-reflection, awareness of social dynamics, advocating for yourself and others).\nDevelop a common purpose and agreement on goals.\nBe able to contribute questions or concerns in a respectful way.\nShare and contribute to the group’s learning in an equitable manner.\n\n\n\nRegularly reflect on your learning to make note of and celebrate your progress, identify opportunities for continued growth, and set goals",
    "crumbs": [
      "Overview",
      "Learning Goals"
    ]
  },
  {
    "objectID": "learning-objectives.html#course-topics",
    "href": "learning-objectives.html#course-topics",
    "title": "Learning Goals",
    "section": "Course Topics",
    "text": "Course Topics\nSpecific learning objectives for our course topics are listed below. Use these to guide your synthesis of video and reading material for specific topics.\nIntroduction to Statistical Machine Learning\n\nFormulate research questions that align with regression, classification, or unsupervised learning tasks.\nIdentify the appropriate task (regression, classification, unsupervised) for a given research question.\n\n\nUnit 1\nEvaluating Regression Models\n\nCreate and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns.\nCalculate and interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way.\n\n\nOverfitting and cross-validation\n\nExplain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance\nAccurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric\nExplain what role CV has in a predictive modeling analysis and its connection to overfitting\nExplain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time\n\n\nUnit 2\nModel selection\n\nExplain the difference between inferential models and predictive models and how the model building processes differ\nClearly describe the backward stepwise selection algorithm and why they are examples of greedy algorithms\nCompare best subset and stepwise algorithms in terms of optimality of output and computational time\n\n\nLASSO (shrinkage/regularization)\n\nExplain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection\nExplain how the lambda tuning parameter affects model performance and how this is related to overfitting\n\n\nUnit 3\nKNN Regression and the Bias-Variance Tradeoff\n\nClearly describe / implement by hand the KNN algorithm for making a regression prediction\nExplain how the number of neighbors relates to the bias-variance tradeoff\nExplain the difference between parametric and nonparametric methods\nExplain how the curse of dimensionality relates to the performance of KNN\n\n\nLocal Regression and Generalized Additive Models\n\nClearly describe the local regression algorithm for making a prediction\nExplain how bandwidth (span) relate to the bias-variance tradeoff\nDescribe some different formulations for a GAM (how the arbitrary functions are represented)\nExplain how to make a prediction from a GAM\nInterpret the output from a GAM\n\n\nUnit 4\nClassification via Logistic regression\n\nUse a logistic regression model to make hard (class) and soft (probability) predictions\nInterpret non-intercept coefficients from logistic regression models in the data context\n\n\nEvaluating classification models\n\nCalculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity\nConstruct and interpret plots of predicted probabilities across classes\nExplain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric\nAppropriately use and interpret the no-information rate to evaluate accuracy metrics\n\n\nDecision trees\n\nClearly describe the recursive binary splitting algorithm for tree building for both regression and classification\nCompute the weighted average Gini index to measure the quality of a classification tree split\nCompute the sum of squared residuals to measure the quality of a regression tree split\nExplain how recursive binary splitting is a greedy algorithm\nExplain how different tree parameters relate to the bias-variance tradeoff\n\n\nBagging and random forests\n\nExplain the rationale for bagging\nExplain the rationale for selecting a random subset of predictors at each split (random forests)\nExplain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff\nExplain the rationale for and implement out-of-bag error estimation for both regression and classification\nExplain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class)\n\n\nHierarchical clustering\n\nClearly describe / implement by hand the hierarchical clustering algorithm\nCompare and contrast k-means and hierarchical clustering in their outputs and algorithms\nInterpret cuts of the dendrogram for single and complete linkage\nDescribe the rationale for how clustering algorithms work in terms of within-cluster variation\nDescribe the tradeoff of more vs. less clusters in terms of interpretability\nImplement strategies for interpreting / contextualizing the clusters\n\n\nK-means clustering\n\nClearly describe / implement by hand the k-means algorithm\nDescribe the rationale for how clustering algorithms work in terms of within-cluster variation\nDescribe the tradeoff of more vs. less clusters in terms of interpretability\nImplement strategies for interpreting / contextualizing the clusters\n\n\nPrincipal Component Analysis\n\nExplain the goal of dimension reduction and how this can be useful in a supervised learning setting\nInterpret and use the information provided by principal component loadings and scores\nInterpret and use a scree plot to guide dimension reduction",
    "crumbs": [
      "Overview",
      "Learning Goals"
    ]
  },
  {
    "objectID": "L01-introductions.html",
    "href": "L01-introductions.html",
    "title": "1  Introductions",
    "section": "",
    "text": "Welcome!\nNote: everything you need for class today is on the course website: https://kegrinde.github.io/stat253_coursenotes/",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#whats-machine-learning",
    "href": "L01-introductions.html#whats-machine-learning",
    "title": "1  Introductions",
    "section": "What’s Machine Learning?",
    "text": "What’s Machine Learning?\n“Machine Learning” was coined back in 1959 by Arthur Samuel, an early contributor to AI.\nFrom Kohavi & Provost (1998): Machine Learning is the exploration & application of algorithms that can learn from existing patterns and make predictions using data. (NOTE: humans are in charge of the exploration & application!)\n\n\nIn STAT 253 we will…\n\nPick up where STAT 155 left off, acquiring tools that can be used to learn from data in greater depth and a wider variety of settings. (STAT 155 is a foundational subset of ML!)\nExplore universal ML concepts using tools and software common among statisticians (hence “statistical” machine learning).\nSurvey a breadth of modern ML tools and algorithms that fall into the workflow below. We’ll focus on concepts and applications over mathematical theory. Part of the cognitive load will be:\n\nkeeping all the tools in place (what are they and when to use them)\nunderstanding the connections between the tools\nadapting (not memorizing) code to implement each tool",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#supervised-learning",
    "href": "L01-introductions.html#supervised-learning",
    "title": "1  Introductions",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nWe want to model the relationship between some output variable \\(y\\) and input variables \\(x = (x_1, x_2,..., x_p)\\):\n\\[\\begin{split}\ny\n& = f(x) + \\varepsilon \\\\\n& = \\text{(trend in the relationship) } + \\text{ (residual deviation from the trend `epsilon`)} \\\\\n\\end{split}\\]\nTypes of supervised learning tasks:\n\nregression: \\(y\\) is quantitative\nexample:\n\\(y\\) = body mass index\n\\(x\\) = (number of live births, age, marital status, education, etc)\nclassification: \\(y\\) is categorical\nexample:\n\\(y\\) = whether a pair of crickets courted (yes, no) \\(x\\) = (species, pair of same species, CHC profile, etc)",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#unsupervised-learning",
    "href": "L01-introductions.html#unsupervised-learning",
    "title": "1  Introductions",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nWe have some input variables \\(x = (x_1, x_2,..., x_p)\\) but there’s no output variable \\(y\\). Thus the goal is to use \\(x\\) to understand and/or modify the structure of our data with respect to \\(x\\).\nTypes of unsupervised learning tasks:\n\nclustering\nIdentify and examine groups or clusters of data points that are similar with respect to their \\(x_i\\) values. example:\n\n\\(x\\) = (body mass index at 2 weeks, 1 month, 2 months, 4 months, 6 months, etc)\n\ndimension reduction\nTurn the original set of \\(p\\) input variables, which are potentially correlated, into a smaller set of \\(k &lt; p\\) variables which still preserve the majority of information in the originals. example:\n\n\\(x\\) = (cuticular hydrocarbon compounds concentrations based on gas chromatography analysis)",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#meet-your-classmates",
    "href": "L01-introductions.html#meet-your-classmates",
    "title": "1  Introductions",
    "section": "Meet Your Classmates!",
    "text": "Meet Your Classmates!\nI used a machine learning algorithm, one we’ll learn later this semester, to form groups based on your responses to the pre-course informational survey. BUT it didn’t provide any explanation of why these are the groups it picked. To that end, we need humans.\n\nGet into your assigned group.\nIntroduce yourselves in whatever way you feel appropriate (ideas: name, pronouns, how you’re feeling at the moment, things you’re looking forward to, best part of summer, why you are motivated to take this class)\nTry to figure out why the algorithm put you into a group together. (I don’t personally know the answer!)\nPrepare to introduce your group to the bigger class:\n\nEach person will introduce themself\nOne person will explain why they think the group was put together",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#meet-your-instructor",
    "href": "L01-introductions.html#meet-your-instructor",
    "title": "1  Introductions",
    "section": "Meet Your Instructor",
    "text": "Meet Your Instructor\nA few highlights from my answers to the Pre-Course Information Gathering Survey…\n\nPreferred name: “Kelsey” or “Professor Grinde”\nPronouns: she/her/hers\nHometown(s): Plymouth –&gt; Northfield –&gt; Seattle –&gt; St. Paul\nCan you tell me a bit about how you’ve been spending your time this summer? What’s been particularly important or meaningful to you? What brings you joy right now? What is on your mind? What do you do when you’re not in class?",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#instructions",
    "href": "L01-introductions.html#instructions",
    "title": "1  Introductions",
    "section": "Instructions",
    "text": "Instructions\n\nDiscuss the following scenarios as a group, talking through your ideas, questions, and reasoning as you go\nI’ll move around to groups to check in on your progress and see what questions you have\nYou can check your answers by clicking the drop-down “Solutions” button",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#questions",
    "href": "L01-introductions.html#questions",
    "title": "1  Introductions",
    "section": "Questions",
    "text": "Questions\nIndicate whether each scenario below represents a regression, classification, or clustering task.\n\nHow is the number of people that rent bikes on a given day in Washington, D.C. (\\(y\\)) explained by the temperature (\\(x_1\\)) and whether or not it’s a weekend (\\(x_2\\))?\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nregression. there’s a quantitative output variable \\(y\\).\n\n\n\nGiven the observed bill length (\\(x_1\\)) and bill depth (\\(x_2\\)) on a set of penguins, how many different penguin species might there be?\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nclustering. there’s no output variable \\(y\\).\n\n\n\nHow can we determine whether somebody has a certain infection (\\(y\\)) based on two different blood sample measurements, Measure A (\\(x_1\\)) and Measure B (\\(x_2\\))?\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nclassification. there’s a categorical output variable \\(y\\).\n\n\n\nMachine learn about each other! Scenario A.\nI collected some data on STAT 253 students (you!) and analyzed it using a machine learning algorithm. In your groups: (1) brainstorm what research question is being investigated; (2) determine whether this is a regression, classification, or clustering task; and (3) summarize what the output tells you about your classmates.\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nclassification (\\(y\\) = major is categorical)\n\n\n\nMachine learn about each other! Scenario B.\nSame directions as for Scenario A:\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nregression (\\(y\\) = time to mac is quantitative)\n\n\n\nMachine learn about each other! Scenario C.\nSame directions as for Scenario A:\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nclustering (no outcome \\(y\\)).\n\n\n\nUse Spotify users’ previous listening behavior to identify groups of similar users.\n\n\n\nSolution\n\nclustering\n\n \n\nPredict workers’ wages by their years of experience.\n\n\n\n\nSolution\n\nregression (\\(y\\) = wages)\n\n\n\nPredict workers’ wages by their college major.\n\n\n\n\nSolution\n\nregression (\\(y\\) = wages)\n\n\n\nUse a customer’s age to predict whether they’ve seen the Barbie movie.\n\n\n\n\nSolution\n\nclassification (\\(y\\) = whether or not watched the film)\n\n\n\nLook for similarities among genetic samples taken from a group of patients.\n\n\n\n\nSolution\n\nclustering (no outcome \\(y\\))",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#scavenger-hunt",
    "href": "L01-introductions.html#scavenger-hunt",
    "title": "1  Introductions",
    "section": "Scavenger Hunt",
    "text": "Scavenger Hunt\nTake a few minutes to make sure you know how to find all of the following:\n\ncourse website\nsyllabus\ntextbook\nSTAT 253 Slack\noffice hour times and locations\nassignment deadlines\ninformation on what you need to complete before class each day\nin-class activities\nassignment instructions / submission",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "L01-introductions.html#whats-next",
    "href": "L01-introductions.html#whats-next",
    "title": "1  Introductions",
    "section": "What’s next?",
    "text": "What’s next?\nWhat to work on after class today:\n\ncarefully review the syllabus\n\nif time allows, we’ll discuss a few highlights now!\nmore to come in the next few class sessions\n\njoin Slack\nupdate your versions of R/RStudio (see R and RStudio Setup)\ncomplete the pre-class tasks for Thursday (videos/reading/checkpoint)\n\nreview the checkpoint recommendations/policies on Moodle before you start!\n\nstart HW0 (due Friday)",
    "crumbs": [
      "Overview",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introductions</span>"
    ]
  },
  {
    "objectID": "U01-motivation.html",
    "href": "U01-motivation.html",
    "title": "Motivating Question",
    "section": "",
    "text": "We are in the regression setting. We want to build a model of some quantitative output variable \\(y\\) by some predictors \\(x\\):\n\\[y = f(x) + \\epsilon\\]\nAfter building this model, it’s important to evaluate it: Is our regression model a “good” model?\n\nIs the model wrong?\nIs the model strong?\nDoes the model produce accurate predictions?\nIs the model fair?\n\n\n\nIs the model wrong? What assumptions does our model make and are these reasonable?\n\n\n\n\n\n\n\n\n\n\nTo check:\nExamine a residual plot, ie. a scatterplot of the residuals vs predictions for each case. Points should appear randomly scattered with no clear pattern. If you see any patterns in the residuals that suggest you are systematically over or underpredicting for different prediction values, this indicates that the assumption about the relationship with predictors could be wrong.\nExample: Model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\). What about the plots below reveals that this model is “wrong”?\n\n\n\n\n\n\n\n\n\n\nWhat about the plots below reveals that this model is “not wrong”?\n\n\n\n\n\n\n\n\n\n\n\nIs the model strong? How well does our model explain the variability in the response?\n\n\n\n\n\n\n\n\n\n\nCheck: \\(R^2\\), the proportion of variability in \\(y\\) that’s explained by the model. The closer to 1 the better.\n\\[R^2 = 1 - \\frac{\\text{Var}(\\text{residuals})}{\\text{Var}(y)} = 1 - \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n(y_i - \\overline{y})^2}\\]\n\n\nDoes the model produce accurate predictions?\n\n\n\n\n\n\n\n\n\n\n\nCheck: Summarize the combined size of the residuals, \\(y_1 - \\hat{y}_1\\), \\(y_2 - \\hat{y}_2\\), …, \\(y_n - \\hat{y}_n\\) where \\(n\\) is sample size. The closer to 0 the better!\n\\[\\begin{split}\n\\text{MSE}  & = \\text{ mean squared error } = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n\\text{RMSE} & = \\text{ root mean squared error } = \\sqrt{MSE}  \\\\\n\\text{MAE}  & = \\text{ mean absolute error } = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \\\\\n\\end{split}\\]\n\n\nIs the model fair?\n\n\nWho collected the data / who funded the data collection?\nHow did they collect the data?\nWhy did they collect the data?\nWhat are the implications of the analysis, ethical or otherwise?\n\nDig Deeper (optional)\nDigging deeper, there’s more theory behind our regression model assumptions, thus more to the question of “is our model wrong?”. Specifically, in applying the linear regression model\n\\[y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k} + \\epsilon\\]\nwe assume that at any given set of predictors \\(x = (x_1,x_2,...,x_n)\\),\n\\[\\epsilon \\stackrel{ind}{\\sim} N(0, \\sigma^2)\\]\nEquivalently, \\(y \\stackrel{ind}{\\sim} N(\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k}, \\; \\sigma^2)\\).\n\nWe can break this assumption and \\(N()\\) notation down into 4 pieces:\n\nIndependence: \\(\\epsilon \\stackrel{\\color{red}{ind}}{\\sim} N(0, \\sigma^2)\\) The observations on subject \\(i\\) are independent of the observations on any other subject.\n\nNOTE: If our data don’t meet this model assumption, our predictions and inference (eg: confidence intervals & hypothesis tests) might produce misleading results. Take Correlated Data to learn more about working with dependent data.\n\nTrend: \\(\\epsilon \\stackrel{ind}{\\sim} N(\\color{red}{0}, \\sigma^2)\\) At any \\(x\\), the residuals have mean 0. That is, responses are balanced above and below the model. Thus the model accurately captures the trend of the relationship.\n\nNOTE: If our data don’t meet this model assumption, our model is wrong. This issue might be corrected by transforming \\(y\\) or \\(x\\).\n\nHomoskedasticity: \\(\\epsilon \\stackrel{ind}{\\sim} N(0, \\color{red}{\\sigma}^2)\\) At any \\(x\\), the standard deviation among the residuals is \\(\\sigma\\). That is, deviations from the trend are no greater at any one “part” of the model than at another NOTE: If our data don’t meet this model assumption, our inference (eg: confidence intervals & hypothesis tests) might produce misleading results. This issue might be corrected by transforming \\(y\\).\nNormality: \\(\\epsilon \\stackrel{ind}{\\sim} \\color{red}{N}(0, \\sigma^2)\\) The residuals are normally distributed. Thus individual responses are normally distributed around the trend (closer to the trend and then tapering off).\n\nNOTE: If our data don’t meet this model assumption and the violation is extreme, our inference (eg: confidence intervals & hypothesis tests) might produce misleading results. This issue might be corrected by transforming \\(y\\).",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html",
    "href": "L02-evaluating-regression-models.html",
    "title": "2  Model Evaluation",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#directions",
    "href": "L02-evaluating-regression-models.html#directions",
    "title": "2  Model Evaluation",
    "section": "Directions",
    "text": "Directions\n\nIn small groups, please first introduce yourself (in whatever way you feel appropriate) and check in with each other as human beings.\nWhen everyone is ready, glance through the summary of concepts covered in the video (see “Video Recap” below) and discuss the following prompts:\n\nWhat vocabulary or notation was new to you?\nWhat concepts were new to you?\nWhat concepts are still unclear to you at this moment?\n\nPrepare to share a few highlights from your group discussion with the entire class",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#video-recap",
    "href": "L02-evaluating-regression-models.html#video-recap",
    "title": "2  Model Evaluation",
    "section": "Video Recap",
    "text": "Video Recap\n\n\n\nWe are in the regression setting. We want to build a model of some quantitative output variable \\(y\\) by some predictors \\(x\\):\n\\[y = f(x) + \\epsilon\\]\nThere are many regression tools that we might use to build this model. We’ll use a linear regression model which assumes that \\(y\\) is a linear combination of the \\(x\\)’s:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots \\beta_p x_p + \\epsilon\\]\nAfter building any model, it’s important to evaluate it: Is our regression model a “good” model?\n\nIs the model wrong?\nIs the model strong?\nDoes the model produce accurate predictions?\nIs the model fair?\n\nWe will review these concepts through today’s exercises. A detailed overview is provided in the “Motivating Question” section under “Regression: Model Evaluation (Unit 1)” on the course website.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#intro-to-tidymodels",
    "href": "L02-evaluating-regression-models.html#intro-to-tidymodels",
    "title": "2  Model Evaluation",
    "section": "Intro to tidymodels",
    "text": "Intro to tidymodels\nThroughout the semester, we are going to use the tidymodels package in R.\n\nSimilar flavor to tidyverse structure\nMore general structure that allows us to fit many other types of models\n\n. . .\nAt first, it will seem like a lot more code (perhaps even unnecessarily so).\n. . .\nFor example, what you did in STAT 155 with\n\nlm(y ~ x1 + x2, data = sample_data)\n\n. . .\nwill now look like\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% # we want a linear regression model\n  set_mode(\"regression\") %&gt;%  # this is a regression task (y is quantitative)\n  set_engine(\"lm\")# we'll estimate the model using the lm function\n\n# STEP 2: model estimation\nmodel_estimate &lt;- lm_spec %&gt;% \n  fit(y ~ x1 + x2, data = sample_data)\n\n\nBut you’ll need to trust me…",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#highlight-useful-model-evaluation-functions",
    "href": "L02-evaluating-regression-models.html#highlight-useful-model-evaluation-functions",
    "title": "2  Model Evaluation",
    "section": "Highlight: Useful Model Evaluation Functions",
    "text": "Highlight: Useful Model Evaluation Functions\nA few useful functions to use on model_estimate:\n\n. . .\n\nmodel_estimate %&gt;% \n  tidy() #gives you coefficients (and se, t-statistics)\n\n\n. . .\n\nmodel_estimate %&gt;% \n  augment(new_data = sample_data) # gives you predictions and residuals for sample_data\n\n\n. . .\n\nmodel_estimate %&gt;% \n  glance() #gives you some model evaluation metrics (is it strong?)\n\n\n. . .\n\nmodel_estimate %&gt;% \n  augment(new_data = sample_data) %&gt;% \n  mae(truth = y, estimate = .pred) # calculates MAE to measure accuracy of predictions\n\n. . .\nMore info, for future reference, below!",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#future-reference-r-code-for-building-and-evaluating-regression-models",
    "href": "L02-evaluating-regression-models.html#future-reference-r-code-for-building-and-evaluating-regression-models",
    "title": "2  Model Evaluation",
    "section": "Future Reference: R Code for Building and Evaluating Regression Models",
    "text": "Future Reference: R Code for Building and Evaluating Regression Models\nThis section is for future reference. It is a summary of code you’ll learn below for building and evaluating regression models. Throughout, suppose we wish to build and evaluate a linear regression model of y vs x1 and x2 using our sample_data.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nBuilding a linear regression model\n\n# STEP 1: specify the type of model to build\nlm_spec &lt;- linear_reg() %&gt;% # we want a linear regression model\n  set_mode(\"regression\") %&gt;%  # this is a regression task (y is quantitative)\n  set_engine(\"lm\") # we'll estimate the model using the lm function\n\n# STEP 2: estimate the specified model using sample data\nmodel_estimate &lt;- lm_spec %&gt;% \n  fit(y ~ x1 + x2, data = sample_data)\n\n# Get the model coefficients\nmodel_estimate %&gt;% \n  tidy()\n\nObtaining predictions (& residuals) for each observation\n\n# Obtain y predictions and residuals for each observation in our sample_data\n# (We can replace sample_data with any data frame that includes y, x1, and x2)\nmodel_estimate %&gt;% \n  augment(new_data = sample_data)\n\n# Obtain y predictions (but not residuals) for some given x1, x2 values, when we haven't yet observed y\n# (We can replace the data.frame with any data frame that includes x1 and x2)\nmodel_estimate %&gt;% \n  augment(new_data = data.frame(x1 = ___, x2 = ___))\n  \n# Another approach using predict()\nmodel_estimate %&gt;% \n  predict(new_data = data.frame(x1 = ___, x2 = ___))\n\nEvaluating the model\n\n# Is it strong? (R^2)\nmodel_estimate %&gt;% \n  glance()\n\n# Does it produce accurate predictions? (MAE)\nmodel_estimate %&gt;% \n  augment(new_data = sample_data) %&gt;% \n  mae(truth = y, estimate = .pred)\n\n# Is it wrong? (residual plot)\nmodel_estimate %&gt;% \n  augment(new_data = sample_data) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#instructions",
    "href": "L02-evaluating-regression-models.html#instructions",
    "title": "2  Model Evaluation",
    "section": "Instructions",
    "text": "Instructions\n\nWork through these exercises as a group, talking through your ideas, questions, and reasoning as you go and taking notes in your QMD\nBe kind to yourself/each other! You will be rusty and make mistakes, and that’s ok! Mistakes are important to learning.\nFocus on patterns in code. Review, but do not try to memorize any provided code. Focus on the general steps and patterns.\nIf you’re given some starter code with blanks (e.g. below), don’t type in those chunks. Instead, copy, paste, and modify the starter code in the chunk below it.\n\n\n# Start small: rides vs temp\nggplot(___, aes(y = ___, x = ___)) + \n  geom___()\n\n\nAsk questions! We will not have time to discuss all exercises at the end of class. Talk through your questions as a group, and ask me questions as I walk around the room!\nCollaborate. We’re sitting in groups for a reason. Collaboration improves higher-level thinking, confidence, communication, community, and more. I expect you to:\n\nActively contribute to discussion (don’t work on your own)\nActively include all group members in discussion\nCreate a space where others feel comfortable making mistakes & sharing their ideas (remember that we all come to this class with different experiences, both personal and academic)\nStay in sync while respecting that everybody has different learning strategies, work styles, note taking strategies, etc. If some people are working on exercise 10 and others are on exercise 2, that’s probably not a good collaboration.\nDon’t rush. You won’t hand anything in and can finish up outside of class.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#questions",
    "href": "L02-evaluating-regression-models.html#questions",
    "title": "2  Model Evaluation",
    "section": "Questions",
    "text": "Questions\n\nCapital Bikeshare provides a bike-sharing service in the Washington DC area. Customers can pick up and drop off bikes at any station around the city. Of primary interest to the company is:\nHow many registered riders can we expect today?\nTo this end, you will build, evaluate, and compare 2 different linear regression models of ridership using the following Capital Bikeshare dataset (originally from the UCI Machine Learning Repository):\n\n# Load packages we'll need to wrangle and plot the data\nlibrary(tidyverse)\n\n# Load the data\nbikes &lt;- read.csv(\"https://mac-stat.github.io/data/bike_share.csv\")\n\n# Only keep / select some variables\n# And round some variables (just for our demo purposes)\nbikes &lt;- bikes %&gt;% \n  rename(rides = riders_registered, temp = temp_feel) %&gt;% \n  mutate(windspeed = round(windspeed), temp = round(temp)) %&gt;% \n  select(rides, windspeed, temp, weekend)\n\n\n# Check out the dimensions\ndim(bikes)\n\n# Check out the first 3 rows\nhead(bikes, 3)\n\nThis dataset contains the following information for a sample of different dates:\n\n\n\nvariable\ndescription\n\n\n\n\nrides\ncount of daily rides by registered users\n\n\nwindspeed\nwind speed in miles per hour\n\n\ntemp\nwhat the temperature feels like in degrees Fahrenheit\n\n\nweekend\nwhether or not it falls on a weekend\n\n\n\n We’ll consider two linear regression models of ridership:\nrides ~ windspeed + temp and rides ~ windspeed + temp + weekend\n\n\nPlot the relationships. First, let’s plot these relationships. REMINDER: Don’t write in any chunk with starter code. Copy, paste, and modify the code in the chunk below it.\n\n\n# Start small: rides vs temp\nggplot(___, aes(y = ___, x = ___)) + \n  geom___()\n\n\n# rides vs temp & windspeed\nggplot(bikes, aes(y = ___, x = ___, ___ = windspeed)) + \n  geom_point()\n\n\n# rides vs temp & windspeed & weekend\nggplot(bikes, aes(y = ___, x = ___, ___ = windspeed)) + \n  geom_point() +  \n  facet_wrap(~ ___)\n\n\n\nSolution\n\n\n# Start small: rides vs temp\nggplot(bikes, aes(y = rides, x = temp)) + \n  geom_point()\n\n\n\n\n\n\n\n# rides vs temp & windspeed\nggplot(bikes, aes(y = rides, x = temp, color = windspeed)) + \n  geom_point()\n\n\n\n\n\n\n\n# rides vs temp & windspeed & weekend\nggplot(bikes, aes(y = rides, x = temp, color = windspeed)) + \n  geom_point() +  \n  facet_wrap(~ weekend)\n\n\n\n\n\n\n\n\n\n\n\ntidymodels STEP 1: model specification. We’ll build and evaluate our two models of ridership using the tidymodels package. This code is more complicated than the lm()function we used in STAT 155. BUT:\n\n\ntidymodels is part of the broader tidyverse (what we use to plot and wrangle data), thus the syntax is more consistent\ntidymodels generalizes to the other ML algorithms we’ll survey in this course, thus will eventually minimize the unique syntax we need to learn\n\n\n# Load package\nlibrary(tidymodels)\n\nThe first step is to specify what type of model we want to build. We’ll store this as lm_spec, our linear regression model (lm) specification (spec).\n\nlm_spec &lt;- linear_reg() %&gt;%   # we want a linear regression model\n  set_mode(\"regression\") %&gt;%  # this is a regression task (y is quantitative)\n  set_engine(\"lm\")# we'll estimate the model using the lm function\n\nThis code specifies but doesn’t build any model – we didn’t even give it any data or specify the variables of interest!\n\n# Check it out\nlm_spec\n\n\n\nSolution\n\n\n# Load package\nlibrary(tidymodels)\n\nlm_spec &lt;- linear_reg() %&gt;% # we want a linear regression model\n  set_mode(\"regression\") %&gt;%  # this is a regression task (y is quantitative)\n  set_engine(\"lm\")# we'll estimate the model using the lm function\n\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n\ntidymodels STEP 2: model estimation. We can now estimate or fit our two ridership models using the specified model structure (lm_spec) and our sample bikes data:\n\n\n# Fit bike_model_1\nbike_model_1 &lt;- lm_spec %&gt;% \n  fit(rides ~ windspeed + temp, data = bikes)\n\n# Check out the coefficients\nbike_model_1 %&gt;% \n  tidy()\n\n\n# YOUR TURN\n# Fit bike_model_2 & check out the coefficients\n\n\n\nSolution\n\n\nbike_model_1 &lt;- lm_spec %&gt;% \n  fit(rides ~ windspeed + temp, data = bikes)\n\nbike_model_2 &lt;- lm_spec %&gt;% \n  fit(rides ~ windspeed + temp + weekend, data = bikes)\n\n# Check out the results:\nbike_model_1 %&gt;% \n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -20.8    300.     -0.0694 9.45e- 1\n2 windspeed      -36.1      9.42   -3.83   1.37e- 4\n3 temp            55.4      3.33   16.6    7.58e-53\n\nbike_model_2 %&gt;% \n  tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    298.     289.        1.03 3.02e- 1\n2 windspeed      -35.6      9.00     -3.95 8.46e- 5\n3 temp            54.3      3.18     17.1  3.82e-55\n4 weekendTRUE   -858.     101.       -8.46 1.47e-16\n\n\n\n\n\n\nIs it fair? Now, let’s evaluate our two models. First, do you have any concerns about the context in which the data were collected and analyzed? About the potential impact of this analysis?\n\n\n\nSolution\n\nWhat do you think?\n\nWho might be harmed?\nWho benefits?\n\n\n\n\nIs it strong? We can measure and compare the strength of these models using \\(R^2\\), the proportion of variability in our response variable that’s explained by the model. Report which model is stronger and interpret its \\(R^2\\).\n\n\n# Obtain R^2 for bike_model_1\nbike_model_1 %&gt;% \n  glance()\n\n\n# YOUR TURN\n# Obtain R^2 for bike_model_2\n\n\n\nSolution\n\nModel 2 is stronger than model 1 (\\(R^2\\) of 0.372 vs 0.310). But it only explains 37% of the variability in ridership from day to day.\n\n# Obtain R^2 for bike_model_1\nbike_model_1 %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.310         0.308 1298.      163. 2.44e-59     2 -6276. 12560. 12578.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nbike_model_2 %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.372         0.369 1239.      143. 5.82e-73     3 -6242. 12493. 12516.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nPause: get the residuals and predictions. Our next model evaluation questions will focus on the models’ predictions and prediction errors, or residuals. We can obtain this information by augmenting our models with our original bikes data. For example:\n\n\n# Calculate predicted ridership (.pred) & corresponding residuals (.resid) using bike_model_1\n# Just look at first 6 days\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  head()\n\nWe can also predict outcomes for new observations using either augment() or predict(). Note the difference in the output:\n\n# Predict ridership on a 60 degree day with 20 mph winds\nbike_model_1 %&gt;% \n  augment(new_data = data.frame(windspeed = 20, temp = 60))\n\n\n# Predict ridership on a 60 degree day with 20 mph winds\nbike_model_1 %&gt;% \n  predict(new_data = data.frame(windspeed = 20, temp = 60))\n\n\n\nSolution\n\naugment() gives the predictions and residuals for all rows in the data. predict() only gives you predictions.\n\n# Obtain the predictions & residuals using bike_model_1\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  head()\n\n# A tibble: 6 × 6\n  .pred .resid rides windspeed  temp weekend\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;  \n1 3183. -2529.   654        11    65 TRUE   \n2 2911. -2241.   670        17    64 TRUE   \n3 2080.  -851.  1229        17    49 FALSE  \n4 2407.  -953.  1454        11    51 FALSE  \n5 2446.  -928.  1518        13    53 FALSE  \n6 2699. -1181.  1518         6    53 FALSE  \n\n# Predict ridership on a 60 degree day with 20 mph winds\nbike_model_1 %&gt;% \n  augment(new_data = data.frame(windspeed = 20, temp = 60))\n\n# A tibble: 1 × 3\n  .pred windspeed  temp\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 2581.        20    60\n\n# Predict ridership on a 60 degree day with 20 mph winds\nbike_model_1 %&gt;% \n  predict(new_data = data.frame(windspeed = 20, temp = 60))\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1 2581.\n\n\n\n\n\nDoes it produce accurate predictions? Recall that the mean absolute error (MAE) measures the typical prediction error. Specifically, it is the mean of the absolute values of the residual errors for the days in our dataset.\n\n\nUse the residuals to calculate the MAE for the 2 models. HINT: abs().\n\n\n# MAE for bike_model_1\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  summarize(mae = ___(___(___)))\n\n# MAE for bike_model_2\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  summarize(mae = ___(___(___)))\n\n\nDoing the calculation from scratch helps solidify your understanding of how MAE is calculated, thus interpreted. Check your calculations using a shortcut function.\n\n\n# Calculate MAE for the first model\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  mae(truth = rides, estimate = .pred)\n\n\n# YOUR TURN\n# Calculate MAE for the second model\n\n\nWhich model has more accurate predictions? Interpret the MAE for this model and comment on whether it’s “large” or “small”. NOTE: “large” or “small” is defined by the context (e.g. relative to the observed range of ridership, the consequences of a bad prediction, etc).\n\n\n\nSolution\n\nOn average, the model 1 predictions are off by ~1080 riders and the model 2 predictions are off by ~1038 riders. Is this a lot? Consider this error relative to the scale of the data: there are roughly 1000 - 7000 riders per day.\n\n# a\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  summarize(mae = mean(abs(.resid)))\n\n# A tibble: 1 × 1\n    mae\n  &lt;dbl&gt;\n1 1080.\n\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  summarize(mae = mean(abs(.resid)))\n\n# A tibble: 1 × 1\n    mae\n  &lt;dbl&gt;\n1 1038.\n\n# b\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  mae(truth = rides, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       1080.\n\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  mae(truth = rides, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       1038.\n\n\n\n\n\nIs it wrong? To determine whether the linear regression assumptions behind bike_model_1 and bike_model_2 are reasonable, we can review residual plots, i.e. plots of the residuals vs predictions for each observation in our dataset. Run the code below and summarize your assessment of whether our models are wrong. RECALL: We want the appoints to appear random and centered around 0 across the entire range of the model / predictions.\n\n\n# Residual plot for bike_model_1\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n# YOUR TURN\n# Residual plot for bike_model_2\n\n\n\nSolution\n\nBoth models look roughly “right” BUT there is a little downward slope at the extreme end of the residual plots. This corresponds to the observed phenomenon that when it’s really hot, ridership starts dipping. In a future model, we might incorporate a quadratic temperature term.\n\n# Residual plot for bike_model_1\nbike_model_1 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \ngeom_point() + \ngeom_hline(yintercept = 0)\n\n\n\n\n\n\n\n# Residual plot for bike_model_2\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \ngeom_point() + \ngeom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\n\n\n\nArt vs science Inspecting residual plots is more art than science.1 It requires a lot of practice. Consider another example using simulated data. First, build a model that assumes all predictors are roughly linearly related:\n\n\n# Import data\nsimulated_data &lt;- read.csv(\"https://ajohns24.github.io/data/simulated_data.csv\")\n\n# Model y by the 6 input variables\nnew_model &lt;- lm_spec %&gt;% \n  fit(y ~ x1 + x2 + x3 + x4 + x5 + x6, simulated_data)\n\nNext, check out a pairs plot. Is there anything here that makes you think that our model assumption is bad?\n\nlibrary(GGally)\nggpairs(simulated_data)\n\n\n\n\n\n\n\n\nFinally, check out a residual plot. Any concerns now?\n\nnew_model %&gt;% \n  ___(new_data = ___) %&gt;% \n  ggplot(aes(x = ___, y = ___)) + \n  geom_point(size = 0.1) + \n  geom_hline(yintercept = 0)\n\n\n\nSolution\n\nArt + Science!\n\nnew_model %&gt;% \n  augment(new_data = simulated_data) %&gt;% \n  ggplot(aes(x = .pred, y = .resid)) + \n  geom_point(size = 0.1) + \n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\n\n\n\nDetails: communication & code style Communication is a key machine learning skill, including written summaries, presentations, and code. Just like an essay, code must have structure, signposts, and grammar that will make it easier for others to follow. The below code runs, but it is “bad code”.\n\n\nFix this code and add comments so that it is easier for yourself and others to follow.\nAlso pay attention to what this code does.\n\n\nbikes%&gt;%group_by(weekend)%&gt;%summarize(median(rides))\n\nmynewdatasetissmallerthantheoriginal&lt;-bikes%&gt;%filter(rides&lt;=700,weekend==FALSE,temp&gt;60)\nmynewdatasetissmallerthantheoriginal\n\nmynewdatasetusescelsius&lt;-bikes%&gt;%mutate(temp=(temp-32)*5/9)\nhead(mynewdatasetusescelsius)\n\n\n\nSolution\n\n\n# Calculate the median ridership by weekend\n# Put each new thought or action on its own line! \n# This makes it easier to follow the steps.\nbikes %&gt;% \n  group_by(weekend) %&gt;% \n  summarize(median(rides))\n\n# A tibble: 2 × 2\n  weekend `median(rides)`\n  &lt;lgl&gt;             &lt;dbl&gt;\n1 FALSE              3848\n2 TRUE               2955\n\n# Obtain days on which there are at most 700 rides,\n# it's the weekend, and temps are above 60 degrees\n# Use a shorter name that's easier to read and type.\n# Add spaces to make things easier to read.\n# Add line breaks to make it easier to follow the steps.\nwarm_weekends &lt;- bikes %&gt;%\n  filter(rides &lt;= 700, weekend == FALSE, temp &gt; 60)\nwarm_weekends\n\n  rides windspeed temp weekend\n1   577        18   67   FALSE\n2   655        18   68   FALSE\n3    20        24   72   FALSE\n\n# Store temp in Celsius\nbikes_celsius &lt;- bikes %&gt;% \n  mutate(temp = (temp - 32)*5/9)\nhead(bikes_celsius)\n\n  rides windspeed      temp weekend\n1   654        11 18.333333    TRUE\n2   670        17 17.777778    TRUE\n3  1229        17  9.444444   FALSE\n4  1454        11 10.555556   FALSE\n5  1518        13 11.666667   FALSE\n6  1518         6 11.666667   FALSE\n\n\n\n\n\nSTAT 155 Review: model interpretation & application Let’s interpret and apply bike_model_2.\n\n\n___ %&gt;% \n  tidy()\n\n\nHow can we interpret the temp coefficient?\n\n\nWe expect roughly 54 more riders on warm days.\nWe expect roughly 54 more riders per every 1 degree increase in temperature.\nWhen controlling for windspeed and weekend status, we expect roughly 54 more riders on warm days.\nWhen controlling for windspeed and weekend status, we expect roughly 54 more riders per every 1 degree increase in temperature.\n\n\nHow can we interpret the weekendTRUE coefficient?\n\n\nWe expect roughly 858 fewer riders on weekends.\nWe expect roughly 858 fewer riders per every extra weekend.\nWhen controlling for windspeed and temperature, we expect roughly 858 fewer riders on weekends.\nWhen controlling for windspeed and temperature, we expect roughly 858 fewer riders per every extra weekend.\n\n\nReproduce the predicted ridership and corresponding residual for day 1 from scratch (how were these calculated?!):\n\n\nbike_model_2 %&gt;% \n  ___(new_data = bikes) %&gt;% \n  head(1)\n\n\n\nSolution\n\n\n# Get the coefficients\nbike_model_2 %&gt;% \n  tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    298.     289.        1.03 3.02e- 1\n2 windspeed      -35.6      9.00     -3.95 8.46e- 5\n3 temp            54.3      3.18     17.1  3.82e-55\n4 weekendTRUE   -858.     101.       -8.46 1.47e-16\n\n\n\nWhen controlling for windspeed and weekend status, we expect roughly 54 more riders per every 1 degree increase in temperature.\nWhen controlling for windspeed and temperature, we expect roughly 858 fewer riders on weekends (compared to weekdays).\n\n\n# Predict ridership on day 1\nbike_model_2 %&gt;% \n  augment(new_data = bikes) %&gt;% \n  head(1)\n\n# A tibble: 1 × 6\n  .pred .resid rides windspeed  temp weekend\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;  \n1 2581. -1927.   654        11    65 TRUE   \n\n# This matches .pred\n298.45 - 35.57*11 + 54.33*65 - 857.76*1\n\n[1] 2580.87\n\n# Calculate the residual (observed - predicted)\n# This matches .resid\n654 - 2580.87\n\n[1] -1926.87\n\n\n\n\n\nSTAT 155 Review: data wrangling Through the “Details: communication & code style” and elsewhere, you’ve reviewed the use of various dplyr data wrangling verbs: filter(), mutate(), summarize(), group_by(), and select(). Use these to complete the following tasks.\n\n\nCalculate the mean temperature across all days in the data set.\nCalculate the mean temperature on weekends vs weekdays.\nPrint out the 3 days with the highest temperatures. HINT: arrange() or arrange(desc())\nName and store a new data set which: - only includes the days that fall on a weekend and have temps below 80 degrees - has a new variable, temp_above_freezing, which calculates how far the temperature is above (or below) freezing (32 degrees F) - only includes the windspeed, temp, and temp_above_freezing variables.\n\n\n\nSolution\n\n\n# a\nbikes %&gt;% \n  summarize(mean(temp))\n\n  mean(temp)\n1   74.69083\n\n# b\nbikes %&gt;% \n  group_by(weekend) %&gt;% \n  summarize(mean(temp))\n\n# A tibble: 2 × 2\n  weekend `mean(temp)`\n  &lt;lgl&gt;          &lt;dbl&gt;\n1 FALSE           75.1\n2 TRUE            73.7\n\n# c\nbikes %&gt;% \n  arrange(desc(temp)) %&gt;% \n  head(3)\n\n  rides windspeed temp weekend\n1  2825         9  108   FALSE\n2  3152        15  106   FALSE\n3  2298         9  104    TRUE\n\n# d\nnew_data &lt;- bikes %&gt;% \n  filter(weekend == TRUE, temp &lt; 80) %&gt;% \n  mutate(temp_above_freezing = temp - 32) %&gt;% \n  select(windspeed, temp, temp_above_freezing)\nhead(new_data)\n\n  windspeed temp temp_above_freezing\n1        11   65                  33\n2        17   64                  32\n3        18   47                  15\n4        24   42                  10\n5        11   54                  22\n6        13   53                  21\n\n\n\n\n\nSTAT 155 Review: plots\nConstruct plots of the following relationships:\n\n\nrides vs temp\nrides vs weekend\nrides vs temp and weekend\n\n\n\nSolution\n\n\n# a. rides vs temp\nggplot(bikes, aes(y = rides, x = temp)) + \n  geom_point()\n\n\n\n\n\n\n\n# b. rides vs weekend\nggplot(bikes, aes(y = rides, x = weekend)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(bikes, aes(x = rides, fill = weekend)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n# c. rides vs temp and weekend\n ggplot(bikes, aes(y = rides, x = temp, color = weekend)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nDone!\n\n\nKnit/render your notes.\nCheck the solutions on the course website.\nGet a head start on the wrap-up steps below.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L02-evaluating-regression-models.html#footnotes",
    "href": "L02-evaluating-regression-models.html#footnotes",
    "title": "2  Model Evaluation",
    "section": "",
    "text": "Stefanski, Leonard A. (2007). Residual (Sur)Realism. “The American Statistician,” 61, pp 163-177.↩︎",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Evaluation</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html",
    "href": "L03-overfitting.html",
    "title": "3  Overfitting",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#directions",
    "href": "L03-overfitting.html#directions",
    "title": "3  Overfitting",
    "section": "Directions",
    "text": "Directions\nLet’s build and evaluate a predicted model of an adult’s height (\\(y\\)) using some predictors \\(x_i\\) (e.g., age, weight, etc.).\n\nIntroduce yourself in whatever way you feel appropriate and check in with each other as human beings\nCome up with a team name\nWork through the steps below as a group, after you are told your group number\n\nEach group will be given a different sample of 40 adults\nStart by predicting height (in) using hip circumference (cm)\nEvaluate the model on your sample.\n\nBe prepared to share your answers to:\n\nHow good is your simple model?\nWhat would happen if we added more predictors?",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#questions",
    "href": "L03-overfitting.html#questions",
    "title": "3  Overfitting",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\nGoal:\n\nLet’s build and evaluate a predictive model of an adult’s height (\\(y\\)) using some predictors \\(x_i\\) (eg: age, height, etc).\nSince \\(y\\) is quantitative this is a regression task.\nThere are countless possible models of \\(y\\) vs \\(x\\). We’ll utilize a linear regression model:\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p + \\varepsilon\\]\n\nAnd after building this model, we’ll evaluate it.\n\n\n\n\nData: Each group will be given a different sample of 40 adults.\n\n# Load packages needed for this analysis\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n# Load your data: fill in the blanks at end of url with your number\n# group 1 = 50\n# group 2 = 143\n# group 3 = 160\n# group 4 = 174\n# group 5 = 86\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat___.csv\") %&gt;% \n  filter(ankle &lt; 30) %&gt;% \n  rename(body_fat = fatSiri)\n\n\n# Check out a density plot of your y outcomes\nggplot(humans, aes(x = height)) + \n  geom_density()\n\n\n\nModel building: Build a linear regression model of height (in) by hip circumference (cm).\n\n# STEP 1: model specification\nlm_spec &lt;- ___() %&gt;% \n  set_mode(___) %&gt;% \n  set_engine(___)\n\n\n# STEP 2: model estimation\nmodel_1 &lt;- ___ %&gt;% \n  ___(height ~ hip, data = humans)\n\n\n# Check out the coefficients\n# Do all groups have the same coefficients? Should they?\n\n\n\nSolution\n\nEach group will have slightly different coefficients because they have different samples of data.\n\n#This is an example with one of the samples\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat50.csv\") %&gt;% \n  filter(ankle &lt; 30) %&gt;% \n  rename(body_fat = fatSiri)\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode('regression') %&gt;% \n  set_engine('lm')\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip, data = humans)\n\n# Check out the coefficients\nmodel_1  %&gt;% \n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)   52.5      7.68        6.84 0.0000000460\n2 hip            0.179    0.0778      2.30 0.0272      \n\n\n\n\nModel evaluation: How good is our model?\n\n# Calculate the R^2 for model_1\n\n\n# Use your model to predict height for your subjects\n# Just print the first 6 results\nmodel_1 %&gt;% \n  ___(new_data = ___) %&gt;% \n  head()\n\n\n# Calculate the MAE, i.e. typical prediction error, for your model\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  ___(truth = ___, estimate = ___)\n\n\n\nSolution\n\nAgain, each group will have slightly different answers here because they have different samples of data.\n\n# Calculate the R^2 for model_1\nmodel_1 %&gt;%\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.125         0.101  2.26      5.29  0.0272     1  -86.1  178.  183.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Use your model to predict height for your subjects\n# Just print the first 6 results\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  head()\n\n# A tibble: 6 × 21\n  .pred .resid fatBrozek body_fat density   age weight height adiposity\n  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  70.3  -1.09      24.7     25.4    1.04    43   177    69.2      26  \n2  70.8  -1.52      22       22.5    1.05    38   187.   69.2      27.5\n3  70.2  -1.19       9.4      8.8    1.08    29   161.   69        23.8\n4  68.9   4.58       7.1      6.3    1.08    49   153.   73.5      19.9\n5  69.3   2.91       9.9      9.4    1.08    23   160.   72.2      21.6\n6  70.2  -2.48      22.7     23.3    1.05    52   167    67.8      25.6\n# ℹ 12 more variables: fatFreeWeight &lt;dbl&gt;, neck &lt;dbl&gt;, chest &lt;dbl&gt;,\n#   abdomen &lt;dbl&gt;, hip &lt;dbl&gt;, thigh &lt;dbl&gt;, knee &lt;dbl&gt;, ankle &lt;dbl&gt;,\n#   biceps &lt;dbl&gt;, forearm &lt;dbl&gt;, wrist &lt;dbl&gt;, hipin &lt;dbl&gt;\n\n# Calculate the MAE, i.e. typical prediction error, for your model\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.88\n\n\n\n\nReflection\nIn addition to hip circumference, suppose we incorporated more predictors into our model of height. What would happen to \\(R^2\\)? To the MAE?\n\n\nSolution\n\n\\(R^2\\) would increase and MAE would decrease.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#directions-1",
    "href": "L03-overfitting.html#directions-1",
    "title": "3  Overfitting",
    "section": "Directions",
    "text": "Directions\n\nTake 5 minutes to complete exercises 1 and 2 (choosing one of three models).\nWe’ll pause for a few minutes to discuss each group’s answers to these exercises.\nThen, and only then, you can finish exercises 3 - 5.\n\nREMINDERS:\n\nBe kind to yourself/each other. You will make mistakes!\nCollaborate:\n\nactively contribute to discussion (don’t work on your own)\nactively include all group members in discussion\ncreate a space where others feel comfortable making mistakes and sharing their ideas\nstay in sync",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#questions-1",
    "href": "L03-overfitting.html#questions-1",
    "title": "3  Overfitting",
    "section": "Questions",
    "text": "Questions\n\nSelect a model\n\nConsider 3 different models of height, estimated below. As a group, use your data to choose which is the best predictive model of height. Calculate the MAE for this model.\n\n# height vs hip\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip, data = humans)\nmodel_1 %&gt;% \n  tidy()\n\n# height vs hip & weight\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight, data = humans)\nmodel_2 %&gt;% \n  tidy()\n\n# height vs a lot of predictors (AND some interaction terms)\nmodel_3 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight * body_fat * abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\nmodel_3 %&gt;% \n  tidy()\n\n\n# Calculate the MAE for your model\n___ %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n\n\nSolution\n\nWill vary by group. MAE is calculated here for each model.\n\n# Build the models\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip, data = humans)\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight, data = humans)\nmodel_3 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight * body_fat * abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\n\n# Evaluate the models\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.88\n\nmodel_2 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.67\n\nmodel_3 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard    1.53e-10\n\n\n\n\n\nShare your results\nOnly when you’re done with exercise 1:\n\nOpen this “Top Model Competition” Google Doc.\nRecord your team name.\nRecord which model you chose (1, 2, or 3).\nRecord the MAE for your model.\nWAIT. Don’t keep going.\n\n\n\nDon’t peak\nWhat do you know?! 40 new people just walked into the doctor’s office and the doctor wants to predict their height:\n\n# Import the new data\nnew_patients &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat182.csv\") %&gt;% \n  filter(ankle &lt; 30) %&gt;% \n  rename(body_fat = fatSiri)\n\n\n\n\n\nIntuition\nConsider using your model to predict height for these 40 new subjects. On average, do you think these predictions will be better or worse than for your original patients? Why?\n\n\n\n\n\nHow well does your model do in the real world?\nUse your model to predict height for the new patients and calculate the typical prediction error (MAE). Record this in the Google sheet.\n\n\n___ %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n\n\nSolution\n\n\n# Predict height (assume, for example, I choose model_1)\nmodel_1 %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  head()\n\n# A tibble: 6 × 21\n  .pred .resid fatBrozek body_fat density   age weight height adiposity\n  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  71.5 -1.96       27.1     28      1.04    62   201.   69.5      29.3\n2  70.4 -0.141      20.9     21.3    1.05    42   163    70.2      23.3\n3  69.7 -0.497      26.1     27      1.04    72   168    69.2      24.7\n4  69.1 -1.39        4.1      3      1.09    35   152.   67.8      23.4\n5  68.5 -2.99        1.9      0.7    1.1     35   126.   65.5      20.6\n6  71.9 -1.91       31       32.3    1.03    57   206.   70        29.5\n# ℹ 12 more variables: fatFreeWeight &lt;dbl&gt;, neck &lt;dbl&gt;, chest &lt;dbl&gt;,\n#   abdomen &lt;dbl&gt;, hip &lt;dbl&gt;, thigh &lt;dbl&gt;, knee &lt;dbl&gt;, ankle &lt;dbl&gt;,\n#   biceps &lt;dbl&gt;, forearm &lt;dbl&gt;, wrist &lt;dbl&gt;, hipin &lt;dbl&gt;\n\n\n\n# Calculate the MAE for model_1\nmodel_1 %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.73\n\n# Calculate the MAE for model_2\nmodel_2 %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.68\n\n# Calculate the MAE for model_3\nmodel_3 %&gt;% \n  augment(new_data = new_patients) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        105.\n\n\n\n\n\nReflection\nIn summary, which model seems best? What’s the central theme here?",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#overfitting",
    "href": "L03-overfitting.html#overfitting",
    "title": "3  Overfitting",
    "section": "Overfitting",
    "text": "Overfitting\nWhen we add more and more predictors into a model, it can become overfit to the noise in our sample data:\n\nour model loses the broader trend / big picture\nthus does not generalize to new data\nthus results in bad predictions and a bad understanding of the relationship among the new data points\n\nPreventing overfitting: training and testing\n\nIn-sample metrics, i.e. measures of how well the model performs on the same sample data that we used to build it, tend to be overly optimistic and lead to overfitting.\nInstead, we should build and evaluate, or train and test, our model using different data.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#r-code",
    "href": "L03-overfitting.html#r-code",
    "title": "3  Overfitting",
    "section": "R Code",
    "text": "R Code\nThis section is for future reference. It is a summary of code you’ll learn below for creating and applying training and testing data. Throughout, suppose we wish to build and evaluate a linear regression model of y vs x1 and x2 using our sample_data.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nSplit the sample data into training and test sets\n\n# Set the random number seed\nset.seed(___)\n\n# Split the sample_data\n# \"prop\" is the proportion of data assigned to the training set\n# it must be some number between 0 and 1\ndata_split &lt;- initial_split(sample_data, strata = y, prop = ___)\n\n# Get the training data from the split\ndata_train &lt;- data_split %&gt;% \n  training()\n\n# Get the testing data from the split\ndata_test &lt;- data_split %&gt;% \n  testing()\n\nBuild a training model\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: model estimation using the training data\nmodel_train &lt;- lm_spec %&gt;% \n  fit(y ~ x1 + x2, data = data_train)\n\nUse the training model to make predictions for the test data\n\n# Make predictions\nmodel_train %&gt;% \n  augment(new_data = data_test)\n\nEvaluate the training model using the test data\n\n# Calculate the test MAE\nmodel_train %&gt;% \n  augment(new_data = data_test) %&gt;% \n  mae(truth = y, estimate = .pred)",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#directions-2",
    "href": "L03-overfitting.html#directions-2",
    "title": "3  Overfitting",
    "section": "Directions",
    "text": "Directions\n\nOpen the Part 2 QMD file\nSame directions as before:\n\nBe kind to yourself/each other\nCollaborate\n\nWe will not discuss these exercises as a class. Be sure to ask questions as I walk around the room.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#questions-2",
    "href": "L03-overfitting.html#questions-2",
    "title": "3  Overfitting",
    "section": "Questions",
    "text": "Questions\nThe following exercises are inspired by Chapter 5.3.1 of ISLR.\n\n# Load packages & data\n# NOTE: You might first need to install the ISLR package\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR)\ndata(Auto)\ncars &lt;- Auto %&gt;% \n  dplyr::select(mpg, horsepower, year)\n\nLet’s use the cars data to compare three linear regression models of fuel efficiency in miles per gallon (mpg) by engine power (horsepower):\n\n# Raw data\ncars_plot &lt;- ggplot(cars, aes(x = horsepower, y = mpg)) + \n  geom_point()\ncars_plot\n\n\n# model 1: 1 predictor (y = b0 + b1 x)\ncars_plot + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n# model 2: 2 predictors (y = b0 + b1 x + b2 x^2)\ncars_plot + \n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 2))\n\n\n# model 3: 19 predictors (y = b0 + b1 x + b2 x^2 + ... + b19 x^19)\ncars_plot + \n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 19))\n\nGoal\nLet’s evaluate and compare these models by training and testing them using different data.\n\n155 review: set.seed()\n\nRun the two chunks below multiple times each. Afterward, summarize what set.seed() does and why it’s important to being able to reproduce a random sample.\n\nsample_n(cars, 2)\n\n\nset.seed(253)\nsample_n(cars, 2)\n\n\n\nSolution\n\nset.seed() is used to create the same “random numbers” each time a random function is called.\nNote that is if you want to get exactly the same random result, set.seed() needs to be run right before the call to random function, every time.\nIt is important so that you can reproduce the same random sample every time you knit your work.\nThere might be different results across computers/platforms as they might be using different pseudo-random number generators. The most important thing is for your code to be consistent.\n\n\n\nTraining and test sets\n\nLet’s randomly split our original 392 sample cars into two separate pieces: select 80% of the cars to train (build) the model and the other 20% to test (evaluate) the model.\n\n# Set the random number seed\nset.seed(8)\n    \n# Split the cars data into 80% / 20%\n# Ensure that the sub-samples are similar with respect to mpg\ncars_split &lt;- initial_split(cars, strata = mpg, prop = 0.8)\n\n\n# Check it out\n# What do these numbers mean?\ncars_split\n\n\n# Get the training data from the split\ncars_train &lt;- cars_split %&gt;% \n  training()\n    \n# Get the testing data from the split\ncars_test &lt;- cars_split %&gt;% \n  testing()\n\n\n# The original data has 392 cars\nnrow(cars)\n    \n# How many cars are in cars_train?\n    \n# How many cars are in cars_test?\n\n\n\nSolution\n\n\n# Set the random number seed\nset.seed(8)\n\n# Split the cars data into 80% / 20%\n# Ensure that the sub-samples are similar with respect to mpg\ncars_split &lt;- initial_split(cars, strata = mpg, prop = 0.8)\ncars_split\n\n&lt;Training/Testing/Total&gt;\n&lt;312/80/392&gt;\n\n# Get the training data from the split\ncars_train &lt;- cars_split %&gt;% \n  training()\n\n# Get the testing data from the split\ncars_test &lt;- cars_split %&gt;% \n  testing()\n\n# The original data has 392 cars\nnrow(cars)\n\n[1] 392\n\n# How many cars are in cars_train?\nnrow(cars_train)\n\n[1] 312\n\n# How many cars are in cars_test?\nnrow(cars_test)\n\n[1] 80\n\n\n\n\n\nReflect on the above code\n\n\nWhy do we want the training and testing data to be similar with respect to mpg (strata = mpg)? What if they weren’t?\nWhy did we need all this new code instead of just using the first 80% of cars in the sample for training and the last 20% for testing?\n\n\n\nSolution\n\n\nSuppose, for example, the training cars all had higher mpg than the test cars. Then the training model likely would not perform well on the test cars, thus we’d get an overly pessimistic measure of model quality.\nIf the cars are ordered in some way (eg: from biggest to smallest) then our training and testing samples would have systematically different properties.\n\n\n\n\nBuild the training model\n\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: model estimation using the training data\n# Construct the 19th order polynomial model using the TRAINING data\nmodel_19_train &lt;- ___ %&gt;% \n  ___(mpg ~ poly(horsepower, 19), data = ___)\n\n\n\nSolution\n\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \nset_mode(\"regression\") %&gt;% \nset_engine(\"lm\")\n\n# STEP 2: model estimation using the training data\n# Construct the 19th order polynomial model using the TRAINING data\nmodel_19_train &lt;- lm_spec %&gt;% \nfit(mpg ~ poly(horsepower, 19), data = cars_train)\n\n\n\n\nEvaluate the training model\n\n\n# How well does the TRAINING model predict the TRAINING data?\n# Calculate the training (in-sample) MAE\nmodel_19_train %&gt;% \n  augment(new_data = ___) %&gt;% \n  mae(truth = mpg, estimate = .pred)\n\n\n# How well does the TRAINING model predict the TEST data?\n# Calculate the test MAE\nmodel_19_train %&gt;% \n  augment(new_data = ___) %&gt;% \n  mae(truth = mpg, estimate = .pred)\n\n\n\nSolution\n\n\n# How well does the training model predict the training data?\n# Calculate the training (in-sample) MAE\nmodel_19_train %&gt;% \naugment(new_data = cars_train) %&gt;% \nmae(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        2.99\n\n# How well does the training model predict the test data?\n# Calculate the test MAE\nmodel_19_train %&gt;% \naugment(new_data = cars_test) %&gt;% \nmae(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        6.59\n\n\n\n\n\nPunchline\nThe table below summarizes your results for train_model_19 as well as the other two models of interest. (You should confirm the other two model results outside of class!)\n\n\n\n\nModel\nTraining MAE\nTesting MAE\n\n\n\n\nmpg ~ horsepower\n3.78\n4.00\n\n\nmpg ~ poly(horsepower, 2)\n3.20\n3.49\n\n\nmpg ~ poly(horsepower, 19)\n2.99\n6.59\n\n\n\nAnswer the following and reflect on why each answer makes sense:\n\nWithin each model, how do the training errors compare to the testing errors? (This isn’t always the case, but is common.)\n\nWhy about the training and test errors for the third model suggest that it is overfit to our sample data?\nWhich model seems the best with respect to the training errors?\n\nWhich model is the best with respect to the testing errors?\n\nWhich model would you choose?\n\n\n\nSolution\n\n\nthe training errors are smaller\n\nthe test MAE is much larger than the training MAE\nthe 19th order polynomial\n\nthe quadratic\n\nthe quadratic\n\nCode for the curious\nI wrote a function calculate_MAE() to automate the calculations in the table. If you’re curious, pick through this code!\n\n# Write function to calculate MAEs\ncalculate_MAE &lt;- function(poly_order){\n  # Construct a training model\n  model &lt;- lm_spec %&gt;% \n    fit(mpg ~ poly(horsepower, poly_order), cars_train)\n  \n  # Calculate the training MAE\n  train_it &lt;- model %&gt;% \n    augment(new_data = cars_train) %&gt;% \n    mae(truth = mpg, estimate = .pred)\n      \n  # Calculate the testing MAE\n  test_it &lt;- model %&gt;% \n    augment(new_data = cars_test) %&gt;% \n    mae(truth = mpg, estimate = .pred)\n      \n  # Return the results\n  return(data.frame(train_MAE = train_it$.estimate, test_MAE = test_it$.estimate))\n}\n    \n# Calculate training and testing MSEs\ncalculate_MAE(poly_order = 1)\n\n  train_MAE test_MAE\n1  3.779331 4.004333\n\ncalculate_MAE(poly_order = 2)\n\n  train_MAE test_MAE\n1  3.199882 3.487022\n\ncalculate_MAE(poly_order = 19)\n\n  train_MAE test_MAE\n1  2.989305 6.592341\n\n\n\n# For those of you interested in trying all orders...\n\nresults &lt;- purrr::map_df(1:19,calculate_MAE) %&gt;% \n  mutate(order = 1:19) %&gt;%\n  pivot_longer(cols=1:2,names_to='Metric',values_to = 'MAE') \n\nresults %&gt;%\n  ggplot(aes(x = order, y = MAE, color = Metric)) + \n  geom_line() + \n  geom_point(data = results %&gt;% filter(Metric == 'test_MAE') %&gt;% slice_min(MAE)) + \n  geom_point(data = results %&gt;% filter(Metric == 'train_MAE') %&gt;% slice_min(MAE))\n\n\n\n\n\n\n\n\n\n\n\nFinal reflection\n\nThe training / testing procedure provided a more honest evaluation and comparison of our model predictions. How might we improve upon this procedure? What problems can you anticipate in splitting our data into 80% / 20%?\nSummarize the key themes from today in your own words.\n\n\n\n\n\n\nSolution\n\nThis will be discussed in the next video!\n\n\n\nSTAT 155 REVIEW: data drill\n\n\nConstruct and interpret a plot of mpg vs horsepower and year.\nCalculate the average mpg.\nCalculate the average mpg for each year. HINT: group_by()\nPlot the average mpg by year.\n\n\n\nSolution\n\n\n# a. One of many options\nggplot(cars, aes(x = horsepower, y = mpg, color = year)) + \n  geom_point()\n\n\n\n\n\n\n\n# b\ncars %&gt;% \nsummarize(mean(mpg))\n\n  mean(mpg)\n1  23.44592\n\n# c\ncars %&gt;% \n  group_by(year) %&gt;% \n  summarize(mean_mpg = mean(mpg))\n\n# A tibble: 13 × 2\n    year mean_mpg\n   &lt;dbl&gt;    &lt;dbl&gt;\n 1    70     17.7\n 2    71     21.1\n 3    72     18.7\n 4    73     17.1\n 5    74     22.8\n 6    75     20.3\n 7    76     21.6\n 8    77     23.4\n 9    78     24.1\n10    79     25.1\n11    80     33.8\n12    81     30.2\n13    82     32  \n\n# d\ncars %&gt;% \n  group_by(year) %&gt;% \n  summarize(mean_mpg = mean(mpg)) %&gt;% \n  ggplot(aes(y = mean_mpg, x = year)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nDigging deeper (optional)\n\nCheck out the online solutions for exercise 6. Instead of calculating MAE from scratch for 3 different models, I wrote a function calculate_MAE() to automate the process. After picking through this code, adapt the function so that it also returns the \\(R^2\\) value of each model.\nDone!\n\nKnit your notes.\nCheck the solutions in the course website.\nIf you finish all that during class, start your homework!",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#finishing-the-activity",
    "href": "L03-overfitting.html#finishing-the-activity",
    "title": "3  Overfitting",
    "section": "Finishing the Activity",
    "text": "Finishing the Activity\n\nIf you didn’t finish the activity, no problem! Be sure to complete the activity outside of class, review the solutions in the online manual, and ask any questions on Slack or in office hours.\nRe-organize and review your notes to help deepen your understanding, solidify your learning, and make homework go more smoothly!",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L03-overfitting.html#after-class",
    "href": "L03-overfitting.html#after-class",
    "title": "3  Overfitting",
    "section": "After Class",
    "text": "After Class\n\nAn R code video, posted under the pre-course materials for today’s class (see the “Schedule” page on this website), talks through the new code. This video is OPTIONAL. Decide what’s right for you.\nContinue to check in on Slack. I’ll be posting announcements there from now on.\nUpcoming due dates:\n\nCP3: due 10 minutes before our next class.\n\nThere are two (short) videos to watch in advance.\n\nHW1 (Regression Model Evaluation): due next Tuesday at 11:59 pm\n\nStart today, even if you just review the directions and scan the exercises. Homework is not designed to be completed in one sitting!\nInvite others to work with you!\n\nStop by office hours (preceptors or mine) with any questions",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Overfitting</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html",
    "href": "L04-cross-validation.html",
    "title": "4  Cross-Validation",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#context-evaluating-regression-models",
    "href": "L04-cross-validation.html#context-evaluating-regression-models",
    "title": "4  Cross-Validation",
    "section": "Context: Evaluating Regression Models",
    "text": "Context: Evaluating Regression Models\nA reminder of our current context:\n\n\n\n\nworld = supervised learning\nWe want to build a model some output variable \\(y\\) by some predictors \\(x\\).\ntask = regression\n\\(y\\) is quantitative\nmodel = linear regression model via least squares algorithm\nWe’ll assume that the relationship between \\(y\\) and \\(x\\) can be represented by\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\varepsilon\\]\n\n\nGOAL: model evaluation\nWe want more honest metrics of prediction quality that\n\nassess how well our model predicts new outcomes; and\nhelp prevent overfitting.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#why-is-overfitting-so-bad",
    "href": "L04-cross-validation.html#why-is-overfitting-so-bad",
    "title": "4  Cross-Validation",
    "section": "Why is overfitting so bad?",
    "text": "Why is overfitting so bad?\nNot only can overfitting produce misleading models, it can have serious societal impacts. Examples:\n\nA former Amazon algorithm built to help sift through resumes was overfit to its current employees in leadership positions (who weren’t representative of the general population or candidate pool).\nFacial recognition algorithms are often overfit to the people who build them (who are not broadly representative of society). As one example, this has led to disproportionate bias in policing. For more on this topic, you might check out Coded Bias, a documentary by Shalini Kantayya which features MIT Media Lab researcher Joy Buolamwini.\nPolygenic risk scores (PRSs), which aim to predict a person’s risk of developing a particular disease/trait based on their genetics, are often overfit to the data on which they are built (which, historically, has exclusively—or at least primarily—included individuals of European ancestry). As a result, PRS predictions tend to be more accurate in European populations and new research suggests that their continued use in clinical settings could exacerbate health disparities.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#k-fold-cross-validation",
    "href": "L04-cross-validation.html#k-fold-cross-validation",
    "title": "4  Cross-Validation",
    "section": "k-fold Cross Validation",
    "text": "k-fold Cross Validation\nWe can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\n\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\n\nFit a model using the data in the other \\(k-1\\) folds (training).\n\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\n\nCalculate the MAE for fold \\(j\\) (testing): \\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\).\n\nCombine this information into one measure of model quality: \\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#definitions",
    "href": "L04-cross-validation.html#definitions",
    "title": "4  Cross-Validation",
    "section": "Definitions",
    "text": "Definitions\n\nalgorithm = a step-by-step procedure for solving a problem (Merriam-Webster)\ntuning parameter = a parameter or quantity upon which an algorithm depends, that must be selected or tuned to “optimize” the algorithm\n\n1",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#prompts",
    "href": "L04-cross-validation.html#prompts",
    "title": "4  Cross-Validation",
    "section": "Prompts",
    "text": "Prompts\n\nAlgorithms\n\n\nWhy is \\(k\\)-fold cross-validation an algorithm?\nWhat is the tuning parameter of this algorithm and what values can this take?\n\n\n\nSolution\n\n\nYes. It follows a list of steps to get to its goal.\n\\(k\\), the number of folds, is a tuning parameter. \\(k\\) can be any integer from 1, 2, …, \\(n\\) where \\(n\\) is our sample size.\n\n\n\n\nTuning the k-fold Cross-Validation algorithm\n\n\n\n\n\nLet’s explore k-fold cross-validation with some personal experience. Our class has a representative sample of cards from a non-traditional population (no “face cards”, not equal numbers, etc). We want to use these to predict whether a new card will be odd or even (a classification task).\n\nBased on all of our cards, do we predict the next card will be odd or even?\nYou’ve been split into 2 groups. Use 2-fold cross-validation to estimate the possible error of using our sample of cards to predict whether a new card will be odd or even. How’s this different than validation?\nRepeat for 3-fold cross-validation. Why might this be better than 2-fold cross-validation?\nRepeat for LOOCV, i.e. n-fold cross-validation where n is the number of students in this room. Why might this be worse than 3-fold cross-validation?\nWhat value of k do you think practitioners typically use?\n\n\n\nSolution\n\n\nUse the percentage of odd and percentage of even among the sample of cards to help you make a prediction.\nWe use both groups as training and testing, in turn.\nWe have a larger dataset to train our model on. We are less likely to get an unrepresentative set as our training data.\nPrediction error for 1 person is highly variable.\nIn practice, \\(k = 10\\) and \\(k=7\\) are common choices for cross-validation. This has been shown to hit the ‘sweet spot’ between the extremes of \\(k=n\\) (LOOCV) and \\(k=2\\).\n\n\n\\(k=2\\) only utilizes 50% of the data for each training model, thus might result in overestimating the prediction error\n\\(k=n\\) leave-one-out cross-validation (LOOCV) requires us to build \\(n\\) training models, thus might be computationally expensive for larger sample sizes \\(n\\). Further, with only one data point in each test set, the training sets have a lot of overlap. This correlation among the training sets can make the ultimate corresponding estimate of prediction error less reliable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Code Preview\n\nWe’ve been doing a 2-step process to build linear regression models using the tidymodels package:\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n  \n# STEP 2: model estimation\nmy_model &lt;- lm_spec %&gt;% \n  fit(\n    y ~ x1 + x2,\n    data = sample_data\n  )\n\nFor k-fold cross-validation, we can tweak STEP 2.\n\nDiscuss the code below and why we need to set the seed.\n\n\n# k-fold cross-validation\nset.seed(___)\nmy_model_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    y ~ x1 + x2, \n    resamples = vfold_cv(sample_data, v = ___), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n\nSolution\n\nThe process of creating the folds is random, so we should set the seed to have reproducibility within our work.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#obtain-k-fold-cross-validated-estimates-of-mae-and-r2",
    "href": "L04-cross-validation.html#obtain-k-fold-cross-validated-estimates-of-mae-and-r2",
    "title": "4  Cross-Validation",
    "section": "Obtain k-fold cross-validated estimates of MAE and \\(R^2\\)",
    "text": "Obtain k-fold cross-validated estimates of MAE and \\(R^2\\)\n(Review above for discussion of these steps.)\n\n# model specification\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# k-fold cross-validation\n# For \"v\", put your number of folds k\nset.seed(___)\nmodel_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    y ~ x1 + x2,\n    resamples = vfold_cv(sample_data, v = ___), \n    metrics = metric_set(mae, rsq)\n)",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#obtain-the-cross-validated-metrics",
    "href": "L04-cross-validation.html#obtain-the-cross-validated-metrics",
    "title": "4  Cross-Validation",
    "section": "Obtain the cross-validated metrics",
    "text": "Obtain the cross-validated metrics\n\nmodel_cv %&gt;% \n  collect_metrics()",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#get-the-mae-and-r-squared-for-each-test-fold",
    "href": "L04-cross-validation.html#get-the-mae-and-r-squared-for-each-test-fold",
    "title": "4  Cross-Validation",
    "section": "Get the MAE and R-squared for each test fold",
    "text": "Get the MAE and R-squared for each test fold\n\n# MAE for each test fold: Model 1\nmodel_cv %&gt;% \n  unnest(.metrics)",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#instructions",
    "href": "L04-cross-validation.html#instructions",
    "title": "4  Cross-Validation",
    "section": "Instructions",
    "text": "Instructions\n\nGo to the Course Schedule and find the QMD template for today\n\nSave this in your STAT 253 Notes folder, NOT your downloads!\n\nWork through the exercises implementing CV to compare two possible models predicting height\nSame directions as before:\n\nBe kind to yourself/each other\nCollaborate\nDON’T edit starter code (i.e., code with blanks ___). Instead, copy-paste into a new code chunk below and edit from there.\n\nAsk me questions as I move around the room",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#questions",
    "href": "L04-cross-validation.html#questions",
    "title": "4  Cross-Validation",
    "section": "Questions",
    "text": "Questions\n\n# Load packages and data\nlibrary(tidyverse)\nlibrary(tidymodels)\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat50.csv\") %&gt;% \n  filter(ankle &lt; 30) %&gt;% \n  rename(body_fat = fatSiri)\n\n\n\n\n\nReview: In-sample metrics\n\nUse the humans data to build two separate models of height:\n\n# STEP 1: model specification\nlm_spec &lt;- ___() %&gt;% \n  set_mode(___) %&gt;% \n  set_engine(___)\n\n\n# STEP 2: model estimation\nmodel_1 &lt;- ___ %&gt;% \n  ___(height ~ hip + weight + thigh + knee + ankle, data = humans)\nmodel_2 &lt;- ___ %&gt;% \n  ___(height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\n\nCalculate the in-sample R-squared for both models:\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% \n  ___()\n\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% \n  ___()\n\nCalculate the in-sample MAE for both models:\n\n# IN-SAMPLE MAE for model_1 = ???\nmodel_1 %&gt;% \n  ___(new_data = ___) %&gt;% \n  mae(truth = ___, estimate = ___)\n\n\n# IN-SAMPLE MAE for model_2 = ???\nmodel_2 %&gt;% \n  ___(new_data = ___) %&gt;% \n  mae(truth = ___, estimate = ___)\n\n\n\nSolution\n\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight + thigh + knee + ankle, data = humans)\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\n\n# IN-SAMPLE R^2 for model_1 = 0.40\nmodel_1 %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.401         0.310  1.98      4.42 0.00345     5  -78.8  172.  183.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# IN-SAMPLE R^2 for model_2 = 0.87\nmodel_2 %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.680  1.35      4.51 0.00205    23  -48.4  147.  188.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# IN-SAMPLE MAE for model_1 = 1.55\nmodel_1 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        1.55\n\n# IN-SAMPLE MAE for model_2 = 0.64\nmodel_2 %&gt;% \n  augment(new_data = humans) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       0.646\n\n\n\n\n\nIn-sample model comparison\nWhich model seems “better” by the in-sample metrics you calculated above? Any concerns about either of these models?\n\n\n\nSolution\n\nThe in-sample metrics are better for model_2, but from experience in our previous class, we should expect this to be overfit.\n\n\n\n10-fold CV\nComplete the code to run 10-fold cross-validation for our two models.\nmodel_1: height ~ hip + weight\nmodel_2: height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist\n\n\n# 10-fold cross-validation for model_1\nset.seed(253)\nmodel_1_cv &lt;- ___ %&gt;% \n  ___(\n    ___,\n    ___ = vfold_cv(___, v = ___), \n    ___ = metric_set(mae, rsq)\n  )\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- ___ %&gt;% \n  ___(\n    ___,\n    ___ = vfold_cv(___, v = ___), \n    ___ = metric_set(mae, rsq)\n  )\n\n\n\nSolution\n\n\n# 10-fold cross-validation for model_1\nset.seed(253)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(humans, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n# STEP 2: 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(humans, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\n\n\n\nCalculating the CV MAE\n\n\nUse collect_metrics() to obtain the cross-validated MAE and \\(R^2\\) for both models.\n\n\n# HINT\n___ %&gt;% \n  collect_metrics()\n\n\nInterpret the cross-validated MAE and \\(R^2\\) for model_1.\n\n\n\nSolution\n\n\n\n\n\n# model_1\n# CV MAE = 1.87, CV R-squared = 0.41\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   1.87     10   0.159 Preprocessor1_Model1\n2 rsq     standard   0.409    10   0.124 Preprocessor1_Model1\n\n# model_2\n# CV MAE = 2.47, CV R-squared = 0.53\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   2.47     10   0.396 Preprocessor1_Model1\n2 rsq     standard   0.526    10   0.122 Preprocessor1_Model1\n\n\n\nWe expect our first model to explain roughly 40% of variability in height among new adults, and to produce predictions of height that are off by 1.9 inches on average.\n\n\n\n\nDetails: fold-by-fold results\ncollect_metrics() gave the final CV MAE, or the average MAE across all 10 test folds. unnest(.metrics) provides the MAE from each test fold.\n\n\nObtain the fold-by-fold results for the model_1 cross-validation procedure using unnest(.metrics).\n\n\n# HINT\n___ %&gt;% \n  unnest(.metrics)\n\n\nWhich fold had the worst average prediction error and what was it?\nRecall that collect_metrics() reported a final CV MAE of 1.87 for model_1. Confirm this calculation by wrangling the fold-by-fold results from part a.\n\n\n\nSolution\n\n\n# a. model_1 MAE for each test fold\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;% \n  filter(.metric == \"mae\")\n\n# A tibble: 10 × 7\n   splits         id     .metric .estimator .estimate .config           .notes  \n   &lt;list&gt;         &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n 1 &lt;split [35/4]&gt; Fold01 mae     standard        2.22 Preprocessor1_Mo… &lt;tibble&gt;\n 2 &lt;split [35/4]&gt; Fold02 mae     standard        2.34 Preprocessor1_Mo… &lt;tibble&gt;\n 3 &lt;split [35/4]&gt; Fold03 mae     standard        2.56 Preprocessor1_Mo… &lt;tibble&gt;\n 4 &lt;split [35/4]&gt; Fold04 mae     standard        1.51 Preprocessor1_Mo… &lt;tibble&gt;\n 5 &lt;split [35/4]&gt; Fold05 mae     standard        1.81 Preprocessor1_Mo… &lt;tibble&gt;\n 6 &lt;split [35/4]&gt; Fold06 mae     standard        2.43 Preprocessor1_Mo… &lt;tibble&gt;\n 7 &lt;split [35/4]&gt; Fold07 mae     standard        1.61 Preprocessor1_Mo… &lt;tibble&gt;\n 8 &lt;split [35/4]&gt; Fold08 mae     standard        1.84 Preprocessor1_Mo… &lt;tibble&gt;\n 9 &lt;split [35/4]&gt; Fold09 mae     standard        1.28 Preprocessor1_Mo… &lt;tibble&gt;\n10 &lt;split [36/3]&gt; Fold10 mae     standard        1.10 Preprocessor1_Mo… &lt;tibble&gt;\n\n# b. fold 3 had the worst error (2.55)\n\n# c. use these metrics to confirm the 1.87 CV MAE for model_1\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;% \n  filter(.metric == \"mae\") %&gt;% \n  summarize(mean(.estimate))\n\n# A tibble: 1 × 1\n  `mean(.estimate)`\n              &lt;dbl&gt;\n1              1.87\n\n\n\n\n\n\n\n\nComparing models\nThe table below summarizes the in-sample and 10-fold CV MAE for both models.\n\n\n\n\n\nModel\nIN-SAMPLE MAE\n10-fold CV MAE\n\n\n\n\nmodel_1\n1.55\n1.87\n\n\nmodel_2\n0.64\n2.47\n\n\n\n\n\n\n\nBased on the in-sample MAE alone, which model appears better?\n\nBased on the CV MAE alone, which model appears better?\n\nBased on all of these results, which model would you pick?\nDo the in-sample and CV MAE suggest that model_1 is overfit to our humans sample data? What about model_2?\n\n\n\nSolution\n\n\nmodel_2\nmodel_1\nmodel_1 – model_2 produces bad predictions for new adults\nmodel_1 is NOT overfit – its predictions of height for new adults seem roughly as accurate as the predictions for the adults in our sample. model_2 IS overfit – its predictions of height for new adults are worse than the predictions for the adults in our sample.\n\n\n\n\nLOOCV\n\nReconsider model_1. Instead of estimating its prediction accuracy using the 10-fold CV MAE, use the LOOCV MAE. THINK: How many people are in our humans sample?\nHow does the LOOCV MAE compare to the 10-fold CV MAE of 1.87? NOTE: These are just two different approaches to estimating the same thing: the typical prediction error when applying our model to new data. Thus we should expect them to be similar.\n\nExplain why we technically don’t need to set.seed() for the LOOCV algorithm.\n\n\n\n\nSolution\n\n\nThere are 40 people in our sample, thus LOOCV is equivalent to 40-fold CV:\n\n\nnrow(humans)\n\n[1] 39\n\nmodel_1_loocv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(humans, v = nrow(humans)), \n    metrics = metric_set(mae)\n  )\n    \nmodel_1_loocv %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    1.82    39   0.179 Preprocessor1_Model1\n\n\n\nThe LOOCV MAE (1.82) is very similar to the 10-fold CV MAE (1.87).\nThere’s no randomness in the test folds. Each test fold is a single person.\n\n\n\n\nData drill\n\n\nCalculate the average height of people under 40 years old vs people 40+ years old.\nPlot height vs age among our subjects that are 30+ years old.\nFix this code:\n\n\nmodel_3&lt;-lm_spec%&gt;%fit(height~age,data=humans)\nmodel_3%&gt;%tidy()\n\n\n\nSolution\n\n\n# a (one of many solutions)\nhumans %&gt;% \n  mutate(younger_older = age &lt; 40) %&gt;% \n  group_by(younger_older) %&gt;% \n  summarize(mean(height))\n\n# A tibble: 2 × 2\n  younger_older `mean(height)`\n  &lt;lgl&gt;                  &lt;dbl&gt;\n1 FALSE                   70.4\n2 TRUE                    69.8\n\n# b\nhumans %&gt;% \n  filter(age &gt;= 30) %&gt;% \n  ggplot(aes(x = age, y = height)) + \n  geom_point()\n\n\n\n\n\n\n\n# c\nmodel_3 &lt;- lm_spec %&gt;%\n  fit(height ~ age, data = humans)\nmodel_3 %&gt;%\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  71.1       1.63      43.7   1.96e-33\n2 age          -0.0210    0.0363    -0.577 5.67e- 1\n\n\n\n\n\nReflection: Part 1\nThe “regular” exercises are over but class is not done! Your group should agree to either work on HW1 or the remaining reflection questions.\nThis is the end of Unit 1 on “Regression: Model Evaluation”! Let’s reflect on the technical content of this unit:\n\n\nWhat was the main motivation / goal behind this unit?\nWhat are the four main questions that were important to this unit?\nFor each of the following tools, describe how they work and what questions they help us address:\n\nR-squared\nresidual plots\nout-of-sample MAE\nin-sample MAE\nvalidation\ncross-validation\n\nIn your own words, define the following:\n\noverfitting\nalgorithm\ntuning parameter\n\nReview the new tidymodels syntax from this unit. Identify key themes and patterns.\n\n\n\n\nReflection: Part 2\nThe reflection above addresses your understanding of/progress toward our course learning goals. Consider the other components that have helped you worked toward this learning throughout Unit 1.\n\nWith respect to collaboration, reflect upon your strengths and what you might change in the next unit:\n\nHow actively did you contribute to group discussions?\nHow actively did you include all other group members in discussion?\nIn what ways did you (or did you not) help create a space where others feel comfortable making mistakes & sharing their ideas?\n\nWith respect to engagement, reflect upon your strengths and what you might change the next unit:\n\nDid you regularly attend, be on time for, & stay for the full class sessions?\nHave you not missed more than 3 in-person class sessions?\nWere you actively present during class (eg: not on your phone, not working on other courses, etc)?\nDid you stay updated on Slack?\nWhen you had questions, did you ask them on Slack or in OH?\n\nWith respect to preparation, how many of checkpoints 1–3 did you complete and pass?\nWith respect to exploration, did you complete and pass HW0? Are you on track to complete and pass HW1?\n\n\nDone!\n\nKnit/render your notes.\nCheck the solutions in the online manual.\nCheck out the wrap-up steps below.\nIf you finish all that during class, work on your homework!",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#todays-material",
    "href": "L04-cross-validation.html#todays-material",
    "title": "4  Cross-Validation",
    "section": "Today’s Material",
    "text": "Today’s Material\n\nIf you didn’t finish the activity, no problem! Be sure to complete the activity outside of class, review the solutions in the course site, and ask any questions on Slack or in office hours.\nThis is the end of Unit 1, so there are reflection questions at the bottom to help you organize the concepts in your mind.\nAn R Tutorial Video, talking through the new code, is posted under the materials for today’s class on the Course Schedule. This video is OPTIONAL. Decide what’s right for you.",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#upcoming-deadlines",
    "href": "L04-cross-validation.html#upcoming-deadlines",
    "title": "4  Cross-Validation",
    "section": "Upcoming Deadlines",
    "text": "Upcoming Deadlines\n\nCP4:\n\ndue 10 minutes before our next class\ncovers one R code video\n\nHW1:\n\ndue next Tuesday at 11:59 pm\nstart today if you haven’t already!\nreview the homework and late work/extension policies on Moodle/Syllabus\nuniversal flexibility: pass/revise grading (as long as your original submission meets certain criteria including on-time submission), late work grace period\n\ndeadline is so we can get timely feedback to you; if you cannot make a deadline, please send me an email/Slack DM (in advance!) and let me know how much time you need",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "L04-cross-validation.html#footnotes",
    "href": "L04-cross-validation.html#footnotes",
    "title": "4  Cross-Validation",
    "section": "",
    "text": "https://www.wallpaperflare.com/grayscale-photography-of-guitar-headstock-music-low-electric-bass-wallpaper-zzbyn↩︎",
    "crumbs": [
      "Regression: Model Evaluation (Unit 1)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "U02-motivation.html",
    "href": "U02-motivation.html",
    "title": "Motivating Question",
    "section": "",
    "text": "Question\nThe field of machine learning is most often associated with the building of predictive models, not inferential models. Specifically, the goal is to build a model which produces good predictions of our response variable \\(y\\), not one that necessarily lends itself to testing specific hypotheses about \\(y\\). In this case:",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "U02-motivation.html#question",
    "href": "U02-motivation.html#question",
    "title": "Motivating Question",
    "section": "",
    "text": "If we have access to a bunch of potential predictors \\(x\\), how can we decide which model to build?",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "U02-motivation.html#model-selection-methods",
    "href": "U02-motivation.html#model-selection-methods",
    "title": "Motivating Question",
    "section": "Model Selection Methods",
    "text": "Model Selection Methods\n\nVariable selection Identify a subset of predictors to use in our model of \\(y\\). Methods: best subset selection, backward stepwise selection, forward stepwise selection\nShrinkage / regularization Shrink / regularize the coefficients of all predictors toward or to 0. Methods: LASSO, ridge regression, elastic net (a combination of LASSO & ridge)\nDimension reduction Combine the predictors into a smaller set of new predictors. Methods: principal components regression",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "L05-model-selection.html",
    "href": "L05-model-selection.html",
    "title": "5  Model Selection",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#statistical-machine-learning-concepts",
    "href": "L05-model-selection.html#statistical-machine-learning-concepts",
    "title": "5  Model Selection",
    "section": "Statistical Machine Learning Concepts",
    "text": "Statistical Machine Learning Concepts\n\nGain intuition about different approaches to variable selection\nClearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms\nCompare best subset and stepwise algorithms in terms of optimality of output and computational time\nDescribe how selection algorithms can give a measure of variable importance",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#general-skills",
    "href": "L05-model-selection.html#general-skills",
    "title": "5  Model Selection",
    "section": "General Skills",
    "text": "General Skills\n\nHighlight: Collaborative Learning\n\nUnderstand and demonstrate characteristics of effective collaboration (team roles, interpersonal communication, self-reflection, awareness of social dynamics, advocating for yourself and others).\nDevelop a common purpose and agreement on goals.\nBe able to contribute questions or concerns in a respectful way.\nShare and contribute to the group’s learning in an equitable manner.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#collaborative-learning",
    "href": "L05-model-selection.html#collaborative-learning",
    "title": "5  Model Selection",
    "section": "Collaborative Learning",
    "text": "Collaborative Learning\nTake 5 minutes to reflect upon your work throughout Unit 1, particularly with respect to collaboration.\nReflect upon your strengths and what you might change in the next unit:\n\nHow actively did you contribute to group discussions?\nHow actively did you include ALL other group members in discussion?\nIn what ways did you (or did you not) help create a space where others feel comfortable making mistakes & sharing their ideas?",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#unit-1-reflection-continued",
    "href": "L05-model-selection.html#unit-1-reflection-continued",
    "title": "5  Model Selection",
    "section": "Unit 1 Reflection (continued)",
    "text": "Unit 1 Reflection (continued)\nIf you did not finish Exercises #9 and #10 from last class, please take time after class today to do so.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#context",
    "href": "L05-model-selection.html#context",
    "title": "5  Model Selection",
    "section": "Context",
    "text": "Context\n\n\n\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = regression\n\\(y\\) is quantitative\nmodel = linear regression\nWe’ll assume that the relationship between \\(y\\) and (\\(x_1, x_2, ..., x_p\\)) can be represented by\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\varepsilon\\]",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#inferential-v.-predictive-models",
    "href": "L05-model-selection.html#inferential-v.-predictive-models",
    "title": "5  Model Selection",
    "section": "Inferential v. Predictive Models",
    "text": "Inferential v. Predictive Models\nIn model building, the decision of which predictors to use depends upon our goal.\nInferential models\n\n\nGoal: Explore & test hypotheses about a specific relationship.\nPredictors: Defined by the goal.\nExample: An economist wants to understand how salaries (\\(y\\)) vary by age (\\(x_1\\)) while controlling for education level (\\(x_2\\)).\n\n\nPredictive models\n\n\nGoal: Produce the “best” possible predictions of \\(y\\).\nPredictors: Any combination of predictors that help us meet this goal.\nExample: A mapping app wants to provide users with quality estimates of arrival time (\\(y\\)) utilizing any useful predictors (eg: time of day, distance, route, speed limit, weather, day of week, traffic radar…)",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#model-selection-goals",
    "href": "L05-model-selection.html#model-selection-goals",
    "title": "5  Model Selection",
    "section": "Model Selection Goals",
    "text": "Model Selection Goals\nModel selection algorithms can help us build a predictive model of \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\nThere are 3 general approaches to this task:\n\n\nVariable selection (today)\nIdentify a subset of predictors to use in our model of \\(y\\).\nShrinkage / regularization (next class)\nShrink / regularize the coefficients of all predictors toward or to 0.\nDimension reduction (later in the semester)\nCombine the predictors into a smaller set of new predictors.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#instructions",
    "href": "L05-model-selection.html#instructions",
    "title": "5  Model Selection",
    "section": "Instructions",
    "text": "Instructions\nOpen the Part 1 QMD. Scroll down to the # Exercises section.\nAs a group, you’ll design a variable selection algorithm to pick which predictors to use in a predictive model of height. Specifically, you will:\n\n15 mins: come up with one algorithm, document it, and try it\n5 mins: try another group’s algorithm\n\nNOTE: This will NOT be perfect! Our goals are to:\n\nHave fun and work together!\nTap into your intuition for key questions and challenges in variable selection.\nDeepen your understanding of “algorithms” and “tuning parameters” by designing and communicating your own.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#questions",
    "href": "L05-model-selection.html#questions",
    "title": "5  Model Selection",
    "section": "Questions",
    "text": "Questions\nLet’s build a predictive model of height in inches using one or more of 12 possible predictors. Other than age and weight, these are circumferences measured in cm.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# Load data\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat1.csv\")\nnames(humans)\n\n [1] \"age\"     \"weight\"  \"neck\"    \"chest\"   \"abdomen\" \"hip\"     \"thigh\"  \n [8] \"knee\"    \"ankle\"   \"biceps\"  \"forearm\" \"wrist\"   \"height\" \n\n\nA heat map displays correlations for each pair of variables in our dataset. Not only is height correlated with multiple predictors, the predictors are correlated with one another (mulicollinear)! We don’t need all of them in our model.\n\n# Get the correlation matrix\nlibrary(reshape2)\ncor_matrix &lt;- cor(humans)\ncor_matrix[lower.tri(cor_matrix)] &lt;- NA\ncor_matrix &lt;- cor_matrix %&gt;% \n  melt() %&gt;% \n  na.omit() %&gt;% \n  rename(correlation = value)\n\n# Visualize the correlation for each pair of variables\nggplot(cor_matrix, aes(x = Var1, y = Var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\", \n    midpoint = 0, limit = c(-1,1)) +\n  labs(x = \"\", y = \"\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\nDesign your own algorithm (15 minutes)\n\nDo not use any materials from outside this class.\nDocument your algorithm in words (not code) in this google doc.\nYour algorithm must:\n\nbe clear to other humans\nbe clear to a machine (cannot utilize context)\nlead to a single model that uses 0-12 of our predictors\ndefine and provide directions for selecting any tuning parameters\n\nImplement as many steps of your algorithm as possible in the time allotted. You can modify the code below to build and evaluate the models in your algorithm:\n\n\n\n# STEP 1: model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: model estimation\nheight_model_1 &lt;- lm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)\n\n# Check it out\nheight_model_1 %&gt;% \n  tidy()\n\n# CV MAE\nset.seed(253)\nlm_spec %&gt;% \n  fit_resamples(\n    height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(humans, v = 10), \n    metrics = metric_set(mae)\n  ) %&gt;% \n  collect_metrics()\n\n\n\nTest another group’s algorithm (5 minutes)\nTry to implement the next algorithm below yours (or the first algorithm if your group’s is last). Think: Are the steps clear? What are the drawbacks to the algorithm?",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#goal",
    "href": "L05-model-selection.html#goal",
    "title": "5  Model Selection",
    "section": "Goal",
    "text": "Goal\nLet’s consider three existing variable selection algorithms.\nHeads up: these algorithms are important to building intuition for the questions and challenges in model selection, BUT have major drawbacks.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-1-best-subset-selection-algorithm",
    "href": "L05-model-selection.html#example-1-best-subset-selection-algorithm",
    "title": "5  Model Selection",
    "section": "Example 1: Best Subset Selection Algorithm",
    "text": "Example 1: Best Subset Selection Algorithm\n\n\nBuild all \\(2^p\\) possible models that use any combination of the available predictors \\((x_1, x_2,..., x_p)\\).\n\nIdentify the best model with respect to some chosen metric (eg: CV MAE) and context.\n\n\nSuppose we used this algorithm for our height model with 12 possible predictors. What’s the main drawback?\n\n\nSolution\n\nIt’s computationally expensive. For our humans example, we’d need to build 4096 models:\n\n2^12\n\n[1] 4096",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-2-backward-stepwise-selection-algorithm",
    "href": "L05-model-selection.html#example-2-backward-stepwise-selection-algorithm",
    "title": "5  Model Selection",
    "section": "Example 2: Backward Stepwise Selection Algorithm",
    "text": "Example 2: Backward Stepwise Selection Algorithm\n\n\nBuild a model with all \\(p\\) possible predictors, \\((x_1, x_2,..., x_p)\\).\n\nRepeat the following until only 1 predictor remains in the model:\n\nRemove the 1 predictor with the biggest p-value.\nBuild a model with the remaining predictors.\n\n\nYou now have \\(p\\) competing models: one with all \\(p\\) predictors, one with \\(p-1\\) predictors, …, and one with 1 predictor. Identify the “best” model with respect to some metric (eg: CV MAE) and context.\n\n\n. . .\nLet’s try out the first few steps!\n. . .\n\n# Load packages and data\nlibrary(tidyverse)\nlibrary(tidymodels)\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat1.csv\")\n\n\n# STEP 1: model specifications\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n\n# STEP 2: model estimate (using all 12 predictors to start)\n# Pick apart this code and make it easier to identify the least \"significant\" predictor!!!\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, \n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n\n# 11 predictors (tweak the code)\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n\n# 10 predictors (tweak the code)\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n\n\nSolution\n\n\n# All 12 predictors\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;%  # use tidy to get p-values for each coefficient\n  filter(term != \"(Intercept)\") %&gt;% # exclude the intercept\n  mutate(p.value = round(p.value, 4)) %&gt;% # round the p-values for easier viewing\n  arrange(desc(p.value)) # added this line to arrange from largest to smallest p-value\n\n# A tibble: 12 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 biceps   -0.0808     0.746    -0.108  0.915 \n 2 neck      0.139      1.17      0.119  0.906 \n 3 knee      0.151      0.941     0.160  0.874 \n 4 wrist     0.836      2.32      0.361  0.721 \n 5 ankle    -0.888      1.28     -0.693  0.494 \n 6 abdomen   0.283      0.354     0.798  0.432 \n 7 age      -0.112      0.132    -0.847  0.405 \n 8 chest    -0.459      0.473    -0.971  0.340 \n 9 forearm   2.25       1.80      1.25   0.223 \n10 weight    0.379      0.213     1.78   0.0864\n11 hip      -0.921      0.510    -1.81   0.0822\n12 thigh    -1.24       0.646    -1.92   0.066 \n\n\n\n# 11 predictors (got rid of biceps)\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4)) %&gt;% \n  arrange(desc(p.value))\n\n# A tibble: 11 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 neck       0.161     1.14      0.142  0.888 \n 2 knee       0.180     0.886     0.203  0.841 \n 3 wrist      0.907     2.18      0.416  0.681 \n 4 ankle     -0.878     1.26     -0.699  0.490 \n 5 abdomen    0.281     0.348     0.809  0.425 \n 6 age       -0.111     0.130    -0.858  0.398 \n 7 chest     -0.453     0.461    -0.982  0.334 \n 8 forearm    2.17      1.62      1.34   0.192 \n 9 hip       -0.902     0.470    -1.92   0.0652\n10 weight     0.369     0.190     1.94   0.0623\n11 thigh     -1.26      0.602    -2.09   0.0454\n\n\n\n# 10 predictors (got rid of neck)\nlm_spec %&gt;% \n  fit(height ~ age + weight  + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4)) %&gt;% \n  arrange(desc(p.value))\n\n# A tibble: 10 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 knee       0.166     0.866     0.191  0.850 \n 2 wrist      0.985     2.07      0.475  0.639 \n 3 ankle     -0.884     1.23     -0.716  0.480 \n 4 age       -0.111     0.127    -0.869  0.392 \n 5 abdomen    0.298     0.322     0.924  0.363 \n 6 chest     -0.460     0.451    -1.02   0.316 \n 7 forearm    2.29      1.37      1.66   0.107 \n 8 weight     0.377     0.179     2.11   0.0435\n 9 thigh     -1.26      0.591    -2.14   0.0409\n10 hip       -0.931     0.416    -2.24   0.0331",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-3-backward-stepwise-selection-step-by-step-results",
    "href": "L05-model-selection.html#example-3-backward-stepwise-selection-step-by-step-results",
    "title": "5  Model Selection",
    "section": "Example 3: Backward Stepwise Selection Step-by-Step Results",
    "text": "Example 3: Backward Stepwise Selection Step-by-Step Results\nBelow is the complete model sequence along with 10-fold CV MAE for each model (using set.seed(253)).\n\n\n\npred\nCV MAE\npredictor list\n\n\n\n\n12\n5.728\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck, biceps\n\n\n11\n5.523\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck\n\n\n10\n5.413\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee\n\n\n9\n5.368\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist\n\n\n8\n5.047\nweight, hip, forearm, thigh, chest, abdomen, age, ankle\n\n\n7\n5.013\nweight, hip, forearm, thigh, chest, abdomen, age\n\n\n6\n4.684\nweight, hip, forearm, thigh, chest, abdomen\n\n\n5\n4.460\nweight, hip, forearm, thigh, chest\n\n\n4\n4.386\nweight, hip, forearm, thigh\n\n\n3\n4.091\nweight, hip, forearm\n\n\n2\n3.733\nweight, hip\n\n\n1\n3.658\nweight\n\n\n\nDISCUSS:\n\n(Review) Interpret the CV MAE for the model of height by weight alone.\nIs this algorithm more or less computationally expensive than the best subset algorithm?\nThe predictors neck and wrist, in that order, are the most strongly correlated with height. Where do these appear in the backward sequence and what does this mean?\n\n\ncor(humans)[,'height'] %&gt;% \n  sort()\n\n      thigh         hip         age     abdomen        knee       chest \n-0.11301249 -0.10648937 -0.05853538 -0.02173587  0.02345904  0.05838830 \n     biceps       ankle      weight     forearm       wrist        neck \n 0.07441696  0.07920867  0.11228791  0.16968040  0.28967468  0.29147610 \n     height \n 1.00000000 \n\n\n\nWe deleted predictors one at a time. Why is this better than deleting a collection of multiple predictors at the same time (eg: kicking out all predictors with p-value &gt; 0.1)?\n\n\n\nSolution\n\n\nUsing a linear model with only weight to predict height, our prediction error would be on average 3.58 inches off from the truth on new data.\nLess. We only have to build 12 models.\nBoth neck and wrist are kicked out early! The 1-predictor model produced by this algorithm isn’t necessarily the best 1-predictor model (same for any number of predictors).\nThe value of the coefficient (and thus the p-value) is dependent on the other variables in the model as we are accounting for or conditioning on them.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-4-backward-stepwise-selection-final-model",
    "href": "L05-model-selection.html#example-4-backward-stepwise-selection-final-model",
    "title": "5  Model Selection",
    "section": "Example 4: Backward Stepwise Selection Final Model",
    "text": "Example 4: Backward Stepwise Selection Final Model\n\nWe have to pick just 1 of the 12 models as our final model.\nThat is, we have to pick a value for our tuning parameter, the number of predictors.\nIt helps to plot the CV MAE for each model in the sequence.\nHere’s what we saw above:\n\n\nCode\ndata.frame(\n    predictors = c(12:1), \n    mae = c(5.728, 5.523, 5.413, 5.368, 5.047, 5.013, 4.684, 4.460, 4.386, 4.091, 3.733, 3.658)) %&gt;% \n  ggplot(aes(x = predictors, y = mae)) + \n    geom_point() + \n    geom_line() + \n    scale_x_continuous(breaks = c(1:12))\n\n\n\n\n\n\n\n\n\nHere’s another example from a different subset of these data:\n\n\nIn the odd “Goldilocks” fairy tale, a kid comes upon a bear den – the first bear’s bed is too hard, the second bear’s is too soft, and the third bear’s is just right. Our plot illustrates a goldilocks problem in tuning the number of predictors in our backward stepwise model. Explain.\n\n\nWhen the number of predictors is too small, the MAE increases because the model is too….\nWhen the number of predictors is too large, the MAE increases because the model is too….\n\n\nWhich model do you pick?!?\n\n\n\nSolution\n\n\nToo few predictors: model is too simple. too many predictors: model is too overfit.\nBased on our data, I think the model with 1 predictor seems pretty reasonable! If I were looking at the other MAE plot, though, I might gravitate pick a model with 1 (the simplest), 2 (still simple, but better MAE than 1 predictor), or 5 predictors (the model with the best CV MAE).",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-5-machine-learning-vs-human-learning",
    "href": "L05-model-selection.html#example-5-machine-learning-vs-human-learning",
    "title": "5  Model Selection",
    "section": "Example 5: machine learning vs human learning",
    "text": "Example 5: machine learning vs human learning\n\nWhen tuning or finalizing a model building algorithm, we (humans!) have our own choices to make. For one, we need to decide what we prefer:\n\na model with the lowest prediction errors; or\na more parsimonious model: one with slightly higher prediction errors but fewer predictors\n\nIn deciding, here are some human considerations:\n\ngoal: How will the model be used? Should it be easy for humans to interpret and apply?\ncost: How many resources (time, money, computer memory, etc) do the model and data needed require?\nimpact: What are the consequences of a bad prediction?\n\nFor each scenario below, which model would you pick: (1) the model with the lowest prediction errors; or (2) a parsimonious model with slightly worse predictions?\n\nGoogle asks us to re-build their search algorithm.\nA small non-profit hires us to help them build a predictive model of the donation dollars they’ll receive throughout the year.\n\n\n\nSolution\n\n\n1\n2",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#example-6-forward-stepwise-selection-algorithm",
    "href": "L05-model-selection.html#example-6-forward-stepwise-selection-algorithm",
    "title": "5  Model Selection",
    "section": "Example 6: Forward Stepwise Selection Algorithm",
    "text": "Example 6: Forward Stepwise Selection Algorithm\n\n\nHow do you think this works?\nIs it more or less computationally expensive than backward stepwise?\n\n\n\nSolution\n\n\nStart with 0 predictors. Add the predictor with the smallest p-value. To this model, add a second predictor with the smallest p-value. Continue until all predictors are in the model.\nmore. For 12 predictors, we’d have to build 12 models in step 1, 11 models in step 2, etc. Thus 12 + 11 + … + 1 = 78 models total.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#warning",
    "href": "L05-model-selection.html#warning",
    "title": "5  Model Selection",
    "section": "WARNING",
    "text": "WARNING\n\nVariable selection algorithms are a nice, intuitive place to start our discussion of model selection techniques.\nBUT we will not use them.\nThey are frowned upon in the broader ML community, so much so that tidymodels doesn’t even implement them! Why?\n\nBest subset selection is computationally expensive.\n\nBackward stepwise selection:\n\nis greedy – it makes locally optimal decisions, thus often misses the globally optimal model\noverestimates the significance of remaining predictors, thus shouldn’t be used for inference\n\nForward stepwise selection:\n\nis computationally expensive\ncan produce odd combinations of predictors (eg: a new predictor may render previously included predictors non-significant).",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#instructions-1",
    "href": "L05-model-selection.html#instructions-1",
    "title": "5  Model Selection",
    "section": "Instructions",
    "text": "Instructions\n\nScroll down to the # Exercises section in the Part 2 QMD.\nGoal: become familiar with new code structures (recipes and workflows)\nAsk me questions as I move around the room.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L05-model-selection.html#questions-1",
    "href": "L05-model-selection.html#questions-1",
    "title": "5  Model Selection",
    "section": "Questions",
    "text": "Questions\nThe video for today introduced the concepts of recipes and workflows in the tidymodels framework. These concepts will become important to our new modeling algorithms. Though they aren’t necessary to linear regression models, let’s explore them in this familiar setting.\nRun through the following discussion and code one step at a time. Take note of the general process, concepts, and questions you have.\nSTEP 1: model specification\nThis specifies the structure or general modeling algorithm we plan to use.\nIt does not specify anything about the variables of interest or our data.\n\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# Check it out\nlm_spec\n\nSTEP 2: recipe specification\nJust as a cooking recipe specifies the ingredients and how to prepare them, a tidymodels recipe specifies:\n\nthe variables in our relationship of interest (the ingredients)\nhow to pre-process or wrangle these variables (how to prepare the ingredients)\nthe data we’ll use to explore these variables (where to find the ingredients)\n\nIt does not specify anything about the model structure we’ll use to explore this relationship.\n\n# A simple recipe with NO pre-processing\ndata_recipe &lt;- recipe(height ~ wrist + ankle, data = humans)\n\n# Check it out\ndata_recipe\n\nSTEP 3: workflow creation (model + recipe)\nThis specifies the general workflow of our modeling process, including our model structure and our variable recipe.\n\nmodel_workflow &lt;- workflow() %&gt;%\n  add_recipe(data_recipe) %&gt;%\n  add_model(lm_spec)\n\n# Check it out\nmodel_workflow\n\nSTEP 4: Model estimation\nThis step estimates or fits our model of interest using our entire sample data.\nThe model (lm_spec) and variable details (here just height ~ wrist + ankle) are specified in the workflow, so we do not need to give that information again!\n\nmy_model &lt;- model_workflow %&gt;% \n  fit(data = humans)\n\nSTEPS 5: Model evaluation\nTo get in-sample metrics, use my_model like normal.\n\n# example: calculate of in-sample metrics\nmy_model %&gt;% \n  glance()\n\nTo get CV metrics, pass the workflow to fit_resamples along with information about how to randomly create folds.\n\nset.seed(253)\nmy_model_cv &lt;- model_workflow %&gt;% \n  fit_resamples(resamples = vfold_cv(humans, v = 10),\n                metrics = metric_set(rsq))\n\nThen, proceed as usual… (my_model_cv %&gt;% collect_metrics(), etc. )",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html",
    "href": "L06-lasso.html",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#announcements",
    "href": "L06-lasso.html#announcements",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Announcements",
    "text": "Announcements\n\nSee the #announcements channel on Slack for upcoming events.\nCheck out this Minnesota Public Radio (MPR) interview on Can AI replace your doctor?! It’s a discussion of AI / machine learning in medicine. NOTE: ML is a subset of AI. (image from Wiki)\n\n\n\nNow that we’re on MPR, journalist David Montgomery used R for data analysis and visualizations using this custom ggtheme to make visuals for MPR. To create a custom ggtheme, check out https://themockup.blog/posts/2020-12-26-creating-and-using-custom-ggplot2-themes/.\nInterested in the intersection between statistics/data science and journalism? Consider applying for this internship with the Star Tribune! (Deadline: Friday, November 1)",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#context",
    "href": "L06-lasso.html#context",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Context",
    "text": "Context\n\n\n\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = regression\n\\(y\\) is quantitative\nmodel = linear regression\nWe’ll assume that the relationship between \\(y\\) and (\\(x_1, x_2, ..., x_p\\)) can be represented by\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\varepsilon\\]\nestimation algorithm = LASSO (instead of least squares)",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#least-absolute-shrinkage-and-selection-operator",
    "href": "L06-lasso.html#least-absolute-shrinkage-and-selection-operator",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Least Absolute Shrinkage and Selection Operator",
    "text": "Least Absolute Shrinkage and Selection Operator\nLASSO: Least Absolute Shrinkage and Selection Operator\n\nDates back to 1996, proposed by Robert Tibshirani (one of the authors of ISLR)\n\nRobert Tibshirani, Regression Shrinkage and Selection Via the Lasso, Journal of the Royal Statistical Society: Series B (Methodological), Volume 58, Issue 1, January 1996, Pages 267–288, https://doi.org/10.1111/j.2517-6161.1996.tb02080.x",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#goal",
    "href": "L06-lasso.html#goal",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Goal",
    "text": "Goal\nGOAL: Model Selection\nUse the LASSO algorithm to help us regularize and select the “best” predictors \\(x\\) to use in a predictive linear regression model of \\(y\\):\n\\[y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\cdots + \\hat{\\beta}_p x_p + \\varepsilon\\]",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#idea",
    "href": "L06-lasso.html#idea",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Idea",
    "text": "Idea\n\nPenalize a predictor for adding complexity to the model (by penalizing its coefficient).\nTrack whether the predictor’s contribution to the model (lowering RSS) is enough to offset this penalty.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#algorithm-criterion",
    "href": "L06-lasso.html#algorithm-criterion",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Algorithm Criterion",
    "text": "Algorithm Criterion\nIdentify the model coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, ...  \\hat{\\beta}_p\\) that minimize the penalized residual sum of squares:\n\\[RSS + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert\\]\nwhere\n\nresidual sum of squares (RSS) measures the overall model prediction error\nthe penalty term measures the overall size of the model coefficients\n\\(\\lambda \\ge 0\\) (“lambda”) is a tuning parameter",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#questions",
    "href": "L06-lasso.html#questions",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Questions",
    "text": "Questions\n1: LASSO vs other algorithms for building linear regression models\n\nLASSO vs least squares\n\nWhat’s one advantage of LASSO vs least squares?\nWhich algorithm(s) require us (or R) to scale the predictors?\n\nWhat is one advantage of LASSO vs backward stepwise selection?\n\n\n\nSolution\n\n\nLASSO helps with model selection, i.e. kicks some predictors out of the model, and preventing overfitting.\nLASSO isn’t greedy and doesn’t overestimate the significance of the predictors it retains (its variable selection isn’t based on p-values).\n\n\n\n2: LASSO tuning\nWe have to pick a \\(\\lambda\\) penalty tuning parameter for our LASSO model. What’s the impact of \\(\\lambda\\)?\n\nWhen \\(\\lambda\\) is 0, …\nAs \\(\\lambda\\) increases, the predictor coefficients ….\nGoldilocks problem: If \\(\\lambda\\) is too big, …. If \\(\\lambda\\) is too small, …\nTo decide between a LASSO that uses \\(\\lambda = 0.01\\) vs \\(\\lambda = 0.1\\) (for example), we can ….\n\n\n\nSolution\n\n\nLASSO is equivalent to least squares.\nshrink toward or to 0.\ntoo big: all predictors are kicked out of the model. too small: too few predictors are kicked out, hence the model is complicated and maybe overfit.\ncompare CV MAE of the LASSOs with these \\(\\lambda\\)\n\n\n\nCOMMENT: Picking \\(\\lambda\\)\nWe cannot know the “best” value for \\(\\lambda\\) in advance. This varies from analysis to analysis.\nWe must try a reasonable range of possible values for \\(\\lambda\\). This also varies from analysis to analysis.\nIn general, we have to use trial-and-error to identify a range that is…\n\nwide enough that it doesn’t miss the best values for \\(\\lambda\\)\nnarrow enough that it focuses on reasonable values for \\(\\lambda\\)",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#instructions",
    "href": "L06-lasso.html#instructions",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Instructions",
    "text": "Instructions\n\nOpen the QMD fo today and scroll down to the Exercises\nWork on implementing LASSO to familiar data\nBecome familiar with the new code structures:\n\ninstead of fit_resamples to run CV, we’ll use tune_grid to tune the algorithm with CV\nnew engine: set_engine('glmnet')\nin general: focus on the concepts over the R code\n\nAs always:\n\nBe kind to yourself/each other\nCollaborate\nAsk me questions as I move around the room",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#questions-1",
    "href": "L06-lasso.html#questions-1",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Questions",
    "text": "Questions\nWe’ll use the LASSO algorithm to help us build a good predictive model of height using the collection of 12 possible predictors in the humans dataset:\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# Resolves package conflicts by preferring tidymodels functions\ntidymodels_prefer()\n\n# Load data\nhumans &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bodyfat2.csv\")\n\n\n\n\nLet’s implement the LASSO. We’ll pause to examine the code. The R code notes section, below, and R code tutorial provide more detail, so no need to take notes here.\n\n# STEP 1: LASSO algorithm / model specification\n# NOTE: we're using a new engine now: glmnet instead of lm\nlasso_spec &lt;- linear_reg() %&gt;%             \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;%                 \n  set_args(mixture = 1, penalty = tune())  \n\n\n# STEP 2: variable recipe\n# NOTE: \"y ~ .\" is shorthand for \"y as a function of all other variables\"\n# NOTE: We'll discuss step_dummy() next class.\nvariable_recipe &lt;- recipe(height ~ ., data = humans) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n# STEP 3: workflow specification (model + recipe)\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\n\n# STEP 4: Estimate 50 LASSO models using\n# lambda values on a \"grid\" or range from 10^(-5) to 10^(-0.1).\n# Calculate the CV MAE for each of the 50 models.\n# NOTE: we use tune_grid instead of fit_resamples to run CV\n# NOTE: I usually start with a range from 10^(-5) to 10^1 and tweak through trial-and-error.\nset.seed(253)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, -0.1)), levels = 50),  \n    resamples = vfold_cv(humans, v = 10),   \n    metrics = metric_set(mae)\n  )\n\n\nExamining the impact of \\(\\lambda\\)\n\nLet’s compare the CV MAEs (y-axis) for our 50 LASSO models which used 50 different \\(\\lambda\\) values (x-axis):\n\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\n\nWe told R to use a range of \\(\\lambda\\) from -5 to -0.1 on the log10 scale. Calculate this range on the non-log scale and confirm that it matches the x-axis.\n\n\n#: eval: false\n10^(-5)\n10^(-0.1)\n\n\nExplain why this plot displays the “Goldilocks” problem of tuning \\(\\lambda\\).\n\n\n\nSolution\n\n\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\n\n\n\n\n\n\n\n\nYep it matches.\n\n\n10^(-5)\n\n[1] 1e-05\n\n10^(-0.1)\n\n[1] 0.7943282\n\n\n\nCV MAE is large when \\(\\lambda\\) is either too small or too big.\n\n\n\n\nPicking a \\(\\lambda\\) value\n\n\nIn the plot above, roughly which value of the \\(\\lambda\\) penalty parameter produces the smallest CV MAE? Check your approximation:\n\n\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\n\nbest_penalty\n\n\nSuppose we prefer a parsimonious model. The plot below adds error bars to the CV MAE estimates of prediction error (+/- one standard error). Any model with a CV MAE that falls within another model’s error bars is not significantly better or worse at prediction:\n\n\n# with error bars\n# NOTE: we start with the same code as above (lines 1--3), \n# then add error bars (`geom_errorbar`)\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\nUse this to approximate the largest \\(\\lambda\\), thus the most simple LASSO model, that produces a CV MAE that’s within 1 standard error of the best model (thus is not significantly worse). Check your approximation:\n\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\n\nparsimonious_penalty\n\n\nMoving forward, we’ll use the parsimonious LASSO model. Simply report the tuning parameter \\(\\lambda\\) here. Just as a radio show needs to tell its audience where to tune the radio dial, it’s important to explicitly report \\(\\lambda\\) so that we and others can reproduce the model!\n\n\n\nSolution\n\n\n\\(\\lambda\\) close to 0\n\n\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\n\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1  0.0126 Preprocessor1_Model32\n\n\n\n\\(\\lambda\\) around 0.2\n\n\n# With error bars\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\n\n\n\n\n\n\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\n\nparsimonious_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.200 Preprocessor1_Model44\n\n\n\n\n0.2\n\n\n\n\nPAUSE: Picking a range to try for \\(\\lambda\\)\nThe range of values we tried for \\(\\lambda\\) had the following nice properties. If it didn’t, we should adjust our range (make it narrower or wider).\n\nOur range was wide enough.\nWe observed the goldilocks effect. Further, the “best” and “parsimonious” \\(\\lambda\\) values were not at the edges of the range, suggesting there aren’t better \\(\\lambda\\) values outside our range.\nOur range was narrow enough.\nWe didn’t observe any loooooong flat lines in CV MAE, thus we narrowed in on the \\(\\lambda\\) values where the “action is happening”, i.e. where changing \\(\\lambda\\) impacts the model.\n\n\n\nFinalizing our LASSO model\n\nLet’s finalize our parsimonious LASSO model:\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = humans)\n    \nfinal_lasso %&gt;% \n  tidy()\n\n\nHow many and which predictors were kept in this model?\nHow do these compare to the 5-predictor model we identify using the backward stepwise selection algorithm with this subset of data: weight, abdomen, thigh, neck, chest\n\n\n\nThrough shrinkage, the LASSO coefficients lose some contextual meaning, so we typically shouldn’t interpret them. Why don’t we care?! THINK: What is the goal of LASSO modeling?\nThe LASSO tidy() summary doesn’t report p-values for testing the “significance” of our predictors. Why don’t we care? (Name two reasons.)\n\n\n\nSolution\n\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = humans)\n\nfinal_lasso %&gt;% \n  tidy() %&gt;% \n  filter(estimate != 0)\n\n# A tibble: 6 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  63.8      0.200\n2 weight        0.0696   0.200\n3 abdomen      -0.126    0.200\n4 thigh        -0.0199   0.200\n5 knee          0.156    0.200\n6 ankle         0.0474   0.200\n\n\n\n5: weight, abdomen, thigh, knee, ankle\n3 of the predictors are the same. LASSO includes knee (which is the most highly correlated with height in this dataset)\n\n\ncor(humans)[,'height'] %&gt;% sort()\n\n       age    abdomen    forearm      thigh       neck      chest      wrist \n-0.1469746  0.1120756  0.1303829  0.2059575  0.2382709  0.2399610  0.2451834 \n    biceps        hip      ankle     weight       knee     height \n 0.3173460  0.3470352  0.3628819  0.4440995  0.4441473  1.0000000 \n\n\n\nWe’re using this model to give good predictions, not to explore / make inferences about relationships.\nThe remaining predictors are those that have significant predictive power in this linear regression model (thus we get conclusions like a hypothesis test without doing a test). Also, our goal is to build a good predictive model, not to do inference.\n\n\n\n\n\nLASSO vs LASSO\n\n\nOur parsimonious LASSO selected only 5 of the 12 possible predictors. Out of curiosity, how many predictors would have remained if we had used the best_penalty value for \\(\\lambda\\)?\n\n\nlasso_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = humans) %&gt;% \n  tidy()\n\n\nBased on this example, do you think LASSO is a greedy algorithm? Are you “stuck” with your past locally optimal choices? Compare the predictors in this larger model with those in the smaller, parsimonious model.\n\n\n\nSolution\n\n\nThis would have 11 predictors.\n\n\nlasso_workflow %&gt;% \n  finalize_workflow(parameters = best_penalty) %&gt;% \n  fit(data = humans) %&gt;% \n  tidy() %&gt;% \n  filter(estimate != 0)\n\n# A tibble: 12 × 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) 104.      0.0126\n 2 age          -0.0208  0.0126\n 3 weight        0.243   0.0126\n 4 neck         -0.595   0.0126\n 5 chest        -0.102   0.0126\n 6 abdomen      -0.200   0.0126\n 7 hip          -0.0491  0.0126\n 8 thigh        -0.307   0.0126\n 9 knee          0.180   0.0126\n10 biceps       -0.145   0.0126\n11 forearm      -0.0640  0.0126\n12 wrist        -0.0864  0.0126\n\n\n\nankle is not in this larger model but it is in the more parsimonious model. Therefore, the algorithm can’t be greedy. If it were greedy, then ankle would get removed for all smaller models.\n\n\n\n\nLASSO vs least squares\n\nLet’s compare our final_lasso model to the least squares model using all predictors:\n\n# Build the least squares model using recipes and workflows\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nls_workflow &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(variable_recipe) \n\nls_model &lt;- ls_workflow %&gt;% \n  fit(data = humans) \n\n# examine coefficients\nls_model %&gt;% \n  tidy()\n\n# get 10-fold CV MAE\nset.seed(253)\nls_workflow %&gt;% \n  fit_resamples(\n    resamples = vfold_cv(humans,v = 10),\n    metrics = metric_set(mae)\n  ) %&gt;% \n  collect_metrics()\n\n\nOur final_lasso has 5 predictors and a CV MAE of 1.9 (calculated above). The ls_model has 12 predictors and a CV MAE of 1.8 (confirm). Comment.\nUse both final_lasso and ls_model to predict the height of the new patient below. How do these compare? Does this add to or calm any fears you might have had about shrinking coefficients?!\n\n\nnew_patient &lt;- data.frame(age = 50, weight = 200, neck = 40, chest = 115, abdomen = 105, hip = 100, thigh = 60, knee = 38, ankle = 23, biceps = 32, forearm = 29, wrist = 19) \n\n\n# LS prediction\n___ %&gt;% \npredict(new_data = ___)\n\n\n# LASSO prediction\n___ %&gt;% \npredict(new_data = ___)\n\n\nWhich final model would you choose, the LASSO or least squares?\n\n\n\nSolution\n\n\n# Build the LS model\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# NOTE: we created variable_recipe above\n# here's what that looked like: \n## variable_recipe &lt;- recipe(height ~ ., data = humans) %&gt;% \n##   step_dummy(all_nominal_predictors())\n\nls_workflow &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(variable_recipe) \n\nls_model &lt;- ls_workflow %&gt;% \n  fit(data = humans) \n\nls_model %&gt;% \n  tidy()\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic     p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 (Intercept) 110.       16.9       6.51   0.000000561\n 2 age          -0.0234    0.0367   -0.637  0.529      \n 3 weight        0.266     0.0573    4.64   0.0000810  \n 4 neck         -0.671     0.335    -2.00   0.0556     \n 5 chest        -0.119     0.131    -0.908  0.372      \n 6 abdomen      -0.196     0.113    -1.73   0.0946     \n 7 hip          -0.0978    0.189    -0.518  0.609      \n 8 thigh        -0.313     0.163    -1.92   0.0661     \n 9 knee          0.199     0.272     0.733  0.470      \n10 ankle        -0.0262    0.449    -0.0583 0.954      \n11 biceps       -0.168     0.200    -0.837  0.410      \n12 forearm      -0.0770    0.144    -0.536  0.596      \n13 wrist        -0.0962    0.647    -0.149  0.883      \n\nset.seed(253)\nls_workflow %&gt;% \n  fit_resamples(\n    resamples = vfold_cv(humans,v = 10),\n    metrics = metric_set(mae)\n  ) %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    1.81    10   0.212 Preprocessor1_Model1\n\n\n\nLASSO model is much simpler, and has only slightly worse predictions (on the scale of inches).\nThey’re very similar! Shrinking coefficients doesn’t mean our predictions are odd.\n\n\nnew_patient &lt;- data.frame(age = 50, weight = 200, neck = 40, chest = 115, abdomen = 105, hip = 100, thigh = 60, knee = 38, ankle = 23, biceps = 32, forearm = 29, wrist = 19) \n\n# LS prediction\nls_model %&gt;% \n  predict(new_data = new_patient)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  70.1\n\n# LASSO prediction\nfinal_lasso %&gt;% \n  predict(new_data = new_patient)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  70.3\n\n\n\nLASSO\n\n\n\n6. Visualizing LASSO shrinkage\nFinally, let’s zoom back out and compare the coefficients for all 50 LASSO models:\n\n# Get output for each LASSO model\nall_lassos &lt;- final_lasso %&gt;% \n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\")\n    \n# Plot coefficient paths as a function of lambda\nplot(all_lassos, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n\n\n\n\n\n\n# Codebook for which variables the numbers correspond to\nrownames(all_lassos$beta)\n\n [1] \"age\"     \"weight\"  \"neck\"    \"chest\"   \"abdomen\" \"hip\"     \"thigh\"  \n [8] \"knee\"    \"ankle\"   \"biceps\"  \"forearm\" \"wrist\"  \n\n\nThere’s a lot of information in this plot!\n\nlines = each line represents a different predictor. The small number to the left of each line indicates the predictor by its order in the rownames() list. Click “Zoom” to zoom in.\nx-axis = our range of \\(\\lambda\\) values, on the log scale\ny-axis = coefficient values at the corresponding \\(\\lambda\\)\n\nnumbers above the plot = how many predictors remain in the model with the corresponding \\(\\lambda\\)\n\nWe’ll process this information in the next 2 exercises.\nIf you’re curious, here is some code to recreate that plot using ggplot:\n\n\nCode\nlasso_coefs &lt;- all_lassos$beta  %&gt;% \n  as.matrix() %&gt;%  \n  t() %&gt;% \n  as.data.frame() %&gt;% \n  mutate(lambda = all_lassos$lambda ) %&gt;%\n  pivot_longer(cols = -lambda, \n               names_to = \"term\", \n               values_to = \"coef\")\n\nlasso_coefs %&gt;% filter(coef != 0, lambda &gt; 1)\n\n\n# A tibble: 2 × 3\n  lambda term      coef\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1   1.01 weight 0.00228\n2   1.01 knee   0.0271 \n\n\nCode\nlasso_coefs %&gt;%\n  ggplot(aes(x = lambda, y = coef, color = term)) +\n  geom_line() +\n  geom_text(data = lasso_coefs %&gt;% filter(lambda == min(lambda)), aes(x = 0.001, label = term), size = 2) + \n  scale_x_log10(limits = c(-2, 2)) +\n  guides(color = FALSE)\n\n\n\n\n\n\n\n\n\n\nplot: examining specific predictors\n\nAnswer the following questions for predictor 7.\n\nWhich predictor is this?\nApproximate the coefficient in the LASSO with \\(log(\\lambda) \\approx -5\\).\nAt what \\(log(\\lambda)\\) does the coefficient start to significantly shrink?\n\nAt what \\(log(\\lambda)\\) does the predictor get dropped from the model?\n\n\n\nSolution\n\n\nthigh\nvery roughly -0.32\nroughly -3.5\n\nroughly -1.8\n\n\n\n\nplot: big picture\n\n\nHow does this plot reflect the LASSO shrinkage phenomenon?\n\nWhat is one of the most “important” or “persistent” predictors?\n\nWhat is one of the least persistent predictors?\n\nOur parsimonious LASSO model had 5 predictors. How many predictors would remain if we had minimized the CV MAE using \\(\\lambda \\approx 0.0126\\) (\\(log(\\lambda) = -4.4\\))?\n\n\n\nSolution\n\n\ncoefficients are shrinking toward or to 0 as \\(\\lambda\\) increases\nweight (variable 2), knee (variable 8)\nlots of options here. look for the lines that drop to 0 sooner.\n11\n\n\n\n\n\n\n\nREVIEW: Model evaluation\n\nLet’s finalize our LASSO analysis. Just as in least squares, it’s important to evaluate a LASSO model before applying it. We’ve already examined whether our LASSO model produces accurate predictions. Use a residual plot to determine if this model is wrong. NOTE: augment() gives predictions, but not residuals :/. You’ll need to calculate them.\n\n# Note what augment() gives us\nfinal_lasso %&gt;% \n  augment(new_data = humans) %&gt;% \n  names()\n\n\n# Now calculate and plot the residuals\nfinal_lasso %&gt;% \n  augment(new_data = humans) %&gt;% \n  mutate(.resid = ___) %&gt;% \n  ggplot(aes(x = ___, y = ___)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\n\n\nSolution\n\n\n# use augment to get predictions (.pred)\n# NOTE: we don't get the residuals (.resid) automatically\nfinal_lasso %&gt;% \n  augment(new_data = humans) %&gt;% \n  names()\n\n [1] \".pred\"   \"age\"     \"weight\"  \"neck\"    \"chest\"   \"abdomen\" \"hip\"    \n [8] \"thigh\"   \"knee\"    \"ankle\"   \"biceps\"  \"forearm\" \"wrist\"   \"height\" \n\n# Now calculate and plot the residuals\nfinal_lasso %&gt;% \n  augment(new_data = humans) %&gt;% \n  mutate(.resid = height - .pred) %&gt;% # resid = observed - predicted\n  ggplot(aes(x = .pred, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0) + \n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nOPTIONAL: Practice on another dataset.\n\nThe Hitters data in the ISLR package contains the salaries and performance measures for 322 Major League Baseball players. Use LASSO to determine the “best” predictive model of player Salary.\n\n# Load the data\nlibrary(ISLR)\ndata(Hitters)\nHitters &lt;- Hitters %&gt;% \n  filter(!is.na(Salary))\n\n# IN THE CONSOLE (not in the QMD): Examine codebook\n#?Hitters  \n\n\nReflection\nThis is the end of the (short!) Unit 2 on “Regression: Model Selection”! Let’s reflect on the technical content of this unit:\n\nWhat was the main motivation / goal behind this unit?\nFor each of the following algorithms, describe the steps, pros, cons, and comparisons to least squares:\n\nbest subset selection\nbackward stepwise selection\nLASSO\n\nIn your own words, define the following: parsimonious models, greedy algorithms, Goldilocks problem.\nReview the new tidymodels syntax from this unit. Identify key themes and patterns.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#todays-material",
    "href": "L06-lasso.html#todays-material",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Today’s Material",
    "text": "Today’s Material\n\nFinish the activity, check the solutions, and reach out with questions.\nReview the R code reference section at the end of today’s notes and an optional R code tutorial video posted for today\nIf you’re curious, there’s an optional “deeper learning” section below that presents two other shrinkage algorithms we won’t cover in this course",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#upcoming-due-dates",
    "href": "L06-lasso.html#upcoming-due-dates",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Upcoming Due Dates",
    "text": "Upcoming Due Dates\n\nNo checkpoint for next class!\n\nSee the schedule for optional readings.\nYour next checkpoint (CP6) will be due a week from today. It’s up on Moodle already if you’d like to get a head start on it.\n\nDue next Tuesday: HW2\n\nYou have everything you need to complete this assignment after today’s class",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#ridge-regression",
    "href": "L06-lasso.html#ridge-regression",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nLASSO isn’t the only shrinkage & regularization algorithm. An alternative is ridge regression. This algorithm also seeks to build a (predictive) linear regression model of \\(y\\):\n\\[y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\cdots + \\hat{\\beta}_p x_p + \\varepsilon\\]\nIt also does so by selecting coefficients which minimize a penalized residual sum of squares. HOWEVER, the ridge regression penalty is based upon the sum of squared coefficients instead of the sum of absolute coefficients:\n\\[RSS + \\lambda \\sum_{j=1}^p \\hat{\\beta}_j^2\\]\nThis penalty regularizes / shrinks the coefficients. BUT, unlike the LASSO, ridge regression does NOT shrink coefficients to 0, thus cannot be used for variable selection. (Check out the ISLR text for a more rigorous, geometric explanation for why LASSO often shrinks coefficients to 0.)",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#elastic-net",
    "href": "L06-lasso.html#elastic-net",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Elastic Net",
    "text": "Elastic Net\nThe elastic net is yet another shrinkage & regularization algorithm. It combines the penalties used in LASSO and ridge regression. This algorithm seeks to build a (predictive) linear regression model of \\(y\\):\n\\[y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\cdots + \\hat{\\beta}_p x_p + \\varepsilon\\]\nby selecting coefficients which minimize the following penalized residual sum of squares:\n\\[RSS + \\lambda_1 \\sum_{j=1}^p \\vert \\hat{\\beta}_j \\vert + \\lambda_2 \\sum_{j=1}^p \\hat{\\beta}_j^2\\]\nNOTE:\n\nElastic net depends upon two tuning parameters, \\(\\lambda_1\\) and \\(\\lambda_2\\), thus is more complicated than the LASSO.\nIn cases when we have a group of correlated predictors, LASSO tends to select only one of these predictors. The elastic net does not.",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "L06-lasso.html#bayesian-connections",
    "href": "L06-lasso.html#bayesian-connections",
    "title": "6  LASSO: Shrinkage / Regularization",
    "section": "Bayesian Connections",
    "text": "Bayesian Connections\nIF you have taken or will take Bayesian statistics (STAT 454), we can also write the LASSO as a Bayesian model. Specifically, LASSO estimates are equivalent to the posterior mode estimates of \\(\\beta_j\\).\n\\[\\begin{split}\nY_i | \\beta_0,...\\beta_k & \\sim N(\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_p X_p, \\sigma^2) \\\\\n\\beta_j & \\sim \\text{ Laplace (double-exponential)}(0, f(\\lambda)) \\\\\n\\sigma^2 & \\sim \\text{ some prior} \\\\\n\\end{split}\\]",
    "crumbs": [
      "Regression: Model Selection (Unit 2)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LASSO: Shrinkage / Regularization</span>"
    ]
  },
  {
    "objectID": "U03-motivation.html",
    "href": "U03-motivation.html",
    "title": "Motivating Question",
    "section": "",
    "text": "GOAL\nWe have a quantitative response variable \\(y\\) and want to build a predictive regression model of \\(y\\) using a bunch of potential predictors \\(x\\).\nBUT\nThe relationships between \\(y\\) and \\(x\\) are complicated, thus our existing modeling tools (e.g. least squares algorithm, LASSO) are too rigid. How can we build a flexible predictive regression model?\n\n\n\n\n\nParametric vs nonparametric models\nThe shared goal behind parametric and nonparametric regression models is to build a model of some quantitative response variable \\(y\\) using predictors \\((x_1, x_2, ..., x_p)\\):\n\\[y = f(x_1, x_2, ..., x_p) + \\varepsilon\\]\n\nparametric models\nParametric regression models assume a specific “parametric” form for \\(f\\). For example, a linear regression model assumes that \\(y\\) is a linear combination of the predictors which is defined by parameters \\(\\beta_i\\):\n\\[y = f(x) + \\varepsilon = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_p x_p + \\varepsilon\\]\nBUT this assumption can be too rigid and inflexible to describe the relationship between \\(y\\) and \\(x\\).\nnonparametric models\nNonparametric models do NOT assume a parametric form for the relationship between \\(y\\) and \\(x\\), \\(f(x_1, x_2, ..., x_p)\\). Thus they are more flexible.\n\n\n\n\nCommon flexible regression models\n\nK Nearest Neighbors (KNN)\nLocal regression / locally weighted scatterplot smoothing (LOESS) & generalized additive models (GAM)\nSmoothing splines",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "L07-nonparametric.html",
    "href": "L07-nonparametric.html",
    "title": "7  Nonparametric Models",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#homework-reminders",
    "href": "L07-nonparametric.html#homework-reminders",
    "title": "7  Nonparametric Models",
    "section": "Homework Reminders",
    "text": "Homework Reminders\nCheck your work!\n\nRemember to submit an HTML (not a QMD) to Moodle for grading\nPlease CHECK that HTML before submitting! In particular, make sure:\n\nyour HTML includes your answers and not the original questions\nthe formatting (headers, etc.) matches the provided template\nall plots and other output appear as intended\n\nTip: Render as you go! This way you can catch issues early and won’t be in a rush to debug right before the submission deadline.\n\n. . .\nLate work:\n\nTo help us manage the large number of assignments we need to review, we will be implementing an 8 hour automatic “grace period” for late submissions.\nIf you need additional beyond this, you must email me to request an extension.\n\n. . .\nExtensions:\n\nFor homework, I will grant ALL 1–3 day extension requests made in advance of the deadline.\nTo request an extension, email me and tell me how much time you need. You do not need to give me a reason why you are requesting an extension.\nI cannot guarantee that I will be able to accommodate longer requests, or requests made after the deadline, so please plan accordingly.\n\n. . .\nFeedback:\n\nYou will receive individual feedback on (almost all) questions and an overall score of PASS / REVISE / NO SUBMISSION\nYou will access this feedback via your STAT 253 Feedback spreadsheet\nIf your overall score is PASS this means:\n\nall of your answers were correct or almost correct\nalthough you have PASSed the assignment, there likely is still room for improvement! make sure you review your feedback and the solutions for all questions (even those marked as correct)\n\nIf your overall score is REVISE this means:\n\nwe noticed a few more areas for improvement in your work and think it could benefit from revision\nyou will have one week to review your feedback, revise your responses, and resubmit\n\n\n. . .\nRevision process:\n\nAssignments that are submitted on time (or with an approved extension) and demonstrate effort to complete most problems are eligible for revision\nTo prepare this revision, you should:\n\ncarefully review feedback received on your original submission\nyou may also consult the posted solutions (although you may not simply copy said solutions) or stop by office hours to discuss\nsubmit your revised HTML via Moodle within 1 week of receiving feedback",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#context",
    "href": "L07-nonparametric.html#context",
    "title": "7  Nonparametric Models",
    "section": "Context",
    "text": "Context\n\n\n\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = regression\n\\(y\\) is quantitative\nmodel = nonparametric regression???",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#goal",
    "href": "L07-nonparametric.html#goal",
    "title": "7  Nonparametric Models",
    "section": "Goal",
    "text": "Goal\nJust as in Unit 2, Unit 3 will focus on model building, but a different aspect:\n\nUnit 2: how do we handle / select predictors for our predictive model of \\(y\\)?\nUnit 3: how do we handle situations in which linear regression models are too rigid to capture the relationship of \\(y\\) vs \\(x\\)?",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#motivating-example",
    "href": "L07-nonparametric.html#motivating-example",
    "title": "7  Nonparametric Models",
    "section": "Motivating Example",
    "text": "Motivating Example\nLet’s build a predictive model of blood glucose level in mg/dl by time in hours (\\(x\\)) since eating a high carbohydrate meal.\nConsider 3 linear regression models of \\(y\\), none of which appear to be very good:\n\\[\\begin{array}{ll}\n\\text{linear:} &  y = f(x) + \\varepsilon = \\beta_0 + \\beta_1 x + \\varepsilon \\\\\n\\text{quadratic:} & y = f(x) + \\varepsilon = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\varepsilon \\\\\n\\text{6th order polynomial:} & y = f(x) + \\varepsilon = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 + \\beta_5 x^5 +  \\beta_6 x^6 + \\varepsilon \\\\\n\\end{array}\\]\n\n\n\n\n\n\n\n\n\n\n\nParametric vs Nonparametric\nThese parametric linear regression models assume (incorrectly) that we can represent glucose over time by the following formula for \\(f(x)\\) that depends upon parameters \\(\\beta_i\\):\n\\[y = f(x) + \\varepsilon = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_p x_p + \\varepsilon\\]\nNonparametric models do NOT assume a parametric form for the relationship between \\(y\\) and \\(x\\), \\(f(x)\\). Thus they are more flexible.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#part-1-intuition",
    "href": "L07-nonparametric.html#part-1-intuition",
    "title": "7  Nonparametric Models",
    "section": "Part 1: Intuition",
    "text": "Part 1: Intuition\nIn Part 1, your task is to come up with a nonparametric algorithm to estimate \\(f(\\text{time})\\) in the equation \\[\\text{glucose} = f(\\text{time}) + \\epsilon\\]\n\n\nMake some nonparametric predictions\nWorking as a group, thinking nonparametrically, and utilizing the plot and data on the sheet provided, predict glucose level after:\n\n1.5 hours\n4.25 hours\n\\(x\\) hours (i.e. what’s your general prediction process at any time point \\(x\\)?)\n\n\n\n\nSolution\n\nWill vary by group.\n\n\n\nBuild a nonparametric algorithm\nWorking as a group:\n\nTranslate your prediction process into a formal algorithm, i.e. step-by-step procedure or recipe, to predict glucose at any time point \\(x\\). THINK:\n\nDoes this depend upon any tuning parameters? For example, did your prediction process use any assumed “thresholds” or quantities?\nIf so, represent this tuning parameter as “t” and write your algorithm using t (not a tuned value for t).\n\nOn the separate page provided, one person should summarize this algorithm and report the predictions you got using this algorithm.\n\n\n\n\n\nSolution\n\nWill vary by group.\n\n\n\nTest your algorithm\nExchange algorithms with another group.\n\nIs the other group’s algorithm similar to yours?\nUse their algorithm to predict glucose after 1.5 hours and 4.25 hours. Do your calculations match theirs? If not, what was unclear about their algorithm that led to the discrepancy?\n\n\n\n\nSolution\n\nWill vary by group.\n\n\n\n\n\nBuilding an algorithm as a class\n\nOn your sheet, sketch a predictive model of glucose by time that a “good” algorithm would produce.\nIn general, how would such an algorithm work? What would be its tuning parameter?\n\n\n\n\nSolution\n\n\nsmooth curve that follows the general trend\ntuning parameter = size of the windows or neighborhoods. in general, we’ll fit “models” within smaller windows",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#part-2-distance",
    "href": "L07-nonparametric.html#part-2-distance",
    "title": "7  Nonparametric Models",
    "section": "Part 2: Distance",
    "text": "Part 2: Distance\nCentral to nonparametric modeling is the concept of using data points within some local window or neighborhood.\nDefining a local window or neighborhood relies on the concept of distance.\nWith only one predictor, this was straightforward in our glucose example: the closest neighbors at time \\(x\\) are the data points observed at the closest time points.\n\nGOAL\nExplore the idea of distance when we have more predictors, and the data-preprocessing steps we have to take in order to implement this idea in practice.\n\n\n\n\nTwo measures of distance\nConsider data on 2 predictors for 2 students:\n\n\nstudent 1: 8 hours sleep Monday (\\(a_1\\)), 9 hours sleep Tuesday (\\(b_1\\))\nstudent 2: 7 hours sleep Monday (\\(a_2\\)), 11 hours sleep Tuesday (\\(b_2\\))\n\n\nCalculate the Manhattan distance between the 2 students. And why do you think this is called “Manhattan” distance?\n\n\\[|a_1 - a_2| + |b_1 - b_2|\\]\n\n\n\n\n\n\n\n\n\n\nCalculate the Euclidean distance between the 2 students:\n\n\\[\\sqrt{(a_1 - a_2)^2 + (b_1 - b_2)^2}\\]\n\n\n\n\n\n\n\n\n\nNOTE: We’ll typically use Euclidean distance in our algorithms. But for the purposes of this activity, use Manhattan distance (just since it’s easier to calculate and gets at the same ideas).\n\n\nSolution\n\n\n# a\nabs(8 - 7) + abs(9 - 11)\n\n[1] 3\n\n# b\nsqrt((8 - 7)^2 + (9 - 11)^2)\n\n[1] 2.236068\n\n\n\n\n\nWho are my neighbors?\nConsider two more possible predictors of some student outcome variable \\(y\\):\n\n\n\\(x_1\\) = number of days old\n\\(x_2\\) = major division (humanities, fine arts, social science, or natural science)\n\nCalculate how many days old you are:\n\n# Record dates in year-month-day format\ntoday &lt;- as.Date(\"2024-10-01\")\nbday  &lt;- as.Date(\"????-??-??\")\n    \n# Calculate difference\ndifftime(today, bday, units = \"days\")\n\nThen for each scenario, identify which of your group members is your nearest neighbor, as defined by Manhattan distance:\n\nUsing only \\(x_1\\).\nUsing only \\(x_2\\). And how are you measuring the distance between students’ major divisions (categories not quantities)?!\nUsing both \\(x_1\\) and \\(x_2\\)\n\n\n\nSolution\n\nWill vary by group.\n\n\n\nMeasuring distance: 2 quantitative predictors\nConsider 2 more measures on another 3 students: \n\n\n\n\n\nDays Old\nDistance from Campus\n\n\n\n\nstudent 1\n7300 days\n0.1 hour\n\n\nstudent 2\n7304 days\n0.1 hour\n\n\nstudent 3\n7300 days\n3.1 hours\n\n\n\n\n\nContextually, not mathematically, do you think student 1 is more similar to student 2 or student 3?\nCalculate the mathematical Manhattan distance between: (1) students 1 and 2; and (2) students 1 and 3.\nDo your contextual and mathematical assessments match? If not, what led to this discrepancy?\n\n\n\nSolution\n\n\nMy opinion: student 2. Being 4 days apart is more “similar” than 2 students that live 3 hours apart.\nstudents 1 and 2: \\(|7300 - 7304| + |0.1 - 0.1| = 4\\), students 1 and 3: \\(|7300 - 7300| + |0.1 - 3.1| = 3\\)\nstudent 3. nope. the variables are on different scales.\n\n\n\n\nMeasuring distance: quantitative & categorical predictors\nLet’s repeat for another 3 students:\n\n\n\n\n\n\nMajor\nDays Old\n\n\n\n\nstudent 1\nSTAT\n7300 days\n\n\nstudent 2\nSTAT\n7302 days\n\n\nstudent 3\nGEOG\n7300 days\n\n\n\n\n\nContextually, do you think student 1 is more similar to student 2 or student 3?\nMathematically, calculate the Manhattan distance between: (1) students 1 and 2; and (2) students 1 and 3. NOTE: The distance between 2 different majors is 1.\nDo your contextual and mathematical assessments match? If not, what led to this discrepancy?\n\n\n\nSolution\n\n\nMy opinion: student 2. Being 2 days apart is more “similar” than different majors.\nstudents 1 and 2: \\(|1 - 1| + |7300 - 7302| = 2\\), students 1 and 3: \\(|1 - 0| + |7300 - 7300| = 1\\)\n\nnope. the variables are on different scales.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#part-3-pre-processing-predictors",
    "href": "L07-nonparametric.html#part-3-pre-processing-predictors",
    "title": "7  Nonparametric Models",
    "section": "Part 3: Pre-processing predictors",
    "text": "Part 3: Pre-processing predictors\nIn nonparametric modeling, we don’t want our definitions of “local windows” or “neighbors” to be skewed by the scales and structures of our predictors.\nIt’s therefore important to create variable recipes which pre-process our predictors before feeding them into a nonparametric algorithm.\nLet’s explore this idea using the bikes data to model rides by temp, season, and breakdowns:\n\n# Load some packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# Load the bikes data and do a little data cleaning\nbikes &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/bike_share.csv\") %&gt;% \n  rename(rides = riders_registered, temp = temp_feel) %&gt;% \n  mutate(temp = round(temp)) %&gt;% \n  mutate(breakdowns = sample(c(rep(0, 728), rep(1, 3)), 731, replace = FALSE)) %&gt;% \n  select(temp, season, breakdowns, rides)\n\n\n\n\n\nStandardizing quantitative predictors\nLet’s standardize or normalize the 2 quantitative predictors, temp and breakdowns, to the same scale: centered at 0 with a standard deviation of 1. Run and reflect upon each chunk below:\n\n\n# Recipe with 1 preprocessing step\nrecipe_1 &lt;- recipe(rides ~ ., data = bikes) %&gt;% \n  step_normalize(all_numeric_predictors())\n    \n# Check it out\nrecipe_1\n\n\n# Check out the first 3 rows of the pre-processed data\n# (Don't worry about the code. Normally we won't do this step.)\nrecipe_1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  head(3)\n\n\n# Compare to first 3 rows of original data\nbikes %&gt;% \n  head(3)\n\nFollow-up questions & comments\n\nTake note of how the pre-processed data compares to the original.\nThe first day had a temp of 65 degrees and a standardized temp of -0.66, i.e. 65 degrees is 0.66 standard deviations below average. Confirm this standardized value “by hand” using the mean and standard deviation in temp:\n\n\nbikes %&gt;% \n    summarize(mean(temp), sd(temp))\n    \n# Standardized temp: (observed - mean) / sd\n(___ - ___) / ___\n\n\n\nSolution\n\n\n# Recipe with 1 preprocessing step\nrecipe_1 &lt;- recipe(rides ~ ., data = bikes) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n# Check it out\nrecipe_1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n\n# Check out the first 3 rows of the pre-processed data\n# (Don't worry about the code. Normally we won't do this step.)\nrecipe_1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  head(3)\n\n# A tibble: 3 × 4\n    temp season breakdowns rides\n   &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 -0.660 winter    -0.0642   654\n2 -0.728 winter    -0.0642   670\n3 -1.75  winter    -0.0642  1229\n\n\n\n# Compare to first 3 rows of original data\nbikes %&gt;% \n  head(3)\n\n  temp season breakdowns rides\n1   65 winter          0   654\n2   64 winter          0   670\n3   49 winter          0  1229\n\n\nFollow-up questions\n\nThe numeric predictors, but not rides, were standardized.\nSee below.\n\n\nbikes %&gt;% \n  summarize(mean(temp), sd(temp))\n\n  mean(temp) sd(temp)\n1   74.69083 14.67838\n\n(65 - 74.69083) / 14.67838\n\n[1] -0.6602111\n\n\n\n\n\nCreating “dummy” variables for categorical predictors\nConsider the categorical season predictor: fall, winter, spring, summer. Since we can’t plug words into a mathematical formula, ML algorithms convert categorical predictors into “dummy variables”, also known as indicator variables. (This is unfortunately the technical term, not something I’m making up.) Run and reflect upon each chunk below:\n\n\n# Recipe with 1 preprocessing step\nrecipe_2 &lt;- recipe(rides ~ ., data = bikes) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n# Check out 3 specific rows of the pre-processed data\n# (Don't worry about the code.)\nrecipe_2 %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  filter(rides %in% c(655, 674))\n\n\n# Compare to the same 3 rows in the original data\nbikes %&gt;% \n  filter(rides %in% c(655, 674))\n\nFollow-up questions & comments\n\n3 of the 4 seasons show up in the pre-processed data as “dummy variables” with 0/1 outcomes. Which season does not appear? This “reference” category is also the one that wouldn’t appear in a table of model coefficients.\nHow is a winter day represented by the 3 dummy variables?\nHow is a fall day represented by the 3 dummy variables?\n\n\n\nSolution\n\n\n# Recipe with 1 preprocessing step\nrecipe_2 &lt;- recipe(rides ~ ., data = bikes) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n# Check it out\nrecipe_2\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n\n# Check out 3 specific rows of the pre-processed data\n# (Don't worry about the code.)\nrecipe_2 %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  filter(rides %in% c(655, 674))\n\n# A tibble: 3 × 6\n   temp breakdowns rides season_spring season_summer season_winter\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1    53          0   674             0             0             1\n2    70          0   674             1             0             0\n3    68          0   655             0             0             0\n\n\n\n# Compare to the same 3 rows in the original data\nbikes %&gt;% \n  filter(rides %in% c(655, 674))\n\n  temp season breakdowns rides\n1   53 winter          0   674\n2   70 spring          0   674\n3   68   fall          0   655\n\n\nFollow-up questions\n\nfall\n0 for spring and summer, 1 for winter\n0 for spring, summer, and winter\n\n\n\n\nCombining pre-processing steps\nWe can also do multiple pre-processing steps! In some cases, order matters. Compare the results of normalizing before creating dummy variables and vice versa:\n\n\n# step_normalize() before step_dummy()\nrecipe(rides ~ ., data = bikes) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  filter(rides %in% c(655, 674))\n\n\n# step_dummy() before step_normalize()\nrecipe(rides ~ ., data = bikes) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  filter(rides %in% c(655, 674))\n\nFollow-up questions / comments\n\nHow did the order of our 2 pre-processing steps impact the outcome?\nThe standardized dummy variables lose some contextual meaning. But, in general, negative values correspond to 0s (not that category), positive values correspond to 1s (in that category), and the further a value is from zero, the less common that category is. We’ll observe in the future how this is advantageous when defining “neighbors”.\n\n\n\nSolution\n\n\n# step_normalize() before step_dummy()\nrecipe(rides ~ ., data = bikes) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  filter(rides %in% c(655, 674))\n\n# A tibble: 3 × 6\n    temp breakdowns rides season_spring season_summer season_winter\n   &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 -1.48     -0.0642   674             0             0             1\n2 -0.320    -0.0642   674             1             0             0\n3 -0.456    -0.0642   655             0             0             0\n\n\n\n# step_dummy() before step_normalize()\nrecipe(rides ~ ., data = bikes) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  filter(rides %in% c(655, 674))\n\n# A tibble: 3 × 6\n    temp breakdowns rides season_spring season_summer season_winter\n   &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 -1.48     -0.0642   674        -0.580        -0.588         1.74 \n2 -0.320    -0.0642   674         1.72         -0.588        -0.573\n3 -0.456    -0.0642   655        -0.580        -0.588        -0.573\n\n\nFollow-up questions / comments\n\nwhen dummies are created second, they remain as 0s and 1s. when dummies are created first, these 0s and 1s are standardized\n\n\n\nPAUSE\nThough our current focus is on nonparametric modeling, the concepts of standardizing and dummy variables are also important in parametric modeling.\n\n\n\nalgorithm\npre-processing step\nnecessary?\ndone automatically behind the R code?\n\n\n\n\nleast squares\nstandardizing\nno\nno (because it’s not necessary!)\n\n\n\ndummy variables\nyes\nyes\n\n\nLASSO\nstandardizing\nyes\nyes\n\n\n\ndummy variables\nyes\nno (we have to pre-process)\n\n\n\n\n\n\n\n\n\nLess common: Removing variables with “near-zero variance”\nNotice that on almost every day in our sample, there were 0 bike station breakdowns. Thus there is near-zero variability (nzv) in the breakdowns predictor:\n\n\nbikes %&gt;% \n  count(breakdowns)\n\nThis extreme predictor could bias our model results – the rare days with 1 breakdown might seem more important than they are, thus have undue influence. To this end, we can use step_nzv():\n\n# Recipe with 3 preprocessing steps\nrecipe_3 &lt;- recipe(rides ~ ., data = bikes) %&gt;% \n  step_nzv(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\n# Check out the first 3 rows of the pre-processed data\n# (Don't worry about the code.)\nrecipe_3 %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  head(3)\n\n\n# Compare to this to the first 3 rows in the original data\nbikes %&gt;% \n  head(3)\n\nFollow-up questions\n\nWhat did step_nzv() do?!\nWe could move step_nzv() to the last step in our recipe. But what advantage is there to putting it first?\n\n\n\nSolution\n\n\n# Recipe with 3 preprocessing steps\nrecipe_3 &lt;- recipe(rides ~ ., data = bikes) %&gt;% \n  step_nzv(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n# Check out the first 3 rows of the pre-processed data\n# (Don't worry about the code.)\nrecipe_3 %&gt;% \n  prep() %&gt;% \n  bake(new_data = bikes) %&gt;% \n  head(3)\n\n# A tibble: 3 × 5\n    temp rides season_spring season_summer season_winter\n   &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 -0.660   654        -0.580        -0.588          1.74\n2 -0.728   670        -0.580        -0.588          1.74\n3 -1.75   1229        -0.580        -0.588          1.74\n\n\n\n# Compare to this to the first 3 rows in the original data\nbikes %&gt;% \n  head(3)\n\n  temp season breakdowns rides\n1   65 winter          0   654\n2   64 winter          0   670\n3   49 winter          0  1229\n\n\nFollow-up questions\n\nit removed breakdowns from the data set.\nmore computationally efficient. don’t spend extra energy on pre-processing breakdowns since we don’t even want to keep it.\n\n\n\n\nThere’s lots more!\n\nThe 3 pre-processing steps above are among the most common. Many others exist and can be handy in specific situations. Run the code below to get a list of possibilities:\n\nls(\"package:recipes\")[startsWith(ls(\"package:recipes\"), \"step_\")]",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#part-4-optional",
    "href": "L07-nonparametric.html#part-4-optional",
    "title": "7  Nonparametric Models",
    "section": "Part 4: Optional",
    "text": "Part 4: Optional\nIf you complete the above exercises in class, you should try the remaining exercises.\nOtherwise, you do not need to loop back – these concepts will be covered in the videos for the next class.\n\n\n\n\nKNN\n\nNow that we have a sense of some themes (defining “local”) and details (measuring “distance”) in nonparametric modeling, let’s explore a common nonparametric algorithm: K Nearest Neighbors (KNN). Let’s start with your intuition for how the KNN works, simply based on its name. On your paper, sketch what you anticipate the following models of the 14 glucose measurements to look like:\n\n\\(K = 1\\) nearest neighbors model\n\n\\(K = 14\\) nearest neighbors model\n\nNOTE: You might start by making predictions at each observed time point (eg: 0, 15 min, 30 min,…). Then think about what the predictions would be for times in between these observations (eg: 5 min).\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nWill vary by group.\n\n\n\nThinking like a machine learner\n\nUpon what tuning parameter does KNN depend?\nWhat’s the smallest value this tuning parameter can take? The biggest?\nSelecting a “good” tuning parameter is a goldilocks challenge:\n\nWhat happens when the tuning parameter is too small?\n\nToo big?\n\n\n\n\n\nSolution\n\n\nnumber of neighbors “K”\n1, 2, …., n (sample size)\nWhen K is too small, our model is too flexible / overfit. When K is too big, our model is too rigid / simple.\n\n\n\n\nDone!\n\n\nRender your notes.\nCheck the solutions on the course website.\nIf you finish all that during class, work on homework!",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#main-points-from-today",
    "href": "L07-nonparametric.html#main-points-from-today",
    "title": "7  Nonparametric Models",
    "section": "Main Points from Today",
    "text": "Main Points from Today\n\nIf the relationship between \\(x\\) and \\(y\\) is not a straight line or a polynomial (such as quadratic), we might need nonparametric methods.\n\nOne needs to consider the scale of variables when calculating distance between observations with more than one predictor.\nPre-processing steps invoke important assumptions that impact your models and predictions.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L07-nonparametric.html#after-class",
    "href": "L07-nonparametric.html#after-class",
    "title": "7  Nonparametric Models",
    "section": "After Class",
    "text": "After Class\n\nFinish the activity, check the solutions, and reach out with questions!\nSubmit HW2 by 11:59 pm TONIGHT\n\nNote the 8-hour grace period / submission cut-off on Moodle\n\nBefore our next class:\n\nComplete CP6\nInstall the kknn and shiny packages\n\nOther things to get started on:\n\nHW1 revisions (due within one week of receiving feedback)\nHW3 (due Thursday, Oct 10) will be posted soon!",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonparametric Models</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html",
    "href": "L08-knn-bias-variance.html",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#context",
    "href": "L08-knn-bias-variance.html#context",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "Context",
    "text": "Context\n\n\n\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = regression\n\\(y\\) is quantitative\n(nonparametric) algorithm = K Nearest Neighbors (KNN)",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#goal",
    "href": "L08-knn-bias-variance.html#goal",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "Goal",
    "text": "Goal\nOur usual parametric models (eg: linear regression) are too rigid to represent the relationship between \\(y\\) and our predictors \\(x\\). Thus we need more flexible nonparametric models.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#knn-regression",
    "href": "L08-knn-bias-variance.html#knn-regression",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "KNN Regression",
    "text": "KNN Regression\nGoal\nBuild a flexible regression model of a quantitative outcome \\(y\\) by a set of predictors \\(x\\),\n\\[y = f(x) + \\varepsilon\\]\n. . .\nIdea\nPredict \\(y\\) using the data on “neighboring” observations. Since the neighbors have similar \\(x\\) values, they likely have similar \\(y\\) values.\n. . .\nAlgorithm\nFor tuning parameter K, take the following steps to estimate \\(f(x)\\) at each set of possible predictor values \\(x\\):\n\nIdentify the K nearest neighbors of \\(x\\) with respect to Euclidean distance.\nObserve the \\(y\\) values of these neighbors.\nEstimate \\(f(x)\\) by the average \\(y\\) value among the nearest neighbors.\n\n. . .\nOutput\nKNN does not produce a nice formula for \\(\\hat{f}(x)\\), but rather a set of rules for how to calculate \\(\\hat{f}(x)\\).\n. . .\nIn pictures (from ISLR)",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#example-1-review",
    "href": "L08-knn-bias-variance.html#example-1-review",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "EXAMPLE 1: REVIEW",
    "text": "EXAMPLE 1: REVIEW\nLet’s review the KNN algorithm using a shiny app. Run the code below and ignore the syntax!!\n\nClick “Go!” one time only to collect a set of sample data.\nCheck out the KNN with K = 1.\n\nWhat does it mean to pick K = 1?\nWhere are the jumps made?\nCan we write the estimated \\(f(x)\\) (red line) as \\(\\beta_0 + \\beta_1 x + ....\\)?\n\nNow try the KNN with K = 25.\n\nWhat does it mean to pick K = 25?\nIs this more or less wiggly / flexible than when K = 1?\n\nSet K = 100 where 100 is the number of data points. Is this what you expected?\n\n\n\nCode\n# Load packages and data\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(kknn)\nlibrary(ISLR)\n\ndata(College)\ncollege_demo &lt;- College %&gt;% \n  mutate(school = rownames(College)) %&gt;% \n  filter(Grad.Rate &lt;= 100)\n\n# Define a KNN plotting function\nplot_knn &lt;- function(k, plot_data){\n  expend_seq &lt;- sort(c(plot_data$Expend, seq(3000, 57000, length = 5000)))\n  #knn_mod &lt;- knn.reg(train = plot_data$Expend, test = data.frame(expend_seq), y = plot_data$Grad.Rate, k = k)\n  knn_results &lt;- nearest_neighbor() %&gt;%\n    set_mode(\"regression\") %&gt;% \n    set_engine(engine = \"kknn\") %&gt;% \n    set_args(neighbors = k) %&gt;% \n    fit(Grad.Rate ~ Expend, data = plot_data) %&gt;% \n    augment(new_data = data.frame(Expend = expend_seq)) %&gt;% \n    rename(expend_seq = Expend, pred_2 = .pred)\n  ggplot(plot_data, aes(x = Expend, y = Grad.Rate)) + \n    geom_point() + \n    geom_line(data = knn_results, aes(x = expend_seq, y = pred_2), color = \"red\") + \n    labs(title = paste(\"K = \", k), y = \"Graduation Rate\", x = \"Per student expenditure ($)\") +\n    lims(y = c(0,100))\n}\n\n\n# BUILD THE SERVER\n# These are instructions for building the app - what plot to make, what quantities to calculate, etc\nserver_KNN &lt;- function(input, output) {\n  new_data &lt;- eventReactive(input$do, {\n    sample_n(college_demo, size = 100)\n  })\n  output$knnpic &lt;- renderPlot({\n    plot_knn(k = input$kTune, plot_data = new_data())\n  })\n}\n\n\n# BUILD THE USER INTERFACE (UI)\n# The UI controls the layout, appearance, and widgets (eg: slide bars).\nui_KNN &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      h4(\"Sample 100 schools:\"), \n      actionButton(\"do\", \"Go!\"),\n      h4(\"Tune the KNN algorithm:\"), \n      sliderInput(\"kTune\", \"K\", min = 1, max = 100, value = 1)\n    ),\n    mainPanel(\n      h4(\"KNN Plot:\"), \n      plotOutput(\"knnpic\")\n    )\n  )\n)\n\n\n# RUN THE SHINY APP!\nshinyApp(ui = ui_KNN, server = server_KNN)\n\n\n\n\nSolution\n\n\ndone\nKNN with K = 1:\n\npredict grad rate using the data on the 1 closest neighbor\nwhere the neighorhood changes (in between observed points)\nno\n\nKNN with K = 25:\n\npredict grad rate by average grad rate among 25 closest neighbors\nless wiggly / less flexible\n\nprobably not. it’s not a straight line. You might have expected a horizontal line at one value. There is a bit more going on to the algorithm (a weighted average).\n\n\n\nKNN DETAILS: WEIGHTED AVERAGE\nThe tidymodels KNN algorithm predicts \\(y\\) using weighted averages.\nThe idea is to give more weight or influence to closer neighbors, and less weight to “far away” neighbors.\nOptional math: Let (\\(y_1, y_2, ..., y_K\\)) be the \\(y\\) outcomes of the K neighbors and (\\(w_1, w_2, ..., w_K\\)) denote the corresponding weights. These weights are defined by a “kernel function” which ensures that: (1) the \\(w_i\\) add up to 1; and (2) the closer the neighbor \\(i\\), the greater its \\(w_i\\). Then the neighborhood prediction of \\(y\\) is:\n\\[\\sum_{i=1}^K w_i y_i\\]",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#example-2-bias-variance-tradeoff",
    "href": "L08-knn-bias-variance.html#example-2-bias-variance-tradeoff",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "EXAMPLE 2: BIAS-VARIANCE TRADEOFF",
    "text": "EXAMPLE 2: BIAS-VARIANCE TRADEOFF\nWhat would happen if we had gotten a different sample of data?!?\n\n\nBias:  On average, across different datasets, how close are the estimates of \\(f(x)\\) to the observed \\(y\\) outcomes?\n\nWe have high bias if our estimates are far from the observed \\(y\\) outcomes\nWe have low bias if our estimates are close to the observed \\(y\\) outcomes\n\nVariance: How variable are the estimates of \\(f(x)\\) from dataset to dataset? Are the estimates stable or do they vary a lot?\n\nWe have high variance if our estimates change a lot from dataset to dataset\nWe have low variance if our estimates don’t change much from dataset to dataset\n\n\n\n\n\nTo explore the properties of overly flexible models, set K = 1 and click “Go!” several times to change the sample data. How would you describe how KNN behaves from dataset to dataset:\n\nlow bias, low variance\nlow bias, high variance\nmoderate bias, low variance\nhigh bias, low variance\nhigh bias, high variance\n\nTo explore the properties of overly rigid models, repeat part a for K = 100:\n\nlow bias, low variance\nlow bias, high variance\nmoderate bias, low variance\nhigh bias, low variance\nhigh bias, high variance\n\nTo explore the properties of more “balanced” models, repeat part a for K = 25:\n\nlow bias, low variance\nlow bias, high variance\nmoderate bias, low variance\nhigh bias, low variance\nhigh bias, high variance\n\n\n\n\nSolution\n\n\nlow bias, high variance\nhigh bias, low variance\nmoderate bias, low variance",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#example-3-bias-variance-reflection",
    "href": "L08-knn-bias-variance.html#example-3-bias-variance-reflection",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "EXAMPLE 3: BIAS-VARIANCE REFLECTION",
    "text": "EXAMPLE 3: BIAS-VARIANCE REFLECTION\nIn general…\n\nWhy is “high bias” bad?\nWhy is “high variability” bad?\nWhat is meant by the bias-variance tradeoff?\n\n\n\nSolution\n\n\nOn average, our prediction errors are large or high\nThe model is not stable / trustworthy, changes depending on the sample data\nIdeally, both bias and variance would be low. BUT when we improve one of the features, we hurt the other.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#example-4-bias-variance-tradeoff-for-past-algorithms",
    "href": "L08-knn-bias-variance.html#example-4-bias-variance-tradeoff-for-past-algorithms",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "EXAMPLE 4: BIAS-VARIANCE TRADEOFF FOR PAST ALGORITHMS",
    "text": "EXAMPLE 4: BIAS-VARIANCE TRADEOFF FOR PAST ALGORITHMS\n\nThe LASSO algorithm depends upon tuning parameter \\(\\lambda\\):\n\nWhen \\(\\lambda\\) is too small, the model might keep too many predictors, hence be overfit.\nWhen \\(\\lambda\\) is too big, the model might kick out too many predictors, hence be too simple.\n\nWith this in mind:\n\n\nFor which values of \\(\\lambda\\) (small or large) will LASSO be the most biased?\n\nFor which values of \\(\\lambda\\) (small or large) will LASSO be the most variable?\n\n\nThe bias-variance tradeoff also comes into play when comparing across algorithms, not just within algorithms. Consider LASSO vs least squares:\n\nWhich will tend to be more biased?\n\nWhich will tend to be more variable?\n\nWhen will LASSO beat least squares in the bias-variance tradeoff game?\n\n\n\n\nSolution\n\n\n.\n\nlarge. too simple / rigid\nsmall. too overfit / flexible\n\n.\n\nLASSO. it’s simpler\nleast squares. it’s more flexible\nwhen the least squares is overfit",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#instructions",
    "href": "L08-knn-bias-variance.html#instructions",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "Instructions",
    "text": "Instructions\nContext\nUsing the College dataset from the ISLR package, we’ll explore the KNN model of college graduation rates (Grad.Rate) by:\n\ninstructional expenditures per student (Expend)\nnumber of first year students (Enroll)\nwhether the college is Private\n\n\n# Load packages\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(ISLR)\n\n# Load data\ndata(College)\n\n# Wrangle the data\ncollege_sub &lt;- College %&gt;% \n  mutate(school = rownames(College)) %&gt;% \n  arrange(factor(school, levels = c(\"Macalester College\", \"Luther College\", \"University of Minnesota Twin Cities\"))) %&gt;% \n  filter(Grad.Rate &lt;= 100) %&gt;% \n  filter((Grad.Rate &gt; 50 | Expend &lt; 40000)) %&gt;% \n  select(Grad.Rate, Expend, Enroll, Private)\n\nCheck out a codebook from the console:\n\n?College\n\n. . .\nGoals\n\nUnderstand how “neighborhoods” are defined using multiple predictors (both quantitative and categorical) and how data pre-processing steps are critical to this definition.\nTune and build a KNN model in R.\nApply the KNN model.\nIt is easier to review code than to deepen your understanding of new concepts outside class. Prioritize and focus on the concepts over the R code. You will later come back and reflect on the code.\n\n. . .\nDirections\n\nStay engaged. Studies show that when you’re playing cards, watching vids, continuously on your message app, it impacts both your learning and the learning of those around you.\nBe kind to yourself. You will make mistakes!\nBe kind to each other. Collaboration improves higher-level thinking, confidence, communication, community, & more.\n\nactively contribute to discussion\nactively include all other group members in discussion\ncreate a space where others feel comfortable making mistakes & sharing their ideas\nstay in sync\n\nAs you go, consider: W.A.I.T. (Why Am/Aren’t I Talking?)",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#questions",
    "href": "L08-knn-bias-variance.html#questions",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "Questions",
    "text": "Questions\n\nPart 1: Identifying neighborhoods\nThe KNN model for Grad.Rate will hinge upon the neighborhoods defined by the 3 Expend, Enroll, and Private predictors. And these neighborhoods hinge upon how we pre-process our predictors.\nWe’ll explore these ideas below using the results of the following chunk. Run this, but DON’T spend time examining the code!\n\n\nrecipe_fun &lt;- function(recipe){\n  recipe &lt;- recipe %&gt;% \n    prep() %&gt;% \n    bake(new_data = college_sub) %&gt;% \n    head(3) %&gt;% \n    select(-Grad.Rate) %&gt;% \n    as.data.frame()\n  row.names(recipe) &lt;- c(\"Mac\", \"Luther\", \"UMN\")\n  return(recipe)\n}\n# Recipe 1: create dummies, but don't standardize\nrecipe_1 &lt;- recipe(Grad.Rate ~ Expend + Enroll + Private, data = college_sub) %&gt;% \n  step_nzv(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors())\nrecipe_1_data &lt;- recipe_fun(recipe_1)\n\n# Recipe 2: standardize, then create dummies\nrecipe_2 &lt;- recipe(Grad.Rate ~ Expend + Enroll + Private, data = college_sub) %&gt;% \n  step_nzv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors())\nrecipe_2_data &lt;- recipe_fun(recipe_2)\n\n# Recipe 3: create dummies, then standardize\nrecipe_3 &lt;- recipe(Grad.Rate ~ Expend + Enroll + Private, data = college_sub) %&gt;% \n  step_nzv(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\nrecipe_3_data &lt;- recipe_fun(recipe_3)\n\n\n\n\n\nFeature space\n\nCheck out the feature space of our 3 predictors and take note of which school is the closer neighbor of Mac: UMN or Luther.\n\nggplot(college_sub, aes(x = Expend, y = Enroll, color = Private)) + \n  geom_point(alpha = 0.5) + \n  geom_text(data = head(college_sub, 3), aes(x = Expend, y = Enroll, label = c(\"Mac\", \"Luther\", \"UMN\")), color = \"black\")\n\n\n\n\n\n\n\n\n\n\nSolution\n\nLuther\n\n\n\nWhat happens when we don’t standardize the predictors?\nOf course, KNN relies upon mathematical metrics (Euclidean distance), not visuals, to define neighborhoods. And these neighborhoods depend upon how we pre-process our predictors. Consider the pre-processing recipe_1 which uses step_dummy() but not step_normalize():\n\n\nrecipe_1_data\n\n       Expend Enroll Private_Yes\nMac     14213    452           1\nLuther   8949    587           1\nUMN     16122   3524           0\n\n\n\nUse this pre-processed data to calculate the Euclidean distance between Mac and Luther:\n\n\nsqrt((14213 - ___)^2 + (452 - ___)^2 + (1 - ___)^2)\n\n\nCheck your distance calculation, and calculate the distances between the other school pairs, using dist().\n\n\ndist(recipe_1_data)\n\n\nBy this metric, is Mac closer to Luther or UMN? So is this a reasonable metric? If not, why did this happen?\n\n\n\nSolution\n\n\n# a\nsqrt((14213 - 8949)^2 + (452 - 587)^2 + (1 - 1)^2)\n\n[1] 5265.731\n\n# b\ndist(recipe_1_data)\n\n            Mac   Luther\nLuther 5265.731         \nUMN    3616.831 7750.993\n\n\n\nUMN. The quantitative predictors are on different scales\n\n\n\n\nWhat happens when we standardize then create dummy predictors?\nThe metric above was misleading because it treated enrollments (people) and expenditures ($) as if they were on the same scale. In contrast, recipe_2 first uses step_normalize() and then step_dummy() to pre-process the predictors:\n\n\nrecipe_2_data\n\n           Expend     Enroll Private_Yes\nMac     0.9025248 -0.3536904           1\nLuther -0.1318021 -0.2085505           1\nUMN     1.2776255  2.9490477           0\n\n\nCalculate the distance between each pair of schools using these pre-processed data:\n\ndist(recipe_2_data)\n\nBy this metric, is Mac closer to Luther or UMN? So is this a reasonable metric?\n\n\nSolution\n\nLuther. Yes.\n\nrecipe_2_data\n\n           Expend     Enroll Private_Yes\nMac     0.9025248 -0.3536904           1\nLuther -0.1318021 -0.2085505           1\nUMN     1.2776255  2.9490477           0\n\ndist(recipe_2_data)\n\n            Mac   Luther\nLuther 1.044461         \nUMN    3.471135 3.599571\n\n\n\n\n\nWhat happens when we create dummy predictors then standardize?\nWhereas recipe_2 first uses step_normalize() and then step_dummy() to pre-process the predictors, recipe_3 first uses step_dummy() and then step_normalize():\n\n\nrecipe_3_data\n\n           Expend     Enroll Private_Yes\nMac     0.9025248 -0.3536904   0.6132441\nLuther -0.1318021 -0.2085505   0.6132441\nUMN     1.2776255  2.9490477  -1.6285680\n\n\n\nHow do the pre-processed data from recipe_3 compare those to recipe_2?\nRECALL: The standardized dummy variables lose some contextual meaning. But, in general, negative values correspond to 0s (not that category), positive values correspond to 1s (in that category), and the further a value is from zero, the less common that category is.\nCalculate the distance between each pair of schools using these pre-processed data. By this metric, is Mac closer to Luther or UMN?\n\n\ndist(recipe_3_data)\n\n\nHow do the distances resulting from recipe_3 compare to those from recipe_2?\n\n\ndist(recipe_2_data)\n\n\nUnlike recipe_2, recipe_3 considered the fact that private schools are relatively more common in this dataset, making the public UMN a bit more unique. Why might this be advantageous when defining neighborhoods? Thus why will we typically first use step_dummy() before step_normalize()?\n\n\ncollege_sub %&gt;% \n  count(Private)\n\n\n\nSolution\n\n\nrecipe_3_data\n\n           Expend     Enroll Private_Yes\nMac     0.9025248 -0.3536904   0.6132441\nLuther -0.1318021 -0.2085505   0.6132441\nUMN     1.2776255  2.9490477  -1.6285680\n\n\n\nPrivate_Yes is no 0.6132441 or -1.6285680, not 1 or 0.\nLuther\n\n\ndist(recipe_3_data)\n\n            Mac   Luther\nLuther 1.044461         \nUMN    4.009302 4.120999\n\n\n\nThe distance between Mac and Luther is the same, but the distance between Mac and UMN is bigger.\n\n\ndist(recipe_2_data)\n\n            Mac   Luther\nLuther 1.044461         \nUMN    3.471135 3.599571\n\n\n\nSince public schools are more “rare”, the difference between Mac (private) and UMN (public) is conceptually bigger than if private and public schools were equally common.\n\n\n\n\n\nPart 2: Build the KNN\nWith a grip on neighborhoods, let’s now build a KNN model for Grad.Rate.\nFor the purposes of this activity (focusing on concepts over R code), simply run each chunk and note what object it’s storing.\nYou will later be asked to come back and comment on the code.\nSTEP 1: Specifying the KNN model\n\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(engine = \"kknn\") %&gt;% \n  set_args(neighbors = tune())\n\nSTEP 2: Variable recipe (with pre-processing)\nNote that we use step_dummy() before step_normalize().\n\nvariable_recipe &lt;- recipe(Grad.Rate ~ ., data = college_sub) %&gt;% \n  step_nzv(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nSTEP 3: workflow specification (model + recipe)\n\nknn_workflow &lt;- workflow() %&gt;% \n  add_model(knn_spec) %&gt;% \n  add_recipe(variable_recipe)\n\nSTEP 4: estimate multiple KNN models\nThis code builds 50 KNN models of Grad.Rate, using 50 possible values of K ranging from 1 to 200 (roughly 25% of the sample size of 775).\nIt then evaluates these models with respect to their 10-fold CV MAE.\n\nset.seed(253)\nknn_models &lt;- knn_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(neighbors(range = c(1, 200)), levels = 50),\n    resamples = vfold_cv(college_sub, v = 10),\n    metrics = metric_set(mae)\n  )\n\n\n\nPart 3: Finalize and apply the KNN\n\nCompare the KNN models\nPlot the CV MAE for each of the KNN models with different tuning parameters K. NOTE: This is the same function we used for the LASSO!\n\n\nknn_models %&gt;% \n  autoplot()\n\n\nUse this plot to describe the goldilocks problem in tuning the KNN:\n\nWhen K is too small, CV MAE increases because the model is too …\n\nWhen K is too big, CV MAE increases because the model is too …\n\n\nWhy did we try a range of K values?\nIn KNN modeling, we typically want to minimize the prediction errors. Given this goal, is the range of K values we tried wide enough? Or might there be a better value for K outside this range?\nIn KNN modeling, why won’t we typically worry about “parsimony”?\n\n\n\nSolution\n\n\nknn_models %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nWhen K is too small, CV MAE increases because the model is too flexible (overfit). When K is too big, CV MAE increases because the model is too rigid (simple).\nBecause we can’t know in advance what a good value of K is.\nYes, it’s wide enough. We observe that CV MAE is minimized when K is around 25, it then increases from there.\n\n\n\n\nPick K\n\n\nIdentify which value of K minimizes the CV MAE. Make sure this matches up with what you observe in the plot.\n\n\nbest_k &lt;- knn_models %&gt;% \n  select_best()\nbest_k\n\n\nCalculate and interpret the CV MAE for this model.\n\n\n# Plug in a number or best_k$neighbors\nknn_models %&gt;% \n  collect_metrics() %&gt;% \n  filter(neighbors == ___)\n\n\n\nSolution\n\n\n33 neighbors\n\n\nbest_k &lt;- knn_models %&gt;% \n  select_best()\nbest_k\n\n# A tibble: 1 × 2\n  neighbors .config              \n      &lt;int&gt; &lt;chr&gt;                \n1        33 Preprocessor1_Model09\n\n\n\nWe expect the predictions of grad rate for new schools (not in our sample) to be off by 11.3 percentage points.\n\n\n# Plug in a number or best_k$neighbors\nknn_models %&gt;% \n  collect_metrics() %&gt;% \n  filter(neighbors == 33)\n\n# A tibble: 1 × 7\n  neighbors .metric .estimator  mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        33 mae     standard    11.3    10   0.516 Preprocessor1_Model09\n\n\n\n\n\nFinal KNN model\n\nBuild your “final” KNN model using the optimal value you found for K above. NOTE: We only looked at roughly every 4th possible value of K (K = 1, 5, 9, etc). If we wanted to be very thorough, we could re-run our algorithm using each value of K close to our optimal K.\n\nfinal_knn &lt;- knn_workflow %&gt;% \n  finalize_workflow(parameters = best_k) %&gt;% \n  fit(data = college_sub)\n\nWhat does a tidy() summary of final_knn give you? Does this surprise you?\n\n# DO NOT REMOVE eval = FALSE\nfinal_knn %&gt;% \n  tidy()\n\n\n\nSolution\n\n\nfinal_knn &lt;- knn_workflow %&gt;% \n  finalize_workflow(parameters = best_k) %&gt;% \n  fit(data = college_sub)\n\nSince this is a nonparametric method, we can summary the functional relationship with parameters and thus tidy() doesn’t have estimates to give us.\n\n\n\n\n\n\nMake some predictions\nCheck out Mac and Luther. NOTE: This is old data. Mac’s current graduation rate is one of the highest (roughly 90%)!\n\n\n# Check out Mac & Luther\nmac_luther &lt;- college_sub %&gt;% \n  head(2)\nmac_luther\n\n                   Grad.Rate Expend Enroll Private\nMacalester College        77  14213    452     Yes\nLuther College            77   8949    587     Yes\n\n\n\nUse your KNN model to predict Mac and Luther’s graduation rates. How close are these to the truth?\n\n\n# Prediction\n___ %&gt;% \n  ___(new_data = ___)\n\n\nDo you have any idea why Mac’s KNN prediction is higher than Luther’s? If so, are you using context or something you learned from the KNN?\n\n\n\nSolution\n\n\n.\n\n\n# Prediction\nfinal_knn %&gt;% \n  predict(new_data = mac_luther)\n\n# A tibble: 2 × 1\n  .pred\n  &lt;dbl&gt;\n1  78.8\n2  75.7\n\n\n\nIf you have any ideas, it’s probably based on context because the KNN hasn’t given us any info about why it returns higher or lower predictions of grad rate.\n\n\n\n\n\n\n\nKNN pros and cons\n\n\nWhat assumptions did the KNN model make about the relationship of Grad.Rate with Expend, Enroll, and Private?\nWhat did the KNN model tell us about the relationship of Grad.Rate with Expend, Enroll, and Private?\nReflecting upon a and b, name one pro of using a nonparametric algorithm like the KNN instead of a parametric algorithm like least squares or LASSO.\nSimilarly, name one con.\nConsider another “con”. Just as with parametric models, we could add more and more predictors to our KNN model. However, the KNN algorithm is known to suffer from the curse of dimensionality. Why? (A quick Google search might help.)\n\n\n\nSolution\n\n\nnone\nnot much – we can just use it for predictions\nKNN doesn’t make assumptions about relationships (which is very flexible!)\nKNN doesn’t provide much insight into the relationships between y and x\nWhen calculated by more and more predictors, our nearest neighbors might actually be far away (thus not very similar).\n\n\n\n\nParametric or nonparametric\n\n\nSuppose we wanted to model the relationship of room and board costs vs out-of-state tuition. Would you use the parametric least squares algorithm or the nonparametric KNN algorithm?\n\n\nggplot(College, aes(x = Outstate, y = Room.Board)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nIn general, in what situations would you use least squares instead of KNN? Why?\n\n\n\nSolution\n\n\nleast squares. the relationship is linear, so doesn’t require the flexibility of a nonparametric algorithm\nwhen the relationships of interest can be reasonably represented by least squares, we should use least squares. It provides much more insight into the relationships.\n\n\n\n\n\n\n\nR code reflection\n\nRevisit all code in Parts 2 and 3 of the exercises. Comment upon each chunk:\n\nWhat is it doing?\nHow, if at all, does it differ from the least squares and LASSO code?\n\n\n\n\n\nData drill\n\nCalculate the mean Enroll for public vs private schools.\nPlot the relationship of Grad.Rate vs Enroll, Private, and Expend.\nIdentify the private schools with first year enrollments exceeding 3000.\nAsk and answer another question of interest to you.\n\n\n\n\nSolution\n\n\n# a\ncollege_sub %&gt;% \n  group_by(Private) %&gt;% \n  summarize(mean(Enroll))\n\n# A tibble: 2 × 2\n  Private `mean(Enroll)`\n  &lt;fct&gt;            &lt;dbl&gt;\n1 No               1641.\n2 Yes               457.\n\n# b\nggplot(college_sub, aes(y = Grad.Rate, x = Enroll, size = Expend, color = Private)) + \n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n# c\ncollege_sub %&gt;% \n  filter(Enroll &gt; 3000, Private == \"Yes\")\n\n                                  Grad.Rate Expend Enroll Private\nBoston University                        72  16836   3810     Yes\nBrigham Young University at Provo        33   7916   4615     Yes\nUniversity of Delaware                   75  10650   3252     Yes",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#todays-material",
    "href": "L08-knn-bias-variance.html#todays-material",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "Today’s Material",
    "text": "Today’s Material\n\nWrap up the activity, check the solutions, and watch the optional R code tutorial posted for today.\nRemember that there’s an R code reference section at the bottom of these notes!",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L08-knn-bias-variance.html#upcoming-deadlines",
    "href": "L08-knn-bias-variance.html#upcoming-deadlines",
    "title": "8  KNN Regression and the Bias-Variance Tradeoff",
    "section": "Upcoming Deadlines",
    "text": "Upcoming Deadlines\n\nCP7: due before our next class\nHomework:\n\nHW1 revisions due next Tuesday\nStart HW3\n\nGroup Assignment 1:\n\nRead the directions\nStart exploring the data and potential models\nMake visualizaitons, fit models, evaluate models\nThis is great practice / review of code & concepts\n\nQuiz 1 coming up in 1.5 weeks!",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>KNN Regression and the Bias-Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html",
    "href": "L09-loess-gams.html",
    "title": "9  LOESS & GAMs",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#announcements",
    "href": "L09-loess-gams.html#announcements",
    "title": "9  LOESS & GAMs",
    "section": "Announcements",
    "text": "Announcements\n\nQuiz 1 next Tuesday\n\nTopic: Regression (Units 1–3)\nPart 1 (on paper):\n\nclosed notes, closed collaboration\ndue by end of class\nmay have to interpret R output, but won’t have to produce code\n\nPart 2 (on computer):\n\nopen* notes (current 253 course materials only), open* collaboration (current 253 students only)\ndue within 24 hours, but designed to finish during class\n\n\nHW3 due Thursday\n\nSpecial instructions for this one – review the Important Notes before you begin!\nLimited extension opportunities to facilitate quick turnaround on feedback/sharing solutions (in time for Quiz 1)",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#learning-goals",
    "href": "L09-loess-gams.html#learning-goals",
    "title": "9  LOESS & GAMs",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nClearly describe the local regression algorithm for making a prediction\nExplain how bandwidth (span) relate to the bias-variance tradeoff\nDescribe some different formulations for a GAM (how the arbitrary functions are represented)\nExplain how to make a prediction from a GAM\nInterpret the output from a GAM",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#context",
    "href": "L09-loess-gams.html#context",
    "title": "9  LOESS & GAMs",
    "section": "Context",
    "text": "Context\n\n\n\nCONTEXT\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = regression\n\\(y\\) is quantitative\n(nonparametric) algorithm\n\n\n GOAL\nOur usual parametric models (eg: linear regression) are too rigid to represent the relationship between \\(y\\) and our predictors \\(x\\). Thus we need more flexible nonparametric models.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#example-1-knn-review",
    "href": "L09-loess-gams.html#example-1-knn-review",
    "title": "9  LOESS & GAMs",
    "section": "Example 1: KNN Review",
    "text": "Example 1: KNN Review\nIn the previous activity, we modeled college Grad.Rate versus Expend, Enroll, and Private using data on 775 schools.\n\nWe chose the KNN with K = 33 because it minimized the CV MAE, i.e. the errors when predicting grad rate for schools outside our sample. We don’t typically worry about a more parsimonious KNN, i.e. a model that has slightly higher prediction errors but is easier to interpret, apply, etc. Why?\nWhat assumptions did the KNN model make about the relationship of Grad.Rate with Expend, Enroll, and Private? Is this a pro or con?\nWhat did the KNN model tell us about the relationship of Grad.Rate with Expend, Enroll, and Private? For example, did it give you a sense of whether grad rates are higher at private or public institutions? At institutions with higher or lower enrollments? Is this a pro or con?\n\n\n\nSolution\n\n\nThe output of the model is: one prediction for one observational unit. There is nothing to interpret (no plots, no coefficients, etc.). Also: there is no way to tune the model to use fewer predictors.\nThe only assumption we made was that the outcome values of \\(y\\) should be similar if the predictor values of \\(x\\) are similar. No other assumptions are made.\n\nThis is a pro if we want flexibility due to non-linear relationships and that assumption is true.\nThis is a con if relationships are actually linear or could be modeled with a parametric model.\n\nNothing.\n\nI’d say this is a con. There is nothing to interpret, so the model is more of a black box in terms of knowing why it gives you a particular prediction.\n\n\nFurther details are in solutions for previous activity.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#example-2-nonparametric-vs-parametric",
    "href": "L09-loess-gams.html#example-2-nonparametric-vs-parametric",
    "title": "9  LOESS & GAMs",
    "section": "Example 2: Nonparametric vs Parametric",
    "text": "Example 2: Nonparametric vs Parametric\nNonparametric KNN vs parametric least squares and LASSO:\n\nWhen should we use a nonparametric algorithm like KNN?\nWhen shouldn’t we?\n\n\n\nSolution\n\n\nUse nonparametric methods when parametric model assumptions are too rigid. Forcing a parametric method in this situation can produce misleading conclusions.\nUse parametric methods when the model assumptions hold. In such cases, parametric models provide more contextual insight (eg: meaningful coefficients) and the ability to detect which predictors are beneficial to the model.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#notes",
    "href": "L09-loess-gams.html#notes",
    "title": "9  LOESS & GAMs",
    "section": "Notes",
    "text": "Notes\nLocal Regression or Locally Estimated Scatterplot Smoothing (LOESS)\n. . .\nGoal:\nBuild a flexible regression model of \\(y\\) by one quantitative predictor \\(x\\),\n\\[y = f(x) + \\varepsilon\\]\n. . .\nIdea:\nFit regression models in small localized regions, where nearby data have greater influence than far data.\n. . .\nAlgorithm:\nDefine the span, aka bandwidth, tuning parameter \\(h\\) where \\(0 \\le h \\le 1\\). Take the following steps to estimate \\(f(x)\\) at each possible predictor value \\(x\\):\n\nIdentify a neighborhood consisting of the \\(100∗h\\)% of cases that are closest to \\(x\\).\n\nPutting more weight on the neighbors closest to \\(x\\) (ie. allowing them to have more influence), fit a linear model in this neighborhood.\n\nUse the local linear model to estimate f(x).\n\n. . .\nIn pictures:",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#discussion",
    "href": "L09-loess-gams.html#discussion",
    "title": "9  LOESS & GAMs",
    "section": "Discussion",
    "text": "Discussion\nOpen the QMD and find the LOESS discussion questions. Work on Examples 3 and 4 with your group.\n\n\nExample 3: LOESS in R\nWe can plot LOESS models using geom_smooth(). Play around with the span parameter below.\n\nWhat happens as we increase the span from roughly 0 to roughly 1?\nWhat is one “pro” of this nonparametric algorithm, relative to KNN?\nWhat questions do you have about this algorithm?\n\n\nlibrary(tidyverse)\nlibrary(ISLR)\ndata(College)\n\nggplot(College, aes(x = Expend, y = Grad.Rate)) + \n  geom_point() + \n  geom_smooth(method = \"loess\", span = 0.5)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\nas we increase span, the model becomes smoother and more simple\ncompare to KNN, LOESS is smoother, more like the shape of the relationship we’re trying to estimate\n\n\n\nNote: you’ll find that you can specify span greater than 1. Use your resources to figure out what that means in terms of the algorithm.\n\n\n\nExample 4: LOESS & the Bias-Variance Tradeoff\nRun the shiny app code, below. Explore the impact of the span tuning parameter h on the LOESS performance across different datasets. Continue to click the Go! button to get different datasets.\nFor what values of \\(h\\) (near 0, somewhere in the middle, near 1) do you get the following:\n\nhigh bias but low variance\nlow bias but high variance\nmoderate bias and low variance\n\n\n\nSolution\n\n\nh near 1\nh near 0\nh somewhere in the middle\n\n\n\n\n# Load packages & data\nlibrary(shiny)\nlibrary(tidyverse)\n\n# Define a LOESS plotting function\nplot_loess &lt;- function(h, plot_data){\n  ggplot(plot_data, aes(x = x, y = y)) + \n    geom_point() + \n    geom_smooth(span = h, se = FALSE) + \n    labs(title = paste(\"h = \", h)) +\n    lims(y = c(-5,12))\n}\n\n# BUILD THE SERVER\n# These are instructions for building the app - what plot to make, what quantities to calculate, etc\nserver_LOESS &lt;- function(input, output) {\n  new_data &lt;- eventReactive(input$do, {\n    x &lt;- c(runif(25, -6, -2.5), runif(50, -3, 3), runif(25, 2.5, 6))\n    y &lt;- 9 - x^2 + rnorm(100, sd = 0.75) \n    y[c(1:25, 76:100)] &lt;- rnorm(50, sd = 0.75)\n    data.frame(x, y)\n  })\n  output$loesspic &lt;- renderPlot({\n    plot_loess(h = input$hTune, plot_data = new_data())\n  })\n}\n\n\n# BUILD THE USER INTERFACE (UI)\n# The UI controls the layout, appearance, and widgets (eg: slide bars).\nui_LOESS &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      h4(\"Sample 100 observations:\"), \n      actionButton(\"do\", \"Go!\"),\n      h4(\"Tune the LOESS:\"), \n      sliderInput(\"hTune\", \"h\", min = 0.05, max = 1, value = 0.05)\n    ),\n    mainPanel(\n      h4(\"LOESS plot:\"), \n      plotOutput(\"loesspic\")\n    )\n  )\n)\n\n\n# RUN THE SHINY APP!\nshinyApp(ui = ui_LOESS, server = server_LOESS)",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#notes-1",
    "href": "L09-loess-gams.html#notes-1",
    "title": "9  LOESS & GAMs",
    "section": "Notes",
    "text": "Notes\nGeneralized Additive Models (GAM)\nGAMs are nonparametric nonlinear models that can handle more than one predictor. They incorporate each predictor \\(x_i\\) through some nonparametric, smooth function \\(f_i()\\):\n\\[y = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p) + \\varepsilon\\]\n. . .\nBig ideas\n\nEach \\(f_j(x_j)\\) is a smooth model of \\(y\\) vs \\(x_j\\) when controlling for the other predictors. More specifically:\n\nEach \\(f_j(x_j)\\) models the behavior in \\(y\\) that’s not explained by the other predictors.\nThis “unexplained behavior” is represented by the residuals from the model of \\(y\\) versus all predictors.\n\nThe \\(f_j()\\) functions are estimated using some smoothing algorithm (e.g. LOESS, smoothing splines, etc).\n\n\n. . .\nTECHNICAL NOTE\nIn tidymodels():\n\nThe GAM f(x) components are estimated using smoothing splines, a nonparametric smoothing technique that’s more nuanced than LOESS.\nSmoothing splines depend upon a \\(\\lambda\\) penalty tuning parameter (labeled adjust_deg_free in tidymodels). As in the LASSO:\n\nthe bigger the \\(\\lambda\\), the more simple / less wiggly the estimate of f(x)\nif \\(\\lambda\\) is big enough, we might even kick a predictor out of the model\n\n\nMore optional details are included in the “Deeper learning” section.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#discussion-1",
    "href": "L09-loess-gams.html#discussion-1",
    "title": "9  LOESS & GAMs",
    "section": "Discussion",
    "text": "Discussion\nFind the GAM discussion questions in your QMD. Discuss the following example with your group.\n\n\nExample 5: GAM\nInterpret the wage analysis in Chapter 7 of ISLR.\nwage = \\(\\beta_0\\) + f(year) + f(age) + f(education) + \\(\\varepsilon\\)\n\n\n\n\n\nSolution\n\nfor example: when controlling for education level and year, wages have a quadratic-ish relationship with age. Both younger and older workers tend to have lower wages.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#getting-started",
    "href": "L09-loess-gams.html#getting-started",
    "title": "9  LOESS & GAMs",
    "section": "Getting Started",
    "text": "Getting Started\nGoals\n\nImplement and explore GAM in R.\n\nExplore how this algorithm compares to others, hence how it fits into our broader ML workflow.\n\nContext\nUsing the College data in the ISLR package, we’ll build a predictive model of Grad.Rate by 6 predictors: Private, PhD (percent of faculty with PhDs), Room.Board, Personal (estimated personal spending), Outstate (out-of-state tuition), perc.alumni (percent of alumni who donate)\n\n# Load packages\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(ISLR)\n\n# Load data\ndata(College)\n\n# Wrangle the data\ncollege_sub &lt;- College %&gt;% \n  filter(Grad.Rate &lt;= 100) %&gt;% \n  filter((Grad.Rate &gt; 50 | Expend &lt; 40000)) %&gt;% \n  select(Grad.Rate, Private, PhD, Room.Board, Personal, Outstate, perc.alumni)",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#part-1-building-a-gam",
    "href": "L09-loess-gams.html#part-1-building-a-gam",
    "title": "9  LOESS & GAMs",
    "section": "Part 1: Building a GAM",
    "text": "Part 1: Building a GAM\nLet’s build a GAM of Grad.Rate by all 6 predictors in college_sub:\n\\[\\text{Grad.Rate} = \\beta_0 + \\beta_1\\text{PrivateYes} + f_2(\\text{PhD})  + \\cdots + f_{6}(\\text{perc.alumni}) + \\varepsilon\\]\nYou’re given all code here. Be sure to scan and reflect upon what’s happening.\n\n\nSTEP 1: Specifying the GAM\nSpecify the GAM algorithm. Ask yourself the following about each line:\n\nHow, if at all, does this differ from the code for LASSO or KNN?\nWhat’s the point?\n\n\ngam_spec &lt;- gen_additive_mod() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(engine = \"mgcv\") %&gt;% \n  set_args(adjust_deg_free = tune(), select_features = TRUE)\n\n\n\n\nSTEP 2: variable recipe\nNOTE: each \\(f_j(x_j)\\) uses only a single predictor \\(x_j\\), we don’t have to worry about predictor scales or pre-processing.\n\nvariable_recipe &lt;- recipe(Grad.Rate ~ ., data = college_sub)\n\n\n\n\n\n\nSTEP 3: workflow specification\nSet up our GAM workflow. Check out the formula required by add_model:\n\nHow did we include the categorical Private predictor?\nHow did we include the other quantitative predictors?\nThinking back to the technical note about the GAM in tidymodels, what do you think s() stands for?\n\n\ngam_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(gam_spec,\n            formula = Grad.Rate ~ Private + s(PhD) + s(Room.Board) + s(Personal) + s(Outstate) + s(perc.alumni))\n\n\n\n\nSTEP 4: estimate multiple GAMs\nFinally, estimate the GAM using a range of values for adjust_deg_free(), the \\(\\lambda\\) tuning parameter.\n\nRecall: the bigger the parameter, the greater the penalty for overfit, wiggly behavior.\nWe’ll try 10 possible values for the penalty parameter, ranging from 0.25 to 4, and evaluate each model with respect to its 10-fold CV MAE.\n\nRunning this code will take some time (typically under 30 seconds). The more predictors we have, the longer it takes!\n\n\nset.seed(253)\ngam_models &lt;- gam_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(adjust_deg_free(range = c(0.25, 4)), \n                        levels = 10),\n    resamples = vfold_cv(college_sub, v = 10),\n    metrics = metric_set(mae)\n  )",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#part-2-identify-and-apply-a-final-gam",
    "href": "L09-loess-gams.html#part-2-identify-and-apply-a-final-gam",
    "title": "9  LOESS & GAMs",
    "section": "Part 2: Identify and apply a “final” GAM",
    "text": "Part 2: Identify and apply a “final” GAM\n\nComputational time\nWe only ran 10 different GAMs of Grad.Rate by our 6 predictors above. What is it about this algorithm that makes it take so long? \nNOTE: In general, the GAM is not great when we have lots of predictors. In that case, we might use a poorly fit LASSO to help pick predictors, and then use GAM with these predictors for our final model and predictions.\n\n\n\nSolution\n\nWithin any 1 model, we’re building many models within local windows.\n\n\n\nIdentify the best GAM\nUsing the same concepts (and code!) as when we tuned the LASSO and KNN algorithms:\n\n\n# Use a plot to compare the predictive performance of our 10 GAM models\n    \n    \n# Pick the best value for the penalty tuning parameter\n# Name this `best_penalty`\n    \n    \n# Fit a final GAM using `best_penalty`\n# Name this `final_gam`\n\n\n\nSolution\n\n\n# Use a plot to compare the predictive performance of our 10 GAM models\ngam_models %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n# Pick the best value for the penalty tuning parameter\n# Name this `best_penalty`\nbest_penalty &lt;- gam_models %&gt;% \n  select_best()\n\n# Fit a final GAM using `best_penalty`\n# Name this `final_gam`\nfinal_gam &lt;- gam_workflow %&gt;% \n  finalize_workflow(parameters = best_penalty) %&gt;%\n  fit(data = college_sub)\n\n\n\n\nExplore the final GAM\nPlot the smoothing functions for each predictor in our model. NOTE: The $ notation accesses nested objects (fit within fit within fit within final_gam).\n\n\nfinal_gam$fit$fit$fit %&gt;% \n  plot(all.terms = TRUE, pages = 1)\n\n\nIs there anything in these plots that justifies our use of the GAM instead of least squares?\nPick 1 or 2 of these plots and interpret your findings.\n\n\n\nSolution\n\n\nfinal_gam$fit$fit$fit %&gt;% \n  plot(all.terms = TRUE, pages = 1)\n\n\n\n\n\n\n\n\n\nkinda. Personal, Outstate, and perc.alumni seem to have complicated non-linear associations with Grad.Rate.\nwill vary\n\n\n\n\nPick an algorithm\nOur overall goal is to build a predictive model of Grad.Rate (\\(y\\)) by 6 predictors:\n\\[y = f(x_1,x_2,...,x_{6}) + \\varepsilon\\]\nThe table below summarizes the CV MAE for 3 possible algorithms: least squares, KNN, and GAM. After examining the results, explain which model you would choose and why. NOTE: There are no typos here! All models had the same CV MAE. Code is provided in the online manual if you’re curious.\n\n\n\nmethod\ntype\nassumption about \\(f(x_1,x_2,...,x_{6})\\)\nCV MAE\n\n\n\n\nleast squares\nparametric\n\\(\\beta_0 + \\beta_1x_1 + \\cdots + \\beta_{6} x_{6}\\)\n10.2\n\n\nKNN w/ \\(K = 33\\)\nnonparametric\naverage \\(y\\) among neighbors\n10.2\n\n\nGAM w/ penalty = 1.5\nnonparametric\n\\(\\beta_0 + f_1(x_1) + \\cdots + f_{6}(x_{6})\\)\n10.2\n\n\n\n\n\n\nSolution\n\nYou can argue a couple of ways.\n\nIf you noted the non-linear relationships in the plots above: GAM is more informative than KNN and less wrong than least squares.\nIf you didn’t think above that GAM was warranted, you should pick least squares. It’s definitely simpler and easier to interpret.\n\nCode\n\n# GAM MAE\ngam_models %&gt;% \n  collect_metrics() %&gt;% \n  filter(adjust_deg_free == 1.5)\n\n# A tibble: 1 × 7\n  adjust_deg_free .metric .estimator  mean     n std_err .config              \n            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1             1.5 mae     standard    10.2    10   0.350 Preprocessor1_Model04\n\n# Run the LS\nset.seed(253)\nls_model &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\") %&gt;% \n  fit_resamples(\n    Grad.Rate ~ .,\n    resamples = vfold_cv(college_sub, v = 10), \n    metrics = metric_set(mae)\n  )\nls_model %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    10.2    10   0.365 Preprocessor1_Model1\n\n# Run the KNN\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(engine = \"kknn\") %&gt;% \n  set_args(neighbors = tune())\nvariable_recipe &lt;- recipe(Grad.Rate ~ ., data = college_sub) %&gt;% \n  step_nzv(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\nknn_workflow &lt;- workflow() %&gt;% \n  add_model(knn_spec) %&gt;% \n  add_recipe(variable_recipe)\nset.seed(253)\nknn_models &lt;- knn_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(neighbors(range = c(1, 200)), levels = 50),\n    resamples = vfold_cv(college_sub, v = 10),\n    metrics = metric_set(mae)\n  )\nbest_K &lt;- select_best(knn_models, metric = \"mae\")\nbest_K\n\n# A tibble: 1 × 2\n  neighbors .config              \n      &lt;int&gt; &lt;chr&gt;                \n1        33 Preprocessor1_Model09\n\nknn_models %&gt;% \n  collect_metrics() %&gt;% \n  filter(neighbors == best_K$neighbors)\n\n# A tibble: 1 × 7\n  neighbors .metric .estimator  mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        33 mae     standard    10.2    10   0.337 Preprocessor1_Model09",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#part-3-reflection",
    "href": "L09-loess-gams.html#part-3-reflection",
    "title": "9  LOESS & GAMs",
    "section": "Part 3: Reflection",
    "text": "Part 3: Reflection\nIn these final exercises, you’ll reflect upon some algorithms we’ve learned thus far. This might feel bumpy if you haven’t reviewed the material recently, so be kind to yourself!\nSome of these exercises are similar to those on Homework 3. Thus solutions are not provided. The following directions are important to deepening your learning and growth.\n\nTyping the questions into ChatGPT does not require any reflection: You may NOT use ChatGPT or other online resources for these questions or on Homework 3.\nLocating is not learning: Challenge yourself to write out concepts in your own words. Do not rely on definitions in the activities, videos, or elsewhere.\n\n\n\n\n\nInterpretability & flexibility\nWhen picking between different model building algorithms, there are several considerations, including flexibility and interpretability. Consider the following graphic from ISLR:\n\n\n\n\n\nConvince yourself that the placement of Subset Selection (e.g. backward stepwise), LASSO, Least Squares, and GAM make sense. (We’ll address the other algorithms later this semester.)\nWhere would you place KNN on this graphic?\n\n\n\nDifferences and similarities\nFor each pair of algorithms below: try to identify a key similarity, a key difference, and any scenario in which they’re “equivalent”.\n\nKNN vs LOESS\nLOESS vs GAM\nGAM vs least squares\nleast squares vs LASSO\n\n\n\n\nPros & cons (there’s a similar question on HW3)\nSummarize at least 1 pro and 1 con about each model building algorithm. You cannot use the same pro or con more than once!\n\nleast squares\nLASSO\nKNN\nGAM\n\n\n\n\nLingo (there’s a similar question on HW3)\nIn your own words, describe what each of these “everyday” words means in the context of ML algorithms:\n\ngreedy\nbiased\nparsimonious\n(computationally) expensive\ngoldilocks problem",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L09-loess-gams.html#whats-next",
    "href": "L09-loess-gams.html#whats-next",
    "title": "9  LOESS & GAMs",
    "section": "What’s Next?",
    "text": "What’s Next?\n\nOur next class:\n\nReview for Quiz 1\nYou will also get time to work on Group Assignment 1. If you must miss class, you’re expected to alert your group members and come up with a plan to contribute to the collaboration outside class.\n\nUpcoming due dates:\n\nHW1 revisions: due tomorrow\nHW3: due Thursday\n\nThis is important review for the quiz\nAs such, extension opportunities will be LIMITED. Please plan accordingly!\n\nQuiz 1: next Tuesday (during class)\nHW2 revisions: due next week",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>LOESS & GAMs</span>"
    ]
  },
  {
    "objectID": "L10-review.html",
    "href": "L10-review.html",
    "title": "10  Regression Review",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression Review</span>"
    ]
  },
  {
    "objectID": "L10-review.html#quiz-1-reminders",
    "href": "L10-review.html#quiz-1-reminders",
    "title": "10  Regression Review",
    "section": "Quiz 1 Reminders",
    "text": "Quiz 1 Reminders\nContent:\n\nRegression (Units 1–3)\nQuestions will range in style: multiple choice, fill in the blank, short response, matching, etc.\n\nPart 1:\n\non paper\nclosed people, closed notes\ndue by the end of class\nyou might be asked to interpret some R output, but I won’t ask you to produce any code\n\nPart 2:\n\non computers\nyou can chat with any current STAT 253 student, but nobody else (including preceptors)\nyou can DM or email me clarifying questions and if there is something that could benefit from broader clarification I’ll share my answer (on Slack) with the entire class\n\nnote: I do not check email/Slack ~ 5pm–7am\n\nyou can use any materials from this STAT 253 course (videos, course website, textbook, homework solutions, etc.), but you may not use any other resources (internet, chatGPT, etc.)\nthis is designed to finish during class, but you can hand it in any time within 24 hours of your class end time (eg 11:10am the next day for the 9:40am section)",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression Review</span>"
    ]
  },
  {
    "objectID": "L10-review.html#context",
    "href": "L10-review.html#context",
    "title": "10  Regression Review",
    "section": "Context",
    "text": "Context\n\n\n\nWhat have we covered so far?\nFor the Regression task:\n\nUnit 1: Model evaluation\nUnit 2: Building models / selecting predictors\nUnit 3: Building flexible (nonparametric, nonlinear) models\n\nGeneral concepts that translate to other ML tasks:\n\nOverfitting\nCross validation\nBias-variance tradeoff\nAlgorithms and tuning parameters\nPreprocessing steps\nParametric vs nonparametric models",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression Review</span>"
    ]
  },
  {
    "objectID": "L10-review.html#review-reflection",
    "href": "L10-review.html#review-reflection",
    "title": "10  Regression Review",
    "section": "Review & Reflection",
    "text": "Review & Reflection\nSTAT 253 is a survey course of statistical machine learning techniques and concepts. It’s important to continuously reflect on these and how they fit together.  The material for class today is designed to help you reflect upon:\n\nML concepts\n\nenduring, big picture concepts\ntechnical concepts\ntidymodels code\n\nYour progress toward…\n\nengagement\ncollaboration\npreparation (checkpoints)\nexploration (homework)\n\n\nFind and make a copy of the following 2 resources. You’ll be given some relevant prompts below, but you should use these materials in whatever way suits you! Take notes, add more content, rearrange, etc.\n\nSTAT 253 concept maps\ntidymodels code comparison",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression Review</span>"
    ]
  },
  {
    "objectID": "L10-review.html#part-1-group-assignment-1",
    "href": "L10-review.html#part-1-group-assignment-1",
    "title": "10  Regression Review",
    "section": "Part 1: Group Assignment 1",
    "text": "Part 1: Group Assignment 1\nPlease fill out this Group Agreement Activity with your group. You will “submit” this activity by adding Kelsey as an editor on your google doc.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression Review</span>"
    ]
  },
  {
    "objectID": "L10-review.html#part-2-preparing-for-quiz-1",
    "href": "L10-review.html#part-2-preparing-for-quiz-1",
    "title": "10  Regression Review",
    "section": "Part 2: Preparing for Quiz 1",
    "text": "Part 2: Preparing for Quiz 1\n\nConcept Maps\nMark up slides 1–4 of the concept map with respect to the prompts below. Much of this overlaps with HW3.\n\n\n\nEnduring, big picture concepts\nIMPORTANT to your learning: Respond in your own words.\n\nWhen do we perform a supervised vs unsupervised learning algorithm?\nWithin supervised learning, when do we use a regression vs a classification algorithm?\nWhat is the importance of “model evaluation” and what questions does it address?\nWhat is “overfitting” and why is it bad?\nWhat is “cross-validation” and what problem is it trying to address?\nWhat is the “bias-variance tradeoff”?\n\n\n\n\nTechnical concepts\nOn page 2, identify some general themes for each model algorithm listed in the lefthand table:\n\nWhat’s the goal?\nIs the algorithm parametric or nonparametric?\nDoes the algorithm have any tuning parameters? What are they, how do we tune them, and how is this a goldilocks problem?\nWhat are the key pros & cons of the algorithm?\n\nFor each algorithm, you should also reflect upon these important technical concepts:\n\nCan you summarize the steps of this algorithm?\nIs the algorithm parametric or nonparametric? (addressed above)\nWhat is the bias-variance tradeoff when working with or tuning this algorithm?\nIs it important to scale / pre-process our predictors before feeding them into this algorithm?\nIs this algorithm “computationally expensive”?\nCan you interpret the technical (RStudio) output for this algorithm? (eg: CV plots, etc)?\n\n\n\n\nModel evaluation\nOn page 2, do the following for each model evaluation question in the righthand table:\n\nIdentify what to check or measure in order to address the question, and how to interpret it.\nExplain the steps of the CV algorithm.\n\n\n\n\nAlgorithm comparisons\n\nUse page 3 to make other observations about the Unit 1-3 modeling algorithms and their connections.\nUse page 4 to address and compare the interpretability & flexibility of the Subset Selection (e.g. backward stepwise), LASSO, Least Squares, and GAM algorithms. Where would you place KNN on this graphic?\n\n\n\n\n\n\nTidymodels Code Comparison\nCheck out and reflect upon some tidymodels code comparisons here. Copy, use, tweak, and add to this in whatever way suits you!",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression Review</span>"
    ]
  },
  {
    "objectID": "L10-review.html#part-3-midterm-learning-reflection",
    "href": "L10-review.html#part-3-midterm-learning-reflection",
    "title": "10  Regression Review",
    "section": "Part 3: Midterm Learning Reflection",
    "text": "Part 3: Midterm Learning Reflection\nThe reflections above address your understanding of key machine learning concepts. At this point in the semester, I’d also like you to take some time to reflect on your engagement with the course and your progress toward the “general skills” learning goals (e.g., collaboration) outlined in the Course Syllabus and on the learning goals page on this website.\nTo this end, please fill out this Google Form sometime in the next week.",
    "crumbs": [
      "Regression: Flexible Models (Unit 3)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression Review</span>"
    ]
  },
  {
    "objectID": "U04-motivation.html",
    "href": "U04-motivation.html",
    "title": "Motivating Question",
    "section": "",
    "text": "Where are we?\nWithin the supervised learning framework, we have a categorical response variable \\(y\\) and a set of potential predictors \\(x\\). For example:\n\ny = vote / don’t vote, x = (age, party id, …)\n\ny = spam / not spam, x = (# of $, # of !, …)\n\ny = human / car / plant, x = (speed, shape, …)\n\n\n\nWe have the following goals:\n\nBuild a classification model\nWe’ll use the following techniques to build classification models of \\(y\\) from predictors \\(x\\):\n\nparametric techniques\n\nlogistic regression (with or without LASSO!)\n\nsupport vector machines (optional)\n\nnonparametric techniques\n\nK Nearest Neighbors (KNN)\nclassification trees\n\nrandom forests and bagging\n\n\n\n\n\n\nEvaluate the quality of a classification model\nWe’ll use the following metrics and tools to evaluate the quality of a classification model:\n\noverall accuracy, sensitivity, & specificity\nWe can approximate these metrics using in-sample and cross validation techniques.\n\nROC (receiver operating characteristic) curves",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "Motivating Question"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html",
    "href": "L11-logistic-regression.html",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#where-are-we",
    "href": "L11-logistic-regression.html#where-are-we",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Where are we?",
    "text": "Where are we?\n\n\n\nCONTEXT\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = CLASSIFICATION\n\\(y\\) is categorical and binary\n(parametric) algorithm\nlogistic regression\n\nGOAL\nUse the parametric logistic regression model to model and classify \\(y\\).",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#logistic-regression-review",
    "href": "L11-logistic-regression.html#logistic-regression-review",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Logistic Regression Review",
    "text": "Logistic Regression Review\n\nLogistic Regression\nLet \\(y\\) be a binary categorical response variable:\n\\[y =\n\\begin{cases}\n1 & \\; \\text{ if event happens} \\\\\n0 & \\; \\text{ if event doesn't happen} \\\\\n\\end{cases}\\]\nFurther define \\[\\begin{split}\np &= \\text{ probability event happens} \\\\\n1-p &= \\text{ probability event doesn't happen} \\\\\n\\text{odds} & = \\text{ odds event happens} = \\frac{p}{1-p} \\\\\n\\end{split}\\]\nThen a logistic regression model of \\(y\\) by \\(x\\) is \\[\\begin{split}\n\\log(\\text{odds}) & = \\beta_0 + \\beta_1 x \\\\\n\\text{odds} & = e^{\\beta_0 + \\beta_1 x} \\\\\np           & = \\frac{\\text{odds}}{\\text{odds}+1} = \\frac{e^{\\beta_0 + \\beta_1 x}}{e^{\\beta_0 + \\beta_1 x}+1} \\\\\n\\end{split}\\]\nCoefficient interpretation\n\\[\\begin{split}    \n\\beta_0 & = \\text{ LOG(ODDS) when } x=0 \\\\\ne^{\\beta_0} & = \\text{ ODDS when } x=0 \\\\\n\\beta_1 & = \\text{ unit change in LOG(ODDS) per 1 unit increase in } x \\\\\ne^{\\beta_1} & = \\text{ multiplicative change in ODDS per 1 unit increase in } x \\\\\n\\end{split}\\]",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#example-1-check-out-the-data",
    "href": "L11-logistic-regression.html#example-1-check-out-the-data",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Example 1: Check out the data",
    "text": "Example 1: Check out the data\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n# Load weather data from rattle package\nlibrary(rattle)\ndata(\"weatherAUS\")\n\n# Wrangle data\nsydney &lt;- weatherAUS %&gt;% \n  filter(Location == \"Sydney\") %&gt;% \n  select(RainTomorrow, Humidity9am, Sunshine)\n\n# Check it out\nhead(sydney)\n\n\n# A tibble: 6 × 3\n  RainTomorrow Humidity9am Sunshine\n  &lt;fct&gt;              &lt;int&gt;    &lt;dbl&gt;\n1 Yes                   92      0  \n2 Yes                   83      2.7\n3 Yes                   88      0.1\n4 Yes                   83      0  \n5 Yes                   88      0  \n6 Yes                   69      8.6\n\n\nLet’s model RainTomorrow, whether or not it rains tomorrow in Sydney, by two predictors:\n\nHumidity9am (% humidity at 9am today)\nSunshine (number of hours of bright sunshine today)\n\nCheck out & comment on the relationship of rain with these 2 predictors:\n\n# Store so we can modify later\nrain_plot &lt;- ggplot(sydney, aes(y = Humidity9am, x = Sunshine, color = RainTomorrow)) + \n  geom_point(alpha = 0.5)\n\n\n\nSolution:\n\n\nrain_plot\n\n\n\n\n\n\n\n\nRainy days tend to be preceded by high humidity and low sunshine.",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#example-2-interpreting-coefficients",
    "href": "L11-logistic-regression.html#example-2-interpreting-coefficients",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Example 2: Interpreting coefficients",
    "text": "Example 2: Interpreting coefficients\n\n\n\n\nThe logistic regression model is:\n\nlog(odds of rain) = -1.01 + 0.0260 Humidity9am - 0.313 Sunshine\nodds of rain = exp(-1.01 + 0.0260 Humidity9am - 0.313 Sunshine)\nprobability of rain = odds / (odds + 1)\n\nLet’s interpret the Sunshine coefficient of -0.313:\n\n# Not transformed\n-0.313\n\n[1] -0.313\n\n# Transformed\nexp(-0.313)\n\n[1] 0.7312499\n\n\n\nWhen controlling for humidity, and for every extra hour of sunshine, the LOG(ODDS) of rain…\n\ndecrease by 0.313\nare roughly 31% as big (i.e. decrease by 69%)\nare roughly 73% as big (i.e. decrease by 27%)\nincrease by 0.731\n\nWhen controlling for humidity, and for every extra hour of sunshine, the ODDS of rain…\n\ndecrease by 0.313\nare roughly 31% as big (i.e. decrease by 69%)\nare roughly 73% as big (i.e. decrease by 27%)\nincrease by 0.731\n\n\n\n\nSolution:\n\n\ndecrease by 0.313\nare roughly 73% as big (i.e. decrease by 27%)",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#definition-odds-ratio",
    "href": "L11-logistic-regression.html#definition-odds-ratio",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Definition: odds ratio",
    "text": "Definition: odds ratio\nThe (slope) coefficients on the odds scale are odds ratios (OR):\n\\[e^{\\beta_1} = \\frac{\\text{odds of event at x + 1}}{\\text{odds of event at x}}\\]",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#example-3-classifications",
    "href": "L11-logistic-regression.html#example-3-classifications",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Example 3: Classifications",
    "text": "Example 3: Classifications\nlog(odds of rain) = -1.01 + 0.0260 Humidity9am - 0.313 Sunshine\nSuppose there’s 99% humidity at 9am today and only 2 hours of bright sunshine.\n\nWhat’s the probability of rain?\n\n\n# log(odds of rain)\nlog_odds &lt;- -1.01 + 0.0260 * 99 - 0.313 * 2\nlog_odds\n\n[1] 0.938\n\n# odds of rain\n\n\n# probability of rain\n\n\nWhat’s your binary classification: do you predict that it will rain or not rain?\n\n\n\nSolution:\n\n\n.\n\n\n# log(odds of rain)\n-1.01 + 0.0260 * 99 - 0.313 * 2\n\n[1] 0.938\n\n# odds of rain (MODIFY THIS)\nexp(-1.01 + 0.0260 * 99 - 0.313 * 2)\n\n[1] 2.554867\n\n# probability of rain\n2.554867 / (1 + 2.554867)\n\n[1] 0.7186955\n\n\n\nrain",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#example-4-classification-rules-intuition",
    "href": "L11-logistic-regression.html#example-4-classification-rules-intuition",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Example 4: Classification rules (intuition)",
    "text": "Example 4: Classification rules (intuition)\nWe used a simple classification rule above with a probability threshold of c = 0.5:\n\nIf the probability of rain &gt;= c, then predict rain.\nOtherwise, predict no rain.\n\nLet’s translate this into a classification rule that partitions the data points into rain / no rain predictions based on the predictor values.\nWhat do you think this classification rule / partition will look like?\n\n\nCode\nrain_plot\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\nwill vary",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#example-5-building-the-classification-rule",
    "href": "L11-logistic-regression.html#example-5-building-the-classification-rule",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Example 5: Building the classification rule",
    "text": "Example 5: Building the classification rule\n\nIf …, then predict rain.\nOtherwise, predict no rain.\n\nWork\nIdentify the pairs of humidity and sunshine values for which the probability of rain is 0.5, hence the log(odds of rain) is 0.\n\nYou should also convince yourself of this statement that if the probability of rain is 0.5, then the log(odds of rain) is 0.\n\n\n\nSolution:\n\n\nSet the log odds to 0:\nlog(odds of rain) = -1.01 + 0.0260 Humidity9am - 0.313 Sunshine = 0\nSolve for Humidity9am:\n\nMove constant and Sunshine term to other side.\n0.0260 Humidity9am = 1.01 + 0.3130 Sunshine\nDivide both sides by 0.026:\nHumidity9am = (1.01 / 0.026) + (0.3130 / 0.026) Sunshine = 38.846 + 12.038 Sunshine",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#example-6-examine-the-classification-rule",
    "href": "L11-logistic-regression.html#example-6-examine-the-classification-rule",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Example 6: Examine the classification rule",
    "text": "Example 6: Examine the classification rule\nLet’s visualize the partition, hence classification regions defined by our classification rule:\n\n\nCode\n# Example data points\nexample &lt;- data.frame(Humidity9am = c(90, 90, 60), Sunshine = c(2, 5, 10), RainTomorrow = c(NA, NA, NA))\n\n# Include the line Humidity9am = 38.84615 + 12.03846 Sunshine\nrain_plot +\n  geom_abline(intercept = 38.84615, slope = 12.03846, size = 2) + \n  geom_point(data = example, color = \"red\", size = 3)\n\n\n\n\n\n\n\n\n\nUse our classification rule to predict rain / no rain for the following days:\n\nDay 1: humidity = 90, sunshine = 2\nDay 2: humidity = 90, sunshine = 5\nDay 3: humidity = 60, sunshine = 10\n\n\n\nSolution:\n\n\nDay 1: rain\nDay 2: no rain\nDay 3: no rain",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#example-7-general-properties",
    "href": "L11-logistic-regression.html#example-7-general-properties",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Example 7: General properties",
    "text": "Example 7: General properties\n\nDoes the logistic regression algorithm have a tuning parameter?\nEstimating the logistic regression model requires the same pre-processing steps as least squares regression.\n\nIs it necessary to standardize quantitative predictors? If so, does the R function do this for us?\nIs it necessary to create dummy variables for our categorical predictors? If so, does the R function do this for us?\n\n\n\n\nSolution:\n\n\nno\n.\n\nno, R doesn’t standardize for logistic.\nyes and yes, R does this for us.",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#part-1-build-the-model",
    "href": "L11-logistic-regression.html#part-1-build-the-model",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Part 1: Build the model",
    "text": "Part 1: Build the model\nLet’s continue with our analysis of RainTomorrow vs Humidity9am and Sunshine. You’re given all code here. Be sure to scan and reflect upon what’s happening.\nSTEP 0: Organize the y categories\nWe want to model the log(odds of rain), thus the Yes category of RainTomorrow.\nBut R can’t read minds.\nWe have to explicitly tell it to treat the No category as the reference level (not the category we want to model).\n\nsydney &lt;- sydney %&gt;%\n  mutate(RainTomorrow = relevel(RainTomorrow, ref = \"No\"))\n\nSTEP 1: logistic regression model specification\nWhat’s new here?\n\nlogistic_spec &lt;- logistic_reg() %&gt;%\n  set_mode(\"classification\") %&gt;% \n  set_engine(\"glm\")\n\nSTEP 2: variable recipe\nThere are no required pre-processing steps, but you could add some. Nothing new here!\n\nvariable_recipe &lt;- recipe(RainTomorrow ~ Humidity9am + Sunshine, data = sydney)\n\nSTEP 3: workflow specification (recipe + model)\nNothing new here!\n\nlogistic_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;%\n  add_model(logistic_spec) \n\nSTEP 4: Estimate the model using the data\nSince the logistic regression model has no tuning parameter to tune, we can just fit() the model using our sample data – no need for tune_grid()!\n\nlogistic_model &lt;- logistic_workflow %&gt;% \n  fit(data = sydney)\n\nCheck out the tidy model summary\nNote that these coefficients are the same that we used in the above examples.\n\nlogistic_model %&gt;% \n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -1.01     0.258       -3.90 9.61e-  5\n2 Humidity9am   0.0260   0.00307      8.45 2.91e- 17\n3 Sunshine     -0.313    0.0119     -26.3  2.81e-152\n\n# Transform coefficients and confidence intervals to the odds scale\n# These are odds ratios (OR)\nlogistic_model %&gt;% \n  tidy() %&gt;%\n  mutate(\n    OR = exp(estimate),\n    OR.conf.low = exp(estimate - 1.96*std.error),\n    OR.conf.high = exp(estimate + 1.96*std.error)\n  )\n\n# A tibble: 3 × 8\n  term     estimate std.error statistic   p.value    OR OR.conf.low OR.conf.high\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 (Interc…  -1.01     0.258       -3.90 9.61e-  5 0.365       0.220        0.606\n2 Humidit…   0.0260   0.00307      8.45 2.91e- 17 1.03        1.02         1.03 \n3 Sunshine  -0.313    0.0119     -26.3  2.81e-152 0.731       0.714        0.748",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L11-logistic-regression.html#part-2-apply-evaluate-the-model",
    "href": "L11-logistic-regression.html#part-2-apply-evaluate-the-model",
    "title": "11  Logistic Regression: Model Building & Evaluation",
    "section": "Part 2: Apply & evaluate the model",
    "text": "Part 2: Apply & evaluate the model\n\nPredictions & classifications\nConsider the weather on 4 days in our data set:\n\n\nexamples &lt;- sydney[7:10,]\nexamples\n\n# A tibble: 4 × 3\n  RainTomorrow Humidity9am Sunshine\n  &lt;fct&gt;              &lt;int&gt;    &lt;dbl&gt;\n1 Yes                   75      5.2\n2 Yes                   77      2.1\n3 Yes                   92      3  \n4 No                    80     10.1\n\n\nUse the logistic_model to calculate the probability of rain and the rain prediction / classification for these 4 days.\n\nlogistic_model %&gt;% \n  augment(new_data = examples)\n\n# A tibble: 4 × 6\n  .pred_class .pred_No .pred_Yes RainTomorrow Humidity9am Sunshine\n  &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;              &lt;int&gt;    &lt;dbl&gt;\n1 No             0.665     0.335 Yes                   75      5.2\n2 Yes            0.417     0.583 Yes                   77      2.1\n3 Yes            0.391     0.609 Yes                   92      3  \n4 No             0.890     0.110 No                    80     10.1\n\n\n\nConvince yourself that you understand what’s being reported in the .pred_class, .pred_No, and .pred_Yes columns, as well as the correspondence between these columns (how they’re related to each other).\nHow many of the 4 classifications were accurate?\n\n\n\nSolution:\n\n\n.pred_class (classification based on probability 0.5 threshold), .pred_No (probability of no rain), and .pred_Yes (probability of rain)\n3\n\n\n\nConfusion matrix\nLet’s calculate the in_sample_classifications for all days in our sydney sample (“in-sample” because we’re evaluating our model using the same data we used to build it):\n\nin_sample_classifications &lt;- logistic_model %&gt;% \n  augment(new_data = sydney)\n    \n# Check it out\nhead(in_sample_classifications)\n\n# A tibble: 6 × 6\n  .pred_class .pred_No .pred_Yes RainTomorrow Humidity9am Sunshine\n  &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;              &lt;int&gt;    &lt;dbl&gt;\n1 Yes            0.201     0.799 Yes                   92      0  \n2 Yes            0.425     0.575 Yes                   83      2.7\n3 Yes            0.223     0.777 Yes                   88      0.1\n4 Yes            0.241     0.759 Yes                   83      0  \n5 Yes            0.218     0.782 Yes                   88      0  \n6 No             0.871     0.129 Yes                   69      8.6\n\n\nA confusion matrix summarizes the accuracy of the .pred_class model classifications. You’ll answer follow-up questions in the next exercises.\n\nin_sample_confusion &lt;- in_sample_classifications %&gt;% \n  conf_mat(truth = RainTomorrow, estimate = .pred_class)\n\n\n# Check it out in table form\nin_sample_confusion\n\n          Truth\nPrediction   No  Yes\n       No  3119  563\n       Yes  301  625\n\n\n\n# Check it out in plot form\nin_sample_confusion %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n# Check it out in a color plot (which we'll store and use later)\nmosaic_plot &lt;- in_sample_confusion %&gt;% \n  autoplot() +\n  aes(fill = rep(colnames(in_sample_confusion$table), ncol(in_sample_confusion$table))) + \n  theme(legend.position = \"none\")\n    \nmosaic_plot\n\n\n\n\n\n\n\n\n\nOverall accuracy\n\n\nin_sample_confusion\n\n          Truth\nPrediction   No  Yes\n       No  3119  563\n       Yes  301  625\n\n\n\nWhat do these numbers add up to, both numerically and contextually?\n\n\nUse this matrix to calculate the overall accuracy of the model classifications. That is, what proportion of the classifications were correct?\n\n\nCheck that your answer to part b matches the accuracy listed in the confusion matrix summary():\n\n\n# event_level indicates that the second RainTomorrow\n# category (Yes) is our category of interest\nsummary(in_sample_confusion, event_level = \"second\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.812\n 2 kap                  binary         0.472\n 3 sens                 binary         0.526\n 4 spec                 binary         0.912\n 5 ppv                  binary         0.675\n 6 npv                  binary         0.847\n 7 mcc                  binary         0.478\n 8 j_index              binary         0.438\n 9 bal_accuracy         binary         0.719\n10 detection_prevalence binary         0.201\n11 precision            binary         0.675\n12 recall               binary         0.526\n13 f_meas               binary         0.591\n\n\n\n\nSolution:\n\n\nsample size (not including data points with NA on variables used in our model)\n\n\n3119 + 563 + 301 + 625\n\n[1] 4608\n\n\n\n\n\n\n(3119 + 625) / (3119 + 563 + 301 + 625)\n\n[1] 0.8125\n\n\n\nyep\n\n\n\n\n\nNo information rate\nAre our model classifications any better than just randomly guessing rain / no rain?! What if we didn’t even build a model, and just always predicted the most common outcome of RainTomorrow: that it wouldn’t rain?!\n\n\nsydney %&gt;% \n  count(RainTomorrow)\n\n# A tibble: 3 × 2\n  RainTomorrow     n\n  &lt;fct&gt;        &lt;int&gt;\n1 No            3443\n2 Yes           1196\n3 &lt;NA&gt;            14\n\n\n\nIgnoring the NA outcomes, prove that if we just always predicted no rain, we’d be correct 74.2% of the time. This is called the no information rate.\n\n\nIs the overall accuracy of our logistic regression model (81.2%) meaningfully better than this random guessing approach?\n\n\n\nSolution:\n\n\nWe’re only right when it doesn’t rain.\n\n\n 3443 / (3443 + 1196)\n\n[1] 0.7421858\n\n\n\nthis is subjective – depends on context / how we’ll use the predictions / consequences for being wrong.\n\n\n\n\nSensitivity\nBeyond overall accuracy, we care about the accuracy within each class (rain and no rain). Our model’s true positive rate or sensitivity is the probability that it correctly classifies rain as rain. This is represented by the fraction of rain observations that are red:\n\n\n# NOTE: We're only plotting RAINY days\nin_sample_classifications %&gt;% \n  filter(RainTomorrow == \"Yes\") %&gt;% \n  mutate(correct = (RainTomorrow == .pred_class)) %&gt;% \n  ggplot(aes(y = Humidity9am, x = Sunshine, color = correct)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = 38.84615, slope = 12.03846, size = 2) + \n  scale_color_manual(values = c(\"black\", \"red\"))\n\n\n\n\n\n\n\n\nOr, the proportion of the Yes column that falls into the Yes prediction box:\n\nmosaic_plot\n\n\n\n\n\n\n\n\n\nVisually, does it appear that the sensitivity is low, moderate, or high?\nCalculate the sensitivity using the confusion matrix.\n\n\nin_sample_confusion\n\n          Truth\nPrediction   No  Yes\n       No  3119  563\n       Yes  301  625\n\n\n\nCheck that your answer to part b matches the sens listed in the confusion matrix summary():\neval=TRUE\n\n\nsummary(in_sample_confusion, event_level = \"second\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.812\n 2 kap                  binary         0.472\n 3 sens                 binary         0.526\n 4 spec                 binary         0.912\n 5 ppv                  binary         0.675\n 6 npv                  binary         0.847\n 7 mcc                  binary         0.478\n 8 j_index              binary         0.438\n 9 bal_accuracy         binary         0.719\n10 detection_prevalence binary         0.201\n11 precision            binary         0.675\n12 recall               binary         0.526\n13 f_meas               binary         0.591\n\n\n\nInterpret the sensitivity and comment on whether this is low, moderate, or high.\n\n\n\nSolution:\n\n\nmoderate (or low)\n.\n\n\n625 / (625 + 563)\n\n[1] 0.5260943\n\n\n\nyep\n\nWe correctly anticipate rain 52.6% of the time. Or, on 52.6% of rainy days, we correctly predict rain.\n\n\n\n\nSpecificity\nSimilarly, we can calculate the model’s true negative rate or specificity, i.e. the probability that it correctly classifies “no rain” as “no rain”. This is represented by the fraction of no rain observations that are red:\n\n\n# NOTE: We're only plotting NON-RAINY days\nin_sample_classifications %&gt;% \n  filter(RainTomorrow == \"No\") %&gt;% \n  mutate(correct = (RainTomorrow == .pred_class)) %&gt;% \n  ggplot(aes(y = Humidity9am, x = Sunshine, color = correct)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = 38.84615, slope = 12.03846, size = 2) + \n  scale_color_manual(values = c(\"black\", \"red\"))\n\n\n\n\n\n\n\n\nOr, the proportion of the No column that falls into the No prediction box:\n\nmosaic_plot\n\n\n\n\n\n\n\n\n\nVisually, does it appear that the specificity is low, moderate, or high?\nCalculate specificity using the confusion matrix.\n\n\nin_sample_confusion\n\n          Truth\nPrediction   No  Yes\n       No  3119  563\n       Yes  301  625\n\n\n\nCheck that your answer to part b matches the spec listed in the confusion matrix summary():\n\n\nsummary(in_sample_confusion, event_level = \"second\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.812\n 2 kap                  binary         0.472\n 3 sens                 binary         0.526\n 4 spec                 binary         0.912\n 5 ppv                  binary         0.675\n 6 npv                  binary         0.847\n 7 mcc                  binary         0.478\n 8 j_index              binary         0.438\n 9 bal_accuracy         binary         0.719\n10 detection_prevalence binary         0.201\n11 precision            binary         0.675\n12 recall               binary         0.526\n13 f_meas               binary         0.591\n\n\n\nInterpret the specificity and comment on whether this is low, moderate, or high.\n\n\n\nSolution:\n\n\nhigh\n.\n\n\n3119 / (3119 + 301)\n\n[1] 0.9119883\n\n\n\nyep\n\n\nsummary(in_sample_confusion, event_level = \"second\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.812\n 2 kap                  binary         0.472\n 3 sens                 binary         0.526\n 4 spec                 binary         0.912\n 5 ppv                  binary         0.675\n 6 npv                  binary         0.847\n 7 mcc                  binary         0.478\n 8 j_index              binary         0.438\n 9 bal_accuracy         binary         0.719\n10 detection_prevalence binary         0.201\n11 precision            binary         0.675\n12 recall               binary         0.526\n13 f_meas               binary         0.591\n\n\n\nOn 91.2% of non-rainy days, we correctly predict no rain.\n\n\n\n\nIn-sample vs CV Accuracy\nThe above in-sample metrics of overall accuracy (0.812), sensitivity (0.526), and specificity (0.912) helped us understand how well our model classifies rain / no rain for the same data points we used to build the model. Let’s calculate the cross-validated metrics to better understand how well our model might classify days in the future:\n\n\n# NOTE: This is very similar to the code for CV with least squares!\n# EXCEPT: We need the \"control\" argument to again specify our interest in the \"Yes\" category\nset.seed(253)\nlogistic_model_cv &lt;- logistic_spec %&gt;% \n  fit_resamples(\n    RainTomorrow ~ Humidity9am + Sunshine,\n    resamples = vfold_cv(sydney, v = 10), \n    control = control_resamples(save_pred = TRUE, event_level = 'second'),\n    metrics = metric_set(accuracy, sensitivity, specificity)\n  )\n\n# Check out the resulting CV metrics\nlogistic_model_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.812    10 0.00651 Preprocessor1_Model1\n2 sensitivity binary     0.527    10 0.0149  Preprocessor1_Model1\n3 specificity binary     0.911    10 0.00677 Preprocessor1_Model1\n\n\nHow similar are the in-sample and CV evaluation metrics? Based on these, do you think our model is overfit?\n\n\nSolution:\n\nThey’re similar, thus our model doesn’t seem overfit.\n\n\n\nSpecificity vs Sensitivity\n\nOur model does better at correctly predicting non-rainy days than rainy days (specificity &gt; sensitivity). Why do you think this is the case?\nIn the context of predicting rain, what would you prefer: high sensitivity or high specificity?\n\nChanging up the probability threshold we use in classifying days as rain / no rain gives us some control over sensitivity and specificity. Consider lowering the threshold from 0.5 to 0.05. Thus if there’s even a 5% chance of rain, we’ll predict rain! What’s your intuition:\n\nsensitivity will decrease and specificity will increase\nsensitivity will increase and specificity will decrease\nboth sensitivity and specificity will increase\n\n\n\n\n\nSolution:\n\n\nbecause non-rainy days are much more common\nwill vary. would you rather risk getting wet instead of carrying your umbrella, or carry your umbrella when it doesn’t rain?\nwill vary\n\n\n\n\nChange up the threshold\nLet’s try lowering the threshold to 0.05!\n\n\n# Calculate .pred_class using a 0.05 threshold\n# (this overwrites the default .pred_class which uses 0.5)\nnew_classifications &lt;- logistic_model %&gt;% \n  augment(new_data = sydney) %&gt;% \n  mutate(.pred_class = ifelse(.pred_Yes &gt;= 0.05, \"Yes\", \"No\")) %&gt;% \n  mutate(.pred_class = as.factor(.pred_class))\n\n\n# Obtain a new confusion matrix\nnew_confusion &lt;- new_classifications %&gt;% \n  conf_mat(truth = RainTomorrow, estimate = .pred_class)\nnew_confusion\n\n          Truth\nPrediction   No  Yes\n       No   622   23\n       Yes 2798 1165\n\n\n\n# Obtain new summaries    \nsummary(new_confusion, event_level = \"second\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.388 \n 2 kap                  binary        0.0922\n 3 sens                 binary        0.981 \n 4 spec                 binary        0.182 \n 5 ppv                  binary        0.294 \n 6 npv                  binary        0.964 \n 7 mcc                  binary        0.205 \n 8 j_index              binary        0.163 \n 9 bal_accuracy         binary        0.581 \n10 detection_prevalence binary        0.860 \n11 precision            binary        0.294 \n12 recall               binary        0.981 \n13 f_meas               binary        0.452 \n\n\n\nHow does the new sensitivity compare to that using the 0.5 threshold (0.526)?\nHow does the new specificity compare to that using the 0.5 threshold (0.912)?\nWas your intuition right? When we decrease the probability threshold…\n\n\nsensitivity decreases and specificity increases\nsensitivity increases and specificity decreases\nboth sensitivity and specificity increase\n\n\nWE get to pick an appropriate threshold for our analysis. Change up 0.05 in the code below to identify a threshold you like.\n\n\n# Calculate .pred_class using a 0.05 threshold\n# (this overwrites the defaulty .pred_class which uses 0.5)\nnew_classifications &lt;- logistic_model %&gt;% \n  augment(new_data = sydney) %&gt;% \n  mutate(.pred_class = ifelse(.pred_Yes &gt;= 0.05, \"Yes\", \"No\")) %&gt;% \n  mutate(.pred_class = as.factor(.pred_class))\n\n# Obtain a new confusion matrix\nnew_confusion &lt;- new_classifications %&gt;% \n  conf_mat(truth = RainTomorrow, estimate = .pred_class)\nnew_confusion\n\n          Truth\nPrediction   No  Yes\n       No   622   23\n       Yes 2798 1165\n\n# Obtain new summaries    \nsummary(new_confusion, event_level = \"second\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.388 \n 2 kap                  binary        0.0922\n 3 sens                 binary        0.981 \n 4 spec                 binary        0.182 \n 5 ppv                  binary        0.294 \n 6 npv                  binary        0.964 \n 7 mcc                  binary        0.205 \n 8 j_index              binary        0.163 \n 9 bal_accuracy         binary        0.581 \n10 detection_prevalence binary        0.860 \n11 precision            binary        0.294 \n12 recall               binary        0.981 \n13 f_meas               binary        0.452 \n\n\n\n\nSolution:\n\n\nmuch higher (0.981)\nmuch lower (0.182)\nsensitivity increases and specificity decreases\nwill vary\n\n\n\n\nOPTIONAL challenge\nIn Example 5, we built the following classification rule based on a 0.5 probability threshold:\n\n\nIf Humidity9am &gt; 38.84615 + 12.03846 Sunshine, then predict rain.\nOtherwise, predict no rain.\n\nAnd we plotted this rule:\n\nggplot(sydney, aes(y = Humidity9am, x = Sunshine, color = RainTomorrow)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = 38.84615, slope = 12.03846, size = 2) + \n  geom_point(data = example, color = \"black\", size = 3)\n\n\n\n\n\n\n\n\nChallenge: Modify this rule and the plot using a 0.05 probability threshold.\n\n\nSolution:\n\n\nIf Humidity9am &gt; -74.385 + 12.03846 Sunshine, then predict rain.\nOtherwise, predict no rain.\n\n\nggplot(sydney, aes(y = Humidity9am, x = Sunshine, color = RainTomorrow)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(intercept = -74.385, slope = 12.03846, size = 2) + \n  geom_point(data = example, color = \"black\", size = 3)\n\n\n\n\n\n\n\n\nWork\n\nprob &lt;- 0.05\nodds &lt;- prob / (1 - prob)\nlog(odds)\n\n[1] -2.944439\n\n\nSet log(odds) to -2.944:\nlog(odds of rain) = -1.01 + 0.0260 Humidity9am - 0.313 Sunshine = -2.944\n0.0260 Humidity9am = -2.944 + 1.01 + 0.3130 Sunshine = -1.934 + 0.3130 Sunshine\nHumidity9am = (-1.934/0.0260) + (0.3130/0.0260) Sunshine = -74.385 + 12.038 Sunshine\n\n\n\nOPTIONAL math\nFor a general logistic regression model\n\n\\[log(\\text{odds}) = \\beta_0 + \\beta_1 x\\]\n\\(\\beta_1\\) is the change in log(odds) when we increase \\(x\\) by 1:\n\\[\\beta_1 = log(\\text{odds at x + 1}) - log(\\text{odds at x})\\]\nProve \\(e^{\\beta_1}\\) is the multiplicative change in odds when we increase \\(x\\) by 1.\n\n\nSolution:\n\n\\[\\begin{split}\n    \\beta_1 & = log(\\text{odds at x + 1}) - log(\\text{odds at x}) \\\\\n    & = log\\left(\\frac{\\text{odds at x + 1}}{\\text{odds at x}} \\right) \\\\\n    e^{\\beta_1} & = e^{log\\left(\\frac{\\text{odds at x + 1}}{\\text{odds at x}} \\right)}\\\\\n    & = \\frac{\\text{odds at x + 1}}{\\text{odds at x}} \\\\\n    \\end{split}\\]",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression: Model Building & Evaluation</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html",
    "href": "L12-evaluating-classification-models.html",
    "title": "12  Evaluating Classification Models",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html#where-are-we",
    "href": "L12-evaluating-classification-models.html#where-are-we",
    "title": "12  Evaluating Classification Models",
    "section": "Where are we?",
    "text": "Where are we?\n\n\n\nCONTEXT\n\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = CLASSIFICATION\n\\(y\\) is categorical and binary\n(parametric) algorithm\nlogistic regression\napplication = classification\n\nUse our algorithm to calculate the probability that y = 1.\nTurn these into binary classifications using a classification rule. For some probability threshold c:\n\nIf the probability that y = 1 is at least c, classify y as 1.\nOtherwise, classify y as 0.\n\nWE get to pick c. This should be guided by context (what are the consequences of misclassification?) and the quality of the resulting classifications.\n\n\n\n\n\n\nGOAL\n\nEvaluate the quality of binary classifications of \\(y\\) (here resulting logistic regression model).",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html#sensitivity-specificity",
    "href": "L12-evaluating-classification-models.html#sensitivity-specificity",
    "title": "12  Evaluating Classification Models",
    "section": "Sensitivity & specificity",
    "text": "Sensitivity & specificity\n\\[\\begin{split}\n\\text{overall accuracy} & = \\text{probability of making a correct classification} \\\\\n\\text{sensitivity} & = \\text{true positive rate}\\\\\n&  = \\text{probability of correctly classifying $y=1$ as $y=1$} \\\\\n\\text{specificity} & = \\text{true negative rate} \\\\\n& =  \\text{probability of correctly classifying $y=0$ as $y=0$} \\\\\n\\text{1 - specificity} & = \\text{false positive rate} \\\\\n&  = \\text{probability of classifying $y=0$ as $y=1$} \\\\\n\\end{split}\\]\n. . .\n\n\nIn-sample estimation (how well our model classifies the same data points we used to build it)\n\n\n\n\ny = 0\ny = 1\n\n\n\n\nclassify as 0\na\nb\n\n\nclassify as 1\nc\nd\n\n\n\n\\[\\begin{split}\n\\text{overall accuracy} & = \\frac{a + d}{a + b + c + d}\\\\\n\\text{sensitivity} & = \\frac{d}{b + d} \\\\\n\\text{specificity} & = \\frac{a}{a + c} \\\\\n\\end{split}\\]\n. . .\n\n\nk-Fold Cross-Validation (how well our model classifies NEW data points)\n\nDivide the data into k folds of approximately equal size.\n\nRepeat the following procedures for each fold j = 1, 2, …, k:\n\nRemove fold j from the data set.\n\nFit a model using the other k - 1 folds.\n\nUse this model to classify the outcomes for the data points in fold j.\n\nCalculate the overall accuracy, sensitivity, and specificity of these classifications.\n\n\nAverage the metrics across the k folds. This gives our CV estimates of overall accuracy, sensitivity, and specificity.",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html#roc-curves",
    "href": "L12-evaluating-classification-models.html#roc-curves",
    "title": "12  Evaluating Classification Models",
    "section": "ROC curves",
    "text": "ROC curves\nROC: Receiver Operating Characteristic curves\n. . .\nSensitivity and specificity depend upon the specific probability threshold c.\n\nIf we lower c, hence make it easier to classify y as 1:\n\nsensitivity increases but specificity decreases\n\nIf we increase c, hence make it tougher to classify y as 1:\n\nspecificity increases but sensitivity decreases\n\n\n. . .\nTo understand this trade-off, for a range of possible thresholds c between 0 and 1, ROC curves calculate and plot\n\ny-axis: sensitivity (true positive rate)\nx-axis: 1 - specificity (false positive rate)\n\nSince specificity is the probability of classifying y = 0 as 0, 1 - specificity is the probability of misclassifying y = 0 as 1.\n\n\n. . .\nWhy we care:\n\nAlong with context, ROC curves can help us identify an appropriate probability threshold c.\nROC curves can help us compare the quality of different models.\nThe area under an ROC curve (AUC) estimates the probability that our algorithm is more likely to classify y = 1 as 1 than to classify y = 0 as 1, hence distinguish between the 2 classes.\n\nPut another way: if we give our model 2 data points, one with y = 0 and the other with y = 1, AUC is the probability that we correctly identify which is which.",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html#example-1",
    "href": "L12-evaluating-classification-models.html#example-1",
    "title": "12  Evaluating Classification Models",
    "section": "Example 1",
    "text": "Example 1\nSuppose we model RainTomorrow in Sydney using only the number of hours of bright Sunshine today.\nUsing a probability threshold of 0.5, this model produces the following classification rule:\n\nIf Sunshine &lt; 3.125, predict rain.\nOtherwise, predict no rain.\n\n\nInterpret these in-sample estimates of the resulting classification quality.\n\nOverall accuracy = 0.803\n\nWe correctly predict the rain outcome (yes or no) 80.3% of the time.\nWe correctly predict “no rain” on 80.3% of non-rainy days.\nWe correctly predict “rain” on 80.3% of rainy days.\n\nSensitivity = 0.506\n\nWe correctly predict the rain outcome (yes or no) 50.6% of the time.\nWe correctly predict “no rain” on 50.6% of non-rainy days.\nWe correctly predict “rain” on 50.6% of rainy days.\n\nSpecificity = 0.906\n\nWe correctly predict the rain outcome (yes or no) 90.6% of the time.\nWe correctly predict “no rain” on 90.6% of non-rainy days.\nWe correctly predict “rain” on 90.6% of rainy days.\n\n\n\n\nSolution:\n\n\nOverall accuracy = 0.803; We correctly predict the rain outcome (yes or no) 80.3% of the time.\nSensitivity = 0.506; We correctly predict “rain” on 50.6% of rainy days.\nSpecificity = 0.906; We correctly predict “no rain” on 90.6% of non-rainy days.",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html#optional-extra-practice",
    "href": "L12-evaluating-classification-models.html#optional-extra-practice",
    "title": "12  Evaluating Classification Models",
    "section": "(OPTIONAL) Extra Practice",
    "text": "(OPTIONAL) Extra Practice\nConfirm the 3 metrics above using the confusion matrix. Work is shown below (peek when you’re ready).\n            Truth\nPrediction   No  Yes\n       No  3108  588\n       Yes  324  602\n\n\nSolution:\n\n\n# Overall accuracy\n(3108 + 602) / (3108 + 602 + 324 + 588)\n\n[1] 0.8026828\n\n# Sensitivity\n602 / (602 + 588)\n\n[1] 0.5058824\n\n# Specificity\n3108 / (3108 + 324)\n\n[1] 0.9055944",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html#example-2-roc-curves",
    "href": "L12-evaluating-classification-models.html#example-2-roc-curves",
    "title": "12  Evaluating Classification Models",
    "section": "Example 2: ROC curves",
    "text": "Example 2: ROC curves\nWe can change up the probability threshold in our classification rule!\nThe ROC curve for our logistic regression model of RainTomorrow by Sunshine plots the sensitivity (true positive rate) vs 1 - specificity (false positive rate) corresponding to “every” possible threshold:\n\n\nWhich point represents the quality of our classification rule using a 0.5 probability threshold?\nThe other point corresponds to a different classification rule which uses a different threshold. Is that threshold smaller or bigger than 0.5?\nWhich classification rule do you prefer?\n\n\n\nSolution:\n\n\nRed point: 0.5 probability rule (sensitivity ~ 0.5, specificity ~ 0.9)\nBlack point (higher sensitivity, lower specificity) has a threshold that is lower than 0.5.\nAnswers will vary. If you don’t like getting wet (accurately predict rain when it does rain), you’ll want a higher sensitivity. If you don’t like carrying an umbrella when it isn’t needed, you’ll want a higher specificity (lower false positive rate).",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html#example-3-area-under-the-roc-curve-auc",
    "href": "L12-evaluating-classification-models.html#example-3-area-under-the-roc-curve-auc",
    "title": "12  Evaluating Classification Models",
    "section": "Example 3: Area Under the ROC Curve (AUC)",
    "text": "Example 3: Area Under the ROC Curve (AUC)\nThe area under an ROC curve (AUC) estimates the probability that our algorithm is more likely to classify y = 1 (rain) as 1 (rain) than to classify y = 0 (no rain) as 1 (rain), hence distinguish between the 2 classes.\nAUC is helpful for evaluating and comparing the overall quality of classification models. Consider 3 different possible predictors (A, B, C) of rainy and non-rainy days:\n\nWhich predictor is the “strongest” predictor of rain tomorrow?\n. . .\nThe ROC curves corresponding to the models RainTomorrow ~ A, RainTomorrow ~ B, RainTomorrow ~ C are shown below.\n\nFor each ROC curve, indicate the corresponding model and the approximate AUC. Do this in any order you want!\nblack ROC curve\n\nRainTomorrow ~ ___\nAUC is roughly ___\n\ngreen ROC curve\n\nRainTomorrow ~ ___\nAUC is roughly ___.\n\norange ROC curve\n\nRainTomorrow ~ ___\nAUC is exactly ___.\n\n\n\nSolution:\n\nblack ROC curve\n\nRainTomorrow ~ A\nAUC is roughly 0.5\n\ngreen ROC curve\n\nRainTomorrow ~ C\nAUC is roughly .95.\n\norange ROC curve\n\nRainTomorrow ~ B\nAUC is exactly 1.",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L12-evaluating-classification-models.html#auc-observation",
    "href": "L12-evaluating-classification-models.html#auc-observation",
    "title": "12  Evaluating Classification Models",
    "section": "AUC Observation",
    "text": "AUC Observation\nIn general:\n\nA perfect classification model has an AUC of 1.\nThe simplest classification model that just randomly predicts y to be 1 or 0 (eg: by flipping a coin), has an AUC of 0.5. It is represented by the diagonal “no discrimination line” in an ROC plot.\nA model with an AUC below 0.5 is worse than just random. Flipping a coin would be better.",
    "crumbs": [
      "Classification: Modeling Building (Unit 4)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Evaluating Classification Models</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html",
    "href": "L13-knn-trees.html",
    "title": "13  KNN and Trees",
    "section": "",
    "text": "Settling In",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#where-are-we",
    "href": "L13-knn-trees.html#where-are-we",
    "title": "13  KNN and Trees",
    "section": "Where are we?",
    "text": "Where are we?\n\nCONTEXT\n\nworld = supervised learning\nWe want to model some output variable \\(y\\) using a set of potential predictors (\\(x_1, x_2, ..., x_p\\)).\ntask = CLASSIFICATION\n\\(y\\) is categorical\nalgorithm = NONparametric\n\n. . .\n\n\nGOAL\nJust as least squares and LASSO in the regression setting, the parametric logistic regression model makes very specific assumptions about the relationship of a binary categorical outcome \\(y\\) with predictors \\(x\\).\nSpecifically, it assumes this relationship can be written as a specific formula with parameters \\(\\beta\\):\n\\[\\text{log(odds that y is 1)} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\\]\nNONparametric algorithms will be necessary when this model is too rigid to capture more complicated relationships.",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#motivating-example",
    "href": "L13-knn-trees.html#motivating-example",
    "title": "13  KNN and Trees",
    "section": "Motivating Example",
    "text": "Motivating Example\nAerial photography studies of land cover is important to land conservation, land management, and understanding environmental impact of land use.\nIMPORTANT: Other aerial photography studies focused on people and movement can be used for surveillance, raising major ethical questions.\n\n\n\nImage Source: https://ncap.org.uk/sites/default/files/EK_land_use_0.jpg\n\n\nLet’s load data on a sample of aerial images from the UCI Machine Learning Repository:\n\n\nCode\n# Load packages\nlibrary(tidymodels)\nlibrary(tidyverse)\n\n# Load & process data\n# There are 9 types of land use. For now, we'll only consider 3 types.\n# There are 147 possible predictors of land use. For now, we'll only consider 4 predictors.\nland_3 &lt;- read.csv(\"https://kegrinde.github.io/stat253_coursenotes/data/land_cover.csv\") %&gt;% \n  rename(type = class) %&gt;% \n  filter(type %in% c(\"asphalt \",\"grass \",\"tree \")) %&gt;% \n  mutate(type = as.factor(type)) %&gt;% \n  select(type, NDVI, Mean_G, Bright_100, SD_NIR)\n\n\n\n# Check it out\nhead(land_3)\n\n      type  NDVI Mean_G Bright_100 SD_NIR\n1    tree   0.31 240.18     161.92  11.50\n2    tree   0.39 184.15     117.76  11.30\n3 asphalt  -0.13  68.07      86.41   6.12\n4   grass   0.19 213.71     175.82  11.10\n5    tree   0.35 201.84     125.71  14.31\n6    tree   0.23 200.16     124.30  12.97\n\n\n\n# Table of land types\nland_3 %&gt;% \n  count(type)\n\n      type   n\n1 asphalt   59\n2   grass  112\n3    tree  106\n\n\n\nThus we have the following variables:\n\ntype = observed type of land cover, hand-labelled by a human (asphalt, grass, or tree)\nfactors computed from the image\nThough the data includes measurements of size and shape, we’ll focus on texture and “spectral” measurements, i.e. how the land interacts with sun radiation.\n\nNDVI = vegetation index\nMean_G = the green-ness of the image\nBright_100 = the brightness of the image\nSD_NIR = a texture measurement (calculated by the standard deviation of “near infrared”)",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#limits-of-logistic-regression",
    "href": "L13-knn-trees.html#limits-of-logistic-regression",
    "title": "13  KNN and Trees",
    "section": "Limits of Logistic Regression",
    "text": "Limits of Logistic Regression\nWhy can’t we use logistic regression to model land type (y) by the possible predictors NDVI, Mean_G, Bright_100, SD_NIR (x)?",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#parametric-v.-nonparametric-classification",
    "href": "L13-knn-trees.html#parametric-v.-nonparametric-classification",
    "title": "13  KNN and Trees",
    "section": "Parametric v. Nonparametric Classification",
    "text": "Parametric v. Nonparametric Classification\nThere are parametric classifications algorithms that can model y outcomes with more than 2 categories. But they’re complicated and not very common.\nWe’ll consider two nonparametric algorithms today: K Nearest Neighbors & classification trees.",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#nonparametric-classification",
    "href": "L13-knn-trees.html#nonparametric-classification",
    "title": "13  KNN and Trees",
    "section": "Nonparametric Classification",
    "text": "Nonparametric Classification\n\nPro: flexibility\n\nAssume there is a relationship between y and x, but don’t make any assumptions about the “shape” of this relationship.\nThis is good when our relationships are complicated!\n\nCons:\n\nlack of insights: Can be useful for classification, but provide fewer insights into the relationships we’re modeling (eg: no coefficients, p-values, etc).\nignoring information about relationships: When the assumptions of a parametric model are appropriate, i.e. when the shape of a relationship is “known”, nonparametric algorithms will typically provide worse classifications by not utilizing that shape. (When the shape of a relationship is “known”, we should use that shape.)\nmore computationally intense",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-1-check-out-the-data",
    "href": "L13-knn-trees.html#exercise-1-check-out-the-data",
    "title": "13  KNN and Trees",
    "section": "Exercise 1: Check out the data",
    "text": "Exercise 1: Check out the data\nWe’ll start by classifying land type using vegetation index (NDVI) and green-ness (Mean_G).\nFirst, plot and describe this relationship.\n\n# Store the plot because we'll use it later\nveg_green_plot &lt;- ggplot(land_3, aes(x = Mean_G, y = NDVI, color = type)) + \n  geom_point() +\n  scale_color_manual(values = c(\"asphalt \" = \"black\",\n                            \"grass \" = \"#E69F00\",\n                            \"tree \" = \"#56B4E9\")) +\n  theme_minimal()\nveg_green_plot\n\n\n\n\n\n\n\n\n\n\n\nSolution:\n\nAsphalt images tend to have low vegetation index and greenness.\nGrass and tree images both tend to have higher vegetation index and greenness than asphalt, and tree images tend to have the highest vegetation index.",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-2-intuition",
    "href": "L13-knn-trees.html#exercise-2-intuition",
    "title": "13  KNN and Trees",
    "section": "Exercise 2: Intuition",
    "text": "Exercise 2: Intuition\nThe red dot below represents a new image with NDVI = 0.335 and Mean_G = 110:\n\nveg_green_plot + \n  geom_point(aes(y = 0.335, x = 110), color = \"red\")\n\n\n\n\n\n\n\n\nHow would you classify this image (asphalt, grass, or tree) using…\n\nthe 1 nearest neighbor\nthe 3 nearest neighbors\nall 277 neighbors in the sample\n\n\n\nSolution:\n\n\ngrass\ntree\ngrass (we learned above that grass images were the most common in our sample)",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-3-details",
    "href": "L13-knn-trees.html#exercise-3-details",
    "title": "13  KNN and Trees",
    "section": "Exercise 3: Details",
    "text": "Exercise 3: Details\nJust as with KNN in the regression setting, it will be important to standardize quantitative x predictors before using them in the algorithm. Why?\n\n\nSolution:\n\nSo that the different scales don’t skew how we identify our nearest neighbors.",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-4-check-your-work",
    "href": "L13-knn-trees.html#exercise-4-check-your-work",
    "title": "13  KNN and Trees",
    "section": "Exercise 4: Check your work",
    "text": "Exercise 4: Check your work\nIn exercise two, your answers should be grass (a), tree (b), and grass (c).  Let’s confirm these results by doing this “by hand” …\nTry writing R code to find the closest neighbors (without tidymodels).\n\n\nSolution:\n\nPreprocessing:\n\n# Scale variables (x - mean)/sd\nland_3 &lt;- land_3 %&gt;% \n  mutate(zNDVI = scale(NDVI), zMean_G = scale(Mean_G)) \n\n# Scale new point using mean, sd of sample\nland_3 &lt;- land_3 %&gt;%\n  mutate(new_zNDVI = (0.335 - mean(NDVI))/sd(NDVI)) %&gt;% \n  mutate(new_zMean_G = (110 - mean(Mean_G))/sd(Mean_G))\n\nCalculate distance:\n\n# Euclidean Distance\nland_3 &lt;- land_3 %&gt;%\n  mutate(dist_to_new = sqrt((new_zNDVI - zNDVI)^2 + ( new_zMean_G - zMean_G)^2))\n\nFind neighbors:\n\n# K = 1\nland_3 %&gt;%\n  arrange(dist_to_new) %&gt;%\n  slice(1)\n\n    type NDVI Mean_G Bright_100 SD_NIR      zNDVI    zMean_G new_zNDVI\n1 grass  0.17 183.07     164.54  11.73 0.06245739 0.06245739  1.010834\n  new_zMean_G dist_to_new\n1  -0.9128659    1.360395\n\n# K = 3\nland_3  %&gt;%\n  arrange(dist_to_new) %&gt;%\n  slice(1:3) %&gt;% \n  count(type) %&gt;%\n  arrange(desc(n))\n\n    type n\n1 grass  2\n2  tree  1\n\n# K = 277\nland_3 %&gt;% \n  count(type) %&gt;%\n  arrange(desc(n))\n\n      type   n\n1   grass  112\n2    tree  106\n3 asphalt   59",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-5-tuning-the-knn-algorithm",
    "href": "L13-knn-trees.html#exercise-5-tuning-the-knn-algorithm",
    "title": "13  KNN and Trees",
    "section": "Exercise 5: Tuning the KNN algorithm",
    "text": "Exercise 5: Tuning the KNN algorithm\nThe KNN algorithm depends upon the tuning parameter \\(K\\), the number of neighbors we consider in our classifications.\nPlay around with the shiny app (code below) to build some intuition here.\nAfter, answer the following questions.\n\nIn general, how would you describe the classification regions defined by the KNN algorithm?\nLet’s explore the goldilocks problem here. When K is too small:\n\nthe classification regions are very ____\nthe model is (overfit or underfit?)\nthe model will have _____ bias and ____ variance\n\nWhen K is too big:\n\nthe classification regions are very ____\nthe model is (overfit or underfit?)\nthe model will have _____ bias and ____ variance\n\nWhat K value would you pick, based solely on the plots? (We’ll eventually tune the KNN using classification accuracy as a guide.)\n\n\n\nCode\n# Define KNN plot\nlibrary(gridExtra)\nlibrary(FNN)\nknnplot &lt;- function(x1, x2, y, k, lab_1, lab_2){\n x1 &lt;- (x1 - mean(x1)) / sd(x1)\n x2 &lt;- (x2 - mean(x2)) / sd(x2)\n x1s &lt;- seq(min(x1), max(x1), len = 100)\n x2s &lt;- seq(min(x2), max(x2), len = 100) \n testdata &lt;- expand.grid(x1s,x2s)\n knnmod &lt;- knn(train = data.frame(x1, x2), test = testdata, cl = y, k = k, prob = TRUE)\n testdata &lt;- testdata %&gt;% mutate(class = knnmod)\n g1 &lt;- ggplot(testdata, aes(x = Var1, y = Var2, color = class)) + \n     geom_point() + \n     labs(x = paste(lab_1), y = paste(lab_2), title = \"KNN classification regions\") + \n     theme_minimal() +\n     scale_color_manual(values = c(\"asphalt \" = \"black\",\n                            \"grass \" = \"#E69F00\",\n                            \"tree \" = \"#56B4E9\")) \n g2 &lt;- ggplot(NULL, aes(x = x1, y = x2, color = y)) + \n     geom_point() + \n     labs(x = paste(lab_1), y = paste(lab_2), title = \"Raw data\") + \n     theme_minimal() +\n     scale_color_manual(values = c(\"asphalt \" = \"black\",\n                            \"grass \" = \"#E69F00\",\n                            \"tree \" = \"#56B4E9\")) \n grid.arrange(g1, g2)\n}\n\n\n\n\nCode\nlibrary(shiny)\n# Build the shiny server\nserver_KNN &lt;- function(input, output) {\n  output$model_plot &lt;- renderPlot({\n    knnplot(x1 = land_3$Mean_G, x2 = land_3$NDVI, y = land_3$type, k = input$k_pick, lab_1 = \"Mean_G (standardized)\", lab_2 = \"NDVI (standardized)\")\n  })\n}\n\n# Build the shiny user interface\nui_KNN &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      h4(\"Pick K:\"), \n      sliderInput(\"k_pick\", \"K\", min = 1, max = 277, value = 1)\n    ),\n    mainPanel(\n      plotOutput(\"model_plot\")\n    )\n  )\n)\n\n\n# Run the shiny app!\nshinyApp(ui = ui_KNN, server = server_KNN)\n\n\n\n\nSolution:\n\n\n\ncool. flexible.\nWhen K is too small:\n\n\nthe classification regions are very small / wiggly / flexible\nthe model is overfit\nthe model will have low bias and high variance\n\n\nWhen K is too big:\n\n\nthe classification regions are very large / rigid / overly simple\nthe model is underfit\nthe model will have high bias and low variance",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#pause-to-reflect-knn-pros-cons",
    "href": "L13-knn-trees.html#pause-to-reflect-knn-pros-cons",
    "title": "13  KNN and Trees",
    "section": "PAUSE TO REFLECT: KNN pros & cons",
    "text": "PAUSE TO REFLECT: KNN pros & cons\nPros:\n\nflexible\nintuitive\ncan model y variables that have more than 2 categories\n\nCons:\n\nlazy learner (technical term!)\nIn logistic regression, we run the algorithm one time to get a formula that we can use to classify all future data points. In KNN, we have to re-calculate distances and identify neighbors, hence start the algorithm over, each time we want to classify a new data point.\ncomputationally expensive (given that we have to start over each time!)\nprovides classifications, but no real sense of the relationship of y with predictors x",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-6-intuition",
    "href": "L13-knn-trees.html#exercise-6-intuition",
    "title": "13  KNN and Trees",
    "section": "Exercise 6: Intuition",
    "text": "Exercise 6: Intuition\nClassification trees are another nonparametric algorithm.\nLet’s build intuition for trees here.\nAsk Kelsey for the paper handout.\nComplete it using only your intuition!",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-7-use-the-tree",
    "href": "L13-knn-trees.html#exercise-7-use-the-tree",
    "title": "13  KNN and Trees",
    "section": "Exercise 7: Use the tree",
    "text": "Exercise 7: Use the tree\nBuild the classification tree in R.\nWe’ll use this tree for prediction in this exercise, and dig into the tree details in the next exercise.\n\n# Build the tree in R\n# This is only demo code! It will change in the future.\nlibrary(rpart)\nlibrary(rpart.plot)\n\ndemo_model &lt;- rpart(type ~ NDVI + SD_NIR, land_3, maxdepth = 2)\nrpart.plot(demo_model) \n\n\n\n\n\n\n\n\nPredict the land type of images with the following properties:\n\nNDVI = 0.05 and SD_NIR = 7\nNDVI = -0.02 and SD_NIR = 7\n\n\n\nSolution:\n\n\ngrass\nasphalt",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-8-understand-the-tree",
    "href": "L13-knn-trees.html#exercise-8-understand-the-tree",
    "title": "13  KNN and Trees",
    "section": "Exercise 8: Understand the tree",
    "text": "Exercise 8: Understand the tree\nThe classification tree is like an upside down real world tree.\nThe root node at the top starts with all 277 sample images, i.e. 100% of the data.\nAmong the images in this node, 21% are asphalt, 40% are grass, and 38% are tree. Thus if we stopped our tree here, we’d classify any new image as “grass” since it’s the most common category.\nLet’s explore the terminal or leaf nodes at the bottom of the tree.\n\nWhat percent of images would be classified as “tree”?\nAmong images classified as tree:\n\nWhat percent are actually trees?\nWhat percent are actually grass (thus misclassified as trees)?\nWhat percent are actually asphalt (thus misclassified as trees)?\n\n\n\n\nSolution:\n\n\n49%\n.\n\n72%\n28%\n0%",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-9-tuning-trees-min_n",
    "href": "L13-knn-trees.html#exercise-9-tuning-trees-min_n",
    "title": "13  KNN and Trees",
    "section": "Exercise 9: tuning trees: min_n",
    "text": "Exercise 9: tuning trees: min_n\nIn the above tree, we only made 2 splits. But we can keep growing our tree! Run the shiny app below.\nKeep cost_complexity at 0 and change min_n. This tuning parameter controls the minimum size, or number of images, that can fall into any (terminal) leaf node.\n\nLet’s explore the goldilocks problem here. When min_n is too small (i.e. the tree is too big):\n\nthe classification regions are very ____\nthe model is (overfit or underfit?)\nthe model will have _____ bias and ____ variance\n\nWhen min_n is too big (i.e. the tree is too small):\n\nthe classification regions are very ____\nthe model is (overfit or underfit?)\nthe model will have _____ bias and ____ variance\n\nWhat min_n value would you pick, based solely on the plots?\nTuning classification trees is also referred to as “pruning”. Why does this make sense?\nCheck out the classification regions. In what way do these differ from the KNN classification regions? What feature of these regions reflects binary splitting?\n\n\n# Define tree plot functions\nlibrary(gridExtra)\ntree_plot &lt;- function(x1, x2, y, lab_1, lab_2, cp = 0, minbucket = 1){\n  model &lt;- rpart(y ~ x1 + x2, cp = cp, minbucket = minbucket)\n  x1s &lt;- seq(min(x1), max(x1), len = 100)\n  x2s &lt;- seq(min(x2), max(x2), len = 100) \n  testdata &lt;- expand.grid(x1s,x2s) %&gt;% \n    mutate(type = predict(model, newdata = data.frame(x1 = Var1, x2 = Var2), type = \"class\"))\n  g1 &lt;- ggplot(testdata, aes(x = Var1, y = Var2, color = type)) + \n    geom_point() + \n    labs(x = paste(lab_1), y = paste(lab_2), title = \"tree classification regions\")  + \n     theme_minimal() +\n     scale_color_manual(values = c(\"asphalt \" = \"black\",\n                            \"grass \" = \"#E69F00\",\n                            \"tree \" = \"#56B4E9\")) + \n    theme(legend.position = \"bottom\")\n  g2 &lt;- ggplot(NULL, aes(x = x1, y = x2, color = y)) + \n    geom_point() + \n    labs(x = paste(lab_1), y = paste(lab_2), title = \"Raw data\") + \n     theme_minimal() +\n     scale_color_manual(values = c(\"asphalt \" = \"black\",\n                            \"grass \" = \"#E69F00\",\n                            \"tree \" = \"#56B4E9\")) + \n    theme(legend.position = \"bottom\")\n  grid.arrange(g1, g2, ncol = 2)\n  \n}\n\ntree_plot_2 &lt;- function(cp, minbucket){\n  model &lt;- rpart(type ~ NDVI + SD_NIR, land_3, cp = cp, minbucket = minbucket)\n  rpart.plot(model,\n    box.palette = 0,\n    extra = 0\n  )  \n}\n\n\nlibrary(shiny)\n# Build the shiny server\nserver_tree &lt;- function(input, output) {\n  output$model_plot &lt;- renderPlot({\n    tree_plot(x1 = land_3$SD_NIR, x2 = land_3$NDVI, y = land_3$type, cp = input$cp_pick, lab_1 = \"SD_NIR\", lab_2 = \"NDVI\", minbucket = input$bucket)\n  })\n  output$trees &lt;- renderPlot({\n    tree_plot_2(cp = input$cp_pick, minbucket = input$bucket)\n  })\n}\n\n# Build the shiny user interface\nui_tree &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bucket\", \"min_n\", min = 1, max = 100, value = 1),\n      sliderInput(\"cp_pick\", \"cost_complexity\", min = 0, max = 0.36, value = -1)\n    ),\n    mainPanel(\n      plotOutput(\"model_plot\"),\n      plotOutput(\"trees\")\n    )\n  )\n)\n\n\n# Run the shiny app!\nshinyApp(ui = ui_tree, server = server_tree)\n\n\n\nSolution:\n\n\nWhen min_n is too small (i.e. the tree is too big):\n\nthe classification regions are very small / flexible\nthe model is overfit\nthe model will have low bias and high variance\n\nWhen min_n is too big (i.e. the tree is too small)\n\nthe classification regions are very big / rigid\nthe model is underfit\nthe model will have high bias and low variance\n\nwill vary\nlike pruning a real tree, tuning classification trees “lops off” branches\nthey are boxy",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-10-tuning-trees-cost-complexity-parameter",
    "href": "L13-knn-trees.html#exercise-10-tuning-trees-cost-complexity-parameter",
    "title": "13  KNN and Trees",
    "section": "Exercise 10: tuning trees: cost-complexity parameter",
    "text": "Exercise 10: tuning trees: cost-complexity parameter\nThere’s another tuning parameter to consider: cost_complexity! Like the LASSO \\(\\lambda\\) penalty parameter penalizes the inclusion of more predictors, cost_complexity penalizes the introduction of new splits. When cost_complexity is 0, there’s no penalty – we can make a split even if it doesn’t improve our classification accuracy. But the bigger (more positive) the cost_complexity, the greater the penalty – we can only make a split if the complexity it adds to the tree is offset by its improvement to the classification accuracy.\n\nLet’s explore the goldilocks problem here. When cost_complexity is too small (i.e. the tree is too big):\n\nthe classification regions are very ____\nthe model is (overfit or underfit?)\nthe model will have _____ bias and ____ variance\n\nWhen cost_complexity is too big (i.e. the tree is too small):\n\nthe classification regions are very ____\nthe model is (overfit or underfit?)\nthe model will have _____ bias and ____ variance\n\n\n\n\nSolution:\n\n\nWhen cost_complexity is too small (i.e. the tree is too big):\n\nthe classification regions are very small / flexible\nthe model is overfit\nthe model will have low bias and high variance\n\nWhen cost_complexity is too big (i.e. the tree is too small):\n\nthe classification regions are very big / rigid\nthe model is underfit\nthe model will have high bias and low variance",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#optional-cost-complexity-details",
    "href": "L13-knn-trees.html#optional-cost-complexity-details",
    "title": "13  KNN and Trees",
    "section": "OPTIONAL: cost complexity details",
    "text": "OPTIONAL: cost complexity details\nDefine some notation:\n\nGreek letter \\(\\alpha\\) (“alpha”) = cost complexity parameter\nT = number of terminal or leaf nodes in the tree\nR(T) = total misclassification rate corresponding to the tree with T leaf nodes\n\nThen our final tree is that which minimizes the combined misclassification rate and penalized number of nodes:\nR(T) + \\(\\alpha\\) T",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L13-knn-trees.html#exercise-11-tree-properties",
    "href": "L13-knn-trees.html#exercise-11-tree-properties",
    "title": "13  KNN and Trees",
    "section": "Exercise 11: tree properties",
    "text": "Exercise 11: tree properties\nNOTE: If you don’t get to this during class, no big deal. These ideas will also be in our next video!\n\nWe called the KNN algorithm lazy since we have to re-build the algorithm every time we want to make a new prediction / classification. Are classification trees lazy?\nWe called the backward stepwise algorithm greedy – it makes the best (local) decision in each step, but these might not end up being globally optimal. Mainly, once we kick out a predictor, we can’t bring it back in even if it would be useful later. Explain why classification trees are greedy.\nIn the KNN algorithm, it’s important to standardize our quantitative predictors to the same scale. Is this necessary for classification trees?\n\n\n\nSolution:\n\n\nNo. Once we finalize the tree, we can use it for all future classifications.\nThe splits are sequential. We can’t undo our first splits even if they don’t end up being optimal later.\nNope.",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L14-knn-trees-2.html",
    "href": "L14-knn-trees-2.html",
    "title": "14  More KNN and Trees",
    "section": "",
    "text": "Settling In\nComing soon!",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>More KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L14-knn-trees-2.html#part-1",
    "href": "L14-knn-trees-2.html#part-1",
    "title": "14  More KNN and Trees",
    "section": "Part 1",
    "text": "Part 1\nLet’s focus on trees.\nWe’ve explored some general ML themes already:\n\nmodel building: parametric vs nonparametric\nbenefits and drawbacks of using nonparametric trees vs parametric algorithm logistic regression\nalgorithm details:\n\nsteps of the tree algorithm\ntuning a tree algorithm\n\n\nIn the exercises, you’ll explore some other important themes:\n\nmodel building: variable selection\nWe have 147 potential predictors of land type. How can we choose which ones to use?\nmodel evaluation & comparison\nHow good are our trees? Which one should we pick?\nregression vs classification\nKNN works in both settings. So do trees!\n\n\nExercise 1: Build 10 trees\nIn modeling land type by all 147 possible predictors, our goal will be to build a tree that optimize the cost_complexity parameter. The chunk below builds 10 trees, each using a different cost_complexity value while fixing our other tuning parameters to be very liberal / not restrictive:\n\ntree_depth = 30 (the default)\nmin_n in each leaf node = 2 (the default is 20)\n\nReflect on the code and pause to answer any questions (Q).\n\n# STEP 1: tree specification\n# Q: WHAT IS NEW HERE?!\ntree_spec &lt;- decision_tree() %&gt;%\n  set_mode(\"classification\") %&gt;% \n  set_engine(engine = \"rpart\") %&gt;% \n  set_args(cost_complexity = tune(),  \n           min_n = 2, \n           tree_depth = NULL)\n\n\n# STEP 2: variable recipe\n# NOTHING IS NEW HERE & THERE ARE NO PREPROCESSING STEPS!\nvariable_recipe_big &lt;- recipe(type ~ ., data = land)\n    \n# STEP 3: tree workflow\n# NOTHING IS NEW HERE!\ntree_workflow_big &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe_big) %&gt;% \n  add_model(tree_spec)\n\n\n# STEP 4: Estimate 10 trees using a range of possible cost complexity values\n# cost_complexity is on the log10 scale (10^(-5) to 10^(0.1))\n# Q: BY WHAT CV METRIC ARE WE COMPARING THE TREES?\n# Q: WHY NOT USE CV MAE?\nset.seed(253)\ntree_models_big &lt;- tree_workflow_big %&gt;% \n  tune_grid(\n    grid = grid_regular(cost_complexity(range = c(-5, 0.1)), levels = 10),\n    resamples = vfold_cv(land, v = 10),\n    metrics = metric_set(accuracy)\n  )\n\n\n\n\n\n\nExercise 2: Whew!\nWe only built 10 trees above, and it took quite a bit of time. Why are trees computationally expensive?\n\n\nSolution:\n\nConsider just 1 tree. At every single split, we must evaluate and compare every possible split value of each of the 147 predictors. Building 10 trees and evaluating each using 10-fold CV means that we had to do this 100 times!\n\n\n\nExercise 3: Compare and finalize the tree\nJust as with our other algorithms with tuning parameters, we can use the CV metrics to compare the 10 trees, and pick which one we prefer.\nHere, we’ll pick the parsimonious tree (which also happens to be the tree with the largest CV accuracy!).\nRun & reflect upon the code below, then answer some follow-up questions.\n\n# Compare the CV metrics for the 10 trees\ntree_models_big %&gt;% \n  autoplot() + \n  scale_x_continuous()\n\n\n\n\n\n\n\n# Pick the parsimonious parameter\nparsimonious_param_big &lt;- tree_models_big %&gt;% \n  select_by_one_std_err(metric = \"accuracy\", desc(cost_complexity))\nparsimonious_param_big\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1         0.00681 Preprocessor1_Model06\n\n# Finalize the tree with parsimonious cost complexity\nbig_tree &lt;- tree_workflow_big %&gt;% \n  finalize_workflow(parameters = parsimonious_param_big) %&gt;% \n  fit(data = land)\n\n\nWhat is happening as the cost-complexity parameter \\(\\alpha\\) increases:\n\nthe tree is getting more complicated and the accuracy is improving\nthe tree is getting more complicated and the accuracy is getting worse\nthe tree is getting simpler and the accuracy is improving\nthe tree is getting simpler and the accuracy is getting worse\n\nWhat will our tree look like if we use a cost complexity parameter bigger than 0.4? As what category will this tree predict all images to be?\nCHALLENGE: For cost complexity parameters bigger than 0.4, the accuracy plateaus at roughly 18.1%. Where does this number come from?! NOTE: If you get stumped here, move on. We’ll come back to this later.\nYou don’t have to write anything out (this is largely review), but convince yourself that you could:\n\ninterpret the plot\ni.d. where our parsimonious tree falls on this plot\nexplain what “parsimonious” means\n\n\n\n\nSolution:\n\n\nthe tree is getting simpler and the accuracy is getting worse\na single root node with no splits, which classifies everything as building\nthis is the no information rate (which we’ll calculate below)\n\n\n\n\nExercise 4: Examine the tree\nLet’s examine the final, tuned big_tree which models land type by all 147 predictors:\n\n# This tree has a bunch of info\n# BUT: (1) it's tough to read; and (2) all branch lengths are the same (not proportional to their improvement)\nbig_tree %&gt;% \n  extract_fit_engine() %&gt;% \n  rpart.plot()\n\n\n\n\n\n\n\n# We can make it a little easier by playing around with\n# font size (cex) and removing some node details (type = 0)\nbig_tree %&gt;% \n  extract_fit_engine() %&gt;% \n  rpart.plot(cex = 0.8, type = 0)\n\n\n\n\n\n\n\n\n\n# This tree (1) is easier to read; and (2) plots branch lengths proportional to their improvement\n# But it has less info about classification accuracy\nbig_tree %&gt;% \n  extract_fit_engine() %&gt;%\n  plot()\nbig_tree %&gt;% \n  extract_fit_engine() %&gt;%\n  text(cex = 0.8)\n\n\n\n\n\n\n\n\nUse the second tree plot to answer the following questions.\n\nAre any land types not captured somewhere in the tree: asphalt, building, car, concrete, grass, pool, shadow, soil, tree? If so, why do you think this is?\nThis tree considered all 147 possible predictors. Do all of these appear in the final tree?\nOf the predictors used in the tree, which seem to be the most important?\n\n\n\nSolution:\n\n\nYes\nNo\nNDVI, Mean_G_40, Bright_100, ShpIndx_100 (roughly)\n\n\n\n\nExercise 5: Identifying useful predictors\nLuckily, we don’t have to guesstimate the importance of different predictors.\nThere’s a mathematical metric: variable importance.\nRoughly, a predictor’s importance measures the total improvement in node purity if we were to split on this predictor (even if the predictor isn’t ultimately used in a split).\nThe bigger the better!\nCheck out the importance of our 147 predictors.\n\n# Check out just the 10 most important (for simplicity)\nbig_tree %&gt;%\n  extract_fit_engine() %&gt;%\n  pluck(\"variable.importance\") %&gt;% \n  head(10)\n\n     NDVI_40         NDVI      NDVI_60       Mean_G    Bright_40    Mean_G_40 \n   140.81912    136.74372    128.66465    117.47329    110.15678     95.89161 \n     NDVI_80     NDVI_100     NDVI_120 Mean_NIR_100 \n    84.91799     76.13970     71.44863     70.28818 \n\n\nAnd plot these to help with comparison:\n\n# By default, this plots only 10 predictors\n# At most, it will plot only half of our predictors here\n# (I'm not sure what the max is in general!)\nlibrary(vip)\nbig_tree %&gt;% \n  vip(geom = \"point\", num_features = 147)\n\n\n\n\n\n\n\n\n\nWhat are the 3 most important predictors by this metric? Do these appear in our tree?\nWhy do you think a predictor can have high importance but not appear in the tree? Name 2 reasons.\nIf you could pick only 3 predictors to model land type, would you pick the 3 with the highest importance? Explain.\n\n\n\nSolution:\n\n\nNDVI_40, NDVI, NDVI_60. NDVI_60 is not in the tree.\nSince variable importance metrics look at predictor contributions even when they aren’t used in the splits, some of the important variables aren’t even used in our tree. This is explained by: (1) these predictors might be highly correlated / multicollinear with other predictors that do show up in the tree (i.e. we don’t need both); and (2) greediness – earlier splits will, in part, determine what predictors are used later in the tree.\nNo. Given what we observed above, variable importance doesn’t tell us about the combined importance of a set of predictors, but their individual importance.\n\n\n\n\nExercise 6: How good is the tree?!? Part 1\nStepping back, our goal for building this tree was to classify land type at for a pixel in an image.\nAs an example, suppose we have a pixel in an image like the first one in our data set:\n\nhead(land, 1)\n\n  type BrdIndx Area Round Bright Compact ShpIndx Mean_G Mean_R Mean_NIR  SD_G\n1 car     1.27   91  0.97 231.38    1.39    1.47 207.92 241.74   244.48 21.41\n  SD_R SD_NIR   LW GLCM1 Rect GLCM2 Dens Assym  NDVI BordLngth   GLCM3\n1 20.4  18.69 2.19  0.48 0.87  6.23  1.6  0.74 -0.08        56 4219.69\n  BrdIndx_40 Area_40 Round_40 Bright_40 Compact_40 ShpIndx_40 Mean_G_40\n1       1.33      97     1.12    227.19       1.32       1.42    203.95\n  Mean_R_40 Mean_NIR_40 SD_G_40 SD_R_40 SD_NIR_40 LW_40 GLCM1_40 Rect_40\n1    237.23      240.38   27.63   28.36     26.18     2      0.5    0.85\n  GLCM2_40 Dens_40 Assym_40 NDVI_40 BordLngth_40 GLCM3_40 BrdIndx_60 Area_60\n1     6.29    1.67      0.7   -0.08           56  3806.36       1.33      97\n  Round_60 Bright_60 Compact_60 ShpIndx_60 Mean_G_60 Mean_R_60 Mean_NIR_60\n1     1.12    227.19       1.32       1.42    203.95    237.23      240.38\n  SD_G_60 SD_R_60 SD_NIR_60 LW_60 GLCM1_60 Rect_60 GLCM2_60 Dens_60 Assym_60\n1   27.63   28.36     26.18     2      0.5    0.85     6.29    1.67      0.7\n  NDVI_60 BordLngth_60 GLCM3_60 BrdIndx_80 Area_80 Round_80 Bright_80\n1   -0.08           56  3806.36       1.33      97     1.12    227.19\n  Compact_80 ShpIndx_80 Mean_G_80 Mean_R_80 Mean_NIR_80 SD_G_80 SD_R_80\n1       1.32       1.42    203.95    237.23      240.38   27.63   28.36\n  SD_NIR_80 LW_80 GLCM1_80 Rect_80 GLCM2_80 Dens_80 Assym_80 NDVI_80\n1     26.18     2      0.5    0.85     6.29    1.67      0.7   -0.08\n  BordLngth_80 GLCM3_80 BrdIndx_100 Area_100 Round_100 Bright_100 Compact_100\n1           56  3806.36        1.33       97      1.12     227.19        1.32\n  ShpIndx_100 Mean_G_100 Mean_R_100 Mean_NIR_100 SD_G_100 SD_R_100 SD_NIR_100\n1        1.42     203.95     237.23       240.38    27.63    28.36      26.18\n  LW_100 GLCM1_100 Rect_100 GLCM2_100 Dens_100 Assym_100 NDVI_100 BordLngth_100\n1      2       0.5     0.85      6.29     1.67       0.7    -0.08            56\n  GLCM3_100 BrdIndx_120 Area_120 Round_120 Bright_120 Compact_120 ShpIndx_120\n1   3806.36        1.33       97      1.12     227.19        1.32        1.42\n  Mean_G_120 Mean_R_120 Mean_NIR_120 SD_G_120 SD_R_120 SD_NIR_120 LW_120\n1     203.95     237.23       240.38    27.63    28.36      26.18      2\n  GLCM1_120 Rect_120 GLCM2_120 Dens_120 Assym_120 NDVI_120 BordLngth_120\n1       0.5     0.85      6.29     1.67       0.7    -0.08            56\n  GLCM3_120 BrdIndx_140 Area_140 Round_140 Bright_140 Compact_140 ShpIndx_140\n1   3806.36        1.33       97      1.12     227.19        1.32        1.42\n  Mean_G_140 Mean_R_140 Mean_NIR_140 SD_G_140 SD_R_140 SD_NIR_140 LW_140\n1     203.95     237.23       240.38    27.63    28.36      26.18      2\n  GLCM1_140 Rect_140 GLCM2_140 Dens_140 Assym_140 NDVI_140 BordLngth_140\n1       0.5     0.85      6.29     1.67       0.7    -0.08            56\n  GLCM3_140\n1   3806.36\n\n\nOur tree correctly predicts that this is a car:\n\nbig_tree %&gt;% \n  predict(new_data = head(land, 1))\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 \"car \"     \n\n\nBut how good are the classifications overall?\nLet’s first consider the CV overall accuracy rate:\n\nparsimonious_param_big\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1         0.00681 Preprocessor1_Model06\n\n\n\nInterpret 0.804, the CV overall accuracy rate for the big_tree.\nCalculate & compare the tree’s CV overall accuracy rate to the no information rate: is the big_tree better than just always guessing the most common land type? You’ll need the following info:\n\n\nland %&gt;% \n  nrow()\n\n[1] 675\n\nland %&gt;% \n  count(type)\n\n       type   n\n1  asphalt   59\n2 building  122\n3      car   36\n4 concrete  116\n5    grass  112\n6     pool   29\n7   shadow   61\n8     soil   34\n9     tree  106\n\n\n\nWhy can’t we calculate, thus compare, the sensitivity & specificity of our tree? HINT: Think of how these are defined.\n\n\n\nSolution:\n\n\nWe estimate that our big_tree will correctly classify roughly 80% of new pixel in an image.\nThe tree’s is much better than the no information rate of 18% associated with always guessing “building”.\n\n\n122/675  \n\n[1] 0.1807407\n\n\n\nsensitivity & specificity measure the accuracy of classifications for binary outcomes y.\n\n\n\n\nExercise 7: How good is the tree?!? Part 2\nThe above CV metric gives us a sense of the overall quality of using our tree to classify new a pixel in an image.\nBut it doesn’t give any insight into the quality of classifications for any particular land type.\nTo that end, let’s consider the in-sample confusion matrix (i.e. how well our tree classified the pixels in an image in our sample).\nNOTE: We could also get a CV version, but the code is long and the CV accuracy will do for now!\n\nin_sample_confusion &lt;- big_tree %&gt;% \n  augment(new_data = land) %&gt;% \n  conf_mat(truth = type, estimate = .pred_class)\n\nin_sample_confusion\n\n           Truth\nPrediction  asphalt  building  car  concrete  grass  pool  shadow  soil  tree \n  asphalt         57         3    2         0      0     0       1     0     0\n  building         0       116    0        16      1     0       0     5     0\n  car              0         0   33         2      0     2       0     0     0\n  concrete         1         3    1        98      0     0       0     9     1\n  grass            0         0    0         0    102     1       0     3     9\n  pool             0         0    0         0      0    26       0     0     0\n  shadow           1         0    0         0      0     0      59     0     1\n  soil             0         0    0         0      0     0       0    17     0\n  tree             0         0    0         0      9     0       1     0    95\n\n# The mosaic plot of this matrix is too messy to be very useful here\nin_sample_confusion %&gt;% \n  autoplot() +    \n  aes(fill = rep(colnames(in_sample_confusion$table), ncol(in_sample_confusion$table))) + \n  scale_fill_manual(values = c(\"#000000\", \"darkgray\", \"red\", \"#7570B3\", \"lightgreen\", \"blue\", \"#E6AB02\", \"brown\", \"#66A61E\")) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nConfirm that 96.6% of asphalt pixels in an image were correctly classified as asphalt.\nWhat land type was the hardest for the tree to classify? Why might this be?\n\n\n\nSolution:\n\n\n57 / (57 + 1 + 1) = 0.966\nsoil. probably because it had the fewest data points.",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>More KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L14-knn-trees-2.html#part-2",
    "href": "L14-knn-trees-2.html#part-2",
    "title": "14  More KNN and Trees",
    "section": "Part 2",
    "text": "Part 2\nJust like KNN, trees can be applied in both regression and classification settings.\nThus trees add to our collection of nonparametric regression techniques, including KNN, LOESS, and GAM.\nTo explore, we’ll use regression trees to model the body_mass_g of penguins by their bill_length_mm and species. This is for demonstration purposes only!!!\nAs the plot below demonstrates, this relationship isn’t complicated enough to justify using a nonparametric algorithm:\n\ndata(penguins)\nggplot(penguins, aes(y = body_mass_g, x = bill_length_mm, color = species)) + \n  geom_point()\n\n\n\n\n\n\n\n\nRun the following code to build a regression tree of this relationship.\nPause to reflect upon the questions in the comments:\n\n# CHUNK GOAL\n# Build a bunch of trees using different cost complexity parameters\n\n# STEP 1: regression tree specification\n# QUESTION: How does this differ from our classification tree specification?\ntree_spec &lt;- decision_tree() %&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(engine = \"rpart\") %&gt;% \n  set_args(cost_complexity = tune(),  \n           min_n = 2, \n           tree_depth = 20)\n\n# STEP 2: variable recipe\n# NOTHING IS NEW HERE!\nvariable_recipe &lt;- recipe(body_mass_g ~ bill_length_mm + species, data = penguins)\n\n# STEP 3: tree workflow\n# NOTHING IS NEW HERE!\ntree_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(tree_spec)\n\n# STEP 4: Estimate multiple trees using a range of possible cost complexity values\n# QUESTION: How do the CV metrics differ from our classification tree?\nset.seed(253)\ntree_models &lt;- tree_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(cost_complexity(range = c(-5, -1)), levels = 10),\n    resamples = vfold_cv(penguins, v = 10),\n    metrics = metric_set(mae)\n  )\n\n\n# CHUNK GOAL:\n# Finalize the tree using the parsimonious cost complexity parameter\n# NOTHING IS NEW HERE\n\n# Identify the parsimonious cost complexityparameter\nparsimonious_param &lt;- tree_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(cost_complexity))\n\n# Finalize the tree with parsimonious cost complexity\nregression_tree &lt;- tree_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_param) %&gt;% \n  fit(data = penguins)\n\n\n\n\n\nExercise 8: Regression tree\nCheck out the resulting regression tree:\n\n# This code is the same as for classification trees!\nregression_tree %&gt;% \n  extract_fit_engine() %&gt;% \n  rpart.plot()\n\n\n\n\n\n\n\n\n\nUse your tree (by “hand”) to predict the body mass for the 2 following penguins:\n\n\nnew_penguins &lt;- data.frame(species = c(\"Adelie\", \"Gentoo\"), bill_length_mm = c(45, 45))\nnew_penguins\n\n  species bill_length_mm\n1  Adelie             45\n2  Gentoo             45\n\n\n\nCheck your work in part a:\n\n\nregression_tree %&gt;% \n  augment(new_data = new_penguins)\n\n# A tibble: 2 × 3\n  .pred species bill_length_mm\n  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 4192. Adelie              45\n2 4782. Gentoo              45\n\n\n\nRegression trees partition the data points into separate prediction regions. Check out this tree’s predictions (dark dots) and convince yourself that these are consistent with the tree:\n\n\nregression_tree %&gt;% \n  augment(new_data = penguins) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) + \n    geom_point(alpha = 0.35, size = 0.5) + \n    geom_point(aes(x = bill_length_mm, y = .pred, color = species), size = 1.5) +\n    facet_wrap(~ species) + \n    theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on what you’ve observed here, what do you think is a drawback of regression trees?\n\n\n\nSolution:\n\n\n4192 and 4782\nsame as a\n…\nat least in this example, the regression tree greatly oversimplifies the relationship of body mass with species and bill length\n\n\n\n\nExercise 9: Visual essay\nCheck out this visual essay on trees!\n\n\nExercise 10: Work on Homework\nWith your remaining time, work on either Homework 5 (logistic regression) or 6 (trees…and forests, which we’ll discuss next week).",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>More KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L14-knn-trees-2.html#r-code-knn",
    "href": "L14-knn-trees-2.html#r-code-knn",
    "title": "14  More KNN and Trees",
    "section": "R code: KNN",
    "text": "R code: KNN\nSuppose we want to build a KNN model of some categorical response variable y using predictors x1 and x2 in our sample_data.\n\n# Load packages\nlibrary(tidymodels)\nlibrary(kknn)\n\n# Resolves package conflicts by preferring tidymodels functions\ntidymodels_prefer()\n\n\n\n\nMake sure that y is a factor variable\n\nsample_data &lt;- sample_data %&gt;% \n  mutate(y = as.factor(y))\n\nBuild models for a variety of tuning parameters K\n\n# STEP 1: KNN model specification\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_mode(\"classification\") %&gt;% \n  set_engine(engine = \"kknn\") %&gt;% \n  set_args(neighbors = tune())\n\n# STEP 2: variable recipe\nvariable_recipe &lt;- recipe(y ~ x1 + x2, data = sample_data) %&gt;% \n  step_nzv(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n# STEP 3: KNN workflow\nknn_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(knn_spec)\n\n# STEP 4: Estimate multiple KNN models using a range of possible K values\nset.seed(___)\nknn_models &lt;- knn_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(neighbors(range = c(___, ___)), levels = ___),\n    resamples = vfold_cv(sample_data, v = ___),\n    metrics = metric_set(accuracy)\n  )\n\n\n\n\nTuning K\n\n# Calculate CV accuracy for each KNN model\nknn_models %&gt;% \n  collect_metrics()\n\n# Plot CV accuracy (y-axis) for the KNN model from each K (x-axis)\nknn_models %&gt;% \n  autoplot()\n\n# Identify K which produced the highest (best) CV accuracy\nbest_K &lt;- select_best(knn_models, metric = \"accuracy\")\nbest_K\n\n# Get the CV accuracy for KNN when using best_K\nknn_models %&gt;% \n  collect_metrics() %&gt;% \n  filter(neighbors == best_K$neighbors)\n\n\n\n\nFinalizing the “best” KNN model\n\nfinal_knn_model &lt;- knn_workflow %&gt;% \n  finalize_workflow(parameters = best_k) %&gt;% \n  fit(data = sample_data)\n\n\n\n\nUse the KNN to make predictions / classifications\n\n# Put in a data.frame object with x1 and x2 values (at minimum)\nfinal_knn_model %&gt;% \n  predict(new_data = ___)",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>More KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L14-knn-trees-2.html#r-code-trees",
    "href": "L14-knn-trees-2.html#r-code-trees",
    "title": "14  More KNN and Trees",
    "section": "R code: trees",
    "text": "R code: trees\nSuppose we want to build a tree of some categorical response variable y using predictors x1 and x2 in our sample_data.\n\n# Load packages\nlibrary(tidymodels)\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Resolves package conflicts by preferring tidymodels functions\ntidymodels_prefer()\n\n\n\n\nMake sure that y is a factor variable\n\nsample_data &lt;- sample_data %&gt;% \n  mutate(y = as.factor(y))\n\n\n\n\nBuild trees for a variety of tuning parameters\nWe’ll focus on optimizing the cost_complexity parameter, while setting min_n and tree_depth to fixed numbers. For example, setting min_n to 2 and tree_depth to 30 set only loose restrictions, letting cost_complexity do the pruning work.\n\n# STEP 1: tree specification\n# If y is quantitative, change \"classification\" to \"regression\"\ntree_spec &lt;- decision_tree() %&gt;%\n  set_mode(\"classification\") %&gt;% \n  set_engine(engine = \"rpart\") %&gt;% \n  set_args(cost_complexity = tune(),  \n           min_n = 2, \n           tree_depth = 30)\n\n# STEP 2: variable recipe\n# There are no necessary preprocessing steps for trees!\nvariable_recipe &lt;- recipe(y ~ x1 + x2, data = sample_data)\n\n# STEP 3: tree workflow\ntree_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(tree_spec)\n\n# STEP 4: Estimate multiple trees using a range of possible cost complexity values\n# - If y is quantitative, change \"accuracy\" to \"mae\"\n# - cost_complexity is on the log10 scale (10^(-5) to 10^(0.1))\n#   I start with a range from -5 to 2 and then tweak\nset.seed(___)\ntree_models &lt;- tree_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(cost_complexity(range = c(___, ___)), levels = ___),\n    resamples = vfold_cv(sample_data, v = ___),\n    metrics = metric_set(accuracy)\n  )\n\n\n\n\nTuning cost complexity\n\n# Plot the CV accuracy vs cost complexity for our trees\n# x-axis is on the original (not log10) scale\ntree_models %&gt;% \n  autoplot() + \n  scale_x_continuous()\n\n# Identify cost complexity which produced the highest CV accuracy\nbest_cost &lt;- tree_models %&gt;% \n  select_best(metric = \"accuracy\")\n\n# Get the CV accuracy when using best_cost\ntree_models %&gt;% \n  collect_metrics() %&gt;% \n  filter(cost_complexity == best_cost$cost_complexity)\n\n# Identify cost complexity which produced the parsimonious tree\nparsimonious_cost &lt;- tree_models %&gt;% \n  select_by_one_std_err(metric = \"accuracy\", desc(cost_complexity))\n\n\n\n\nFinalizing the tree\n\n# Plug in best_cost or parsimonious_cost\nfinal_tree &lt;- tree_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = sample_data)\n\n\n\n\nPlot the tree\n\n# Tree with accuracy info in each node\n# Branches are NOT proportional to classification improvement\nfinal_tree %&gt;% \n  extract_fit_engine() %&gt;% \n  rpart.plot()\n\n# Tree withOUT accuracy info in each node\n# Branches ARE proportional to classification improvement\nfinal_tree %&gt;% \n  extract_fit_engine() %&gt;% \n  plot()\nfinal_tree %&gt;% \n  extract_fit_engine() %&gt;% \n  text()\n\n\n\n\nUse the tree to make predictions / classifications\n\n# Put in a data.frame object with x1 and x2 values (at minimum)\nfinal_tree %&gt;% \n  predict(new_data = ___)  \n\n# OR \nfinal_tree %&gt;% \n  augment(new_data = ___)\n\n\n\n\nExamine variable importance\n\n# Print the metrics\nfinal_tree %&gt;%\n  extract_fit_engine() %&gt;%\n  pluck(\"variable.importance\")\n\n# Plot the metrics\n# Plug in the number of top predictors you wish to plot\n# (The upper limit varies by application!)\nlibrary(vip)\nfinal_tree %&gt;% \n  vip(geom = \"point\", num_features = ___)\n\n\n\n\nEvaluate the classifications using in-sample metrics\n\n# Get the in-sample confusion matrix\nin_sample_confusion &lt;- final_tree %&gt;% \n  augment(new_data = sample_data) %&gt;% \n  conf_mat(truth = type, estimate = .pred_class)\nin_sample_confusion\n\n# Plot the matrix using a mosaic plot\n# See exercise for what to do when there are more categories than colors in our pallette!\nin_sample_confusion %&gt;% \n  autoplot() +    \n  aes(fill = rep(colnames(in_sample_confusion$table), ncol(in_sample_confusion$table))) + \n  theme(legend.position = \"none\")",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>More KNN and Trees</span>"
    ]
  },
  {
    "objectID": "L15-forests.html",
    "href": "L15-forests.html",
    "title": "15  Random forests & bagging",
    "section": "",
    "text": "Settling In\nComing soon!",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Random forests & bagging</span>"
    ]
  },
  {
    "objectID": "L15-forests.html#footnotes",
    "href": "L15-forests.html#footnotes",
    "title": "15  Random forests & bagging",
    "section": "",
    "text": "citation: https://daviddalpiaz.github.io/r4sl/ensemble-methods.html#tree-versus-ensemble-boundaries↩︎",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Random forests & bagging</span>"
    ]
  },
  {
    "objectID": "L16-review.html",
    "href": "L16-review.html",
    "title": "16  Unit 4 - 5 Reflection",
    "section": "",
    "text": "Warning\n\n\n\nThis page is currently under construction. Check back soon for updates!\n\n\n\nSettling In\nComing soon!\n\n\nReview & Reflection\nSTAT 253 is a survey course of statistical machine learning techniques and concepts. It’s important to continuously reflect on these and how they fit together. Though you won’t hand anything in, or work on this in class today, you’re strongly encouraged to complete this activity. This material is designed to help you reflect upon:\n\nML concepts\n\nenduring, big picture concepts\ntechnical concepts\ntidymodels code\n\n\nFollow the links below and make a copy of the STAT 253 concept maps (or find and modify the copy you made while reviewing the regression unit).\nYou’ll be given some relevant prompts below, but you should use these materials in whatever way suits you! Take notes, add more content, rearrange, etc.\n\nSTAT 253 concept maps\ntidymodels code comparison\n\n\n\nSTAT 253 concept maps\nReview slides 3–5 (classification) of the concept map, and mark up slides 1, 7, and 8 with respect to the prompts below.\n\n\n\nEnduring, big picture concepts\nIMPORTANT to your learning: Respond in your own words.\n\nWhen do we perform a supervised vs unsupervised learning algorithm?\nWithin supervised learning, when do we use a regression vs a classification algorithm?\nWhat is the importance of “model evaluation” and what questions does it address?\nWhat is “overfitting” and why is it bad?\nWhat is “cross-validation” and what problem is it trying to address?\nWhat is the “bias-variance tradeoff”?\n\n\n\n\nTechnical concepts\nOn page 7, identify some general themes for each model algorithm listed in the lefthand table:\n\nWhat’s the goal?\nIs the algorithm parametric or nonparametric?\nDoes the algorithm have any tuning parameters? What are they, how do we tune them, and how is this a goldilocks problem?\nWhat are the key pros & cons of the algorithm?\n\nFor each algorithm, you should also reflect upon the important technical concepts listed in the syllabus:\n\nCan you summarize the steps of this algorithm?\nIs the algorithm parametric or nonparametric? (addressed above)\nWhat is the bias-variance tradeoff when working with or tuning this algorithm?\nIs it important to scale / pre-process our predictors before feeding them into this algorithm?\nIs this algorithm “computationally expensive”?\nCan you interpret the technical (RStudio) output for this algorithm? (eg: CV plots, etc)?\n\nAnd some details:\n\nIf this algorithm is parametric, could you:\n\ninterpret its coefficients?\ncalculate / predict the probability of different y outcomes from these coefficients?\ncome up with a classification rule for a given probability cut-off?\n\nIf this algorithm is non-parametric:\n\nCould you implement the alghorithm “by hand” for a small sample of data points?\n\nIf this algorithm is a tree-based method:\n\nCould you explain the difference between in-sample, OOB, and CV metrics?\n\n\nAnd what about narrowing down to important predictors?\n\nWhat tools do we have to give us a sense of important predictors?\n\nbinary outcome?\nmulticlass outcome?\n\n\n\n\n\nModel evaluation\nOn page 7, the righthand table lists some model evaluation metrics for binary classification algorithms. Do the following:\n\nDefine each metric. THINK: Could you calculate these metrics if given a confusion matrix?\nExplain the steps of the CV algorithm.\n\n\n\n\nAlgorithm comparisons\nUse page 8 to make other observations about the Unit 4-5 modeling algorithms and their connections.\n\n\nWrapping Up\nComing soon!",
    "crumbs": [
      "Classification: Building Flexible Models (Unit 5)",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Unit 4 - 5 Reflection</span>"
    ]
  },
  {
    "objectID": "r_rstudio.html",
    "href": "r_rstudio.html",
    "title": "R and RStudio Setup",
    "section": "",
    "text": "Troubleshooting\nHere’s how to fix it:",
    "crumbs": [
      "Appendices",
      "R and RStudio Setup"
    ]
  },
  {
    "objectID": "r_rstudio.html#troubleshooting",
    "href": "r_rstudio.html#troubleshooting",
    "title": "R and RStudio Setup",
    "section": "",
    "text": "Problem: You are on a Mac and getting the following error (or something similar):\n\n\n    Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n     there is no package called ‘rlang’\n\n\nFirst install the suite of Command Line Tools for Mac using the instructions here.\nNext enter install.packages(\"rlang\") in the Console.\nFinally check that entering library(ggplot2) gives no errors.",
    "crumbs": [
      "Appendices",
      "R and RStudio Setup"
    ]
  },
  {
    "objectID": "r_resources.html",
    "href": "r_resources.html",
    "title": "R Resources",
    "section": "",
    "text": "Tidymodels resources",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#tidymodels-resources",
    "href": "r_resources.html#tidymodels-resources",
    "title": "R Resources",
    "section": "",
    "text": "Tidymodels package documentation\nTidy Modeling with R textbook (Max Kuhn and Julia Silge)\nISLR Labs with Tidymodels (Emil Hvitfeldt)\nIntro to Tidymodels Presentation (Lucy D’Agostino McGowan)",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#tidyverse-resources",
    "href": "r_resources.html#tidyverse-resources",
    "title": "R Resources",
    "section": "Tidyverse resources",
    "text": "Tidyverse resources\n\nBrianna Heggeseth’s COMP/STAT 112 website (with code examples and videos)\nR for Data Science\nExploratory Data Analysis with R\nJohn’s Hopkins Tidyverse course text",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#visualization-resources",
    "href": "r_resources.html#visualization-resources",
    "title": "R Resources",
    "section": "Visualization resources",
    "text": "Visualization resources\n\nggplot2 reference\nColors in R",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#general-r-resources",
    "href": "r_resources.html#general-r-resources",
    "title": "R Resources",
    "section": "General R resources",
    "text": "General R resources\n\nRStudio cheatsheets\nAdvanced R\nR Programming Wikibook\nDebugging in R\n\nArticle\nVideo",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "r_resources.html#some-example-code",
    "href": "r_resources.html#some-example-code",
    "title": "R Resources",
    "section": "Some example code",
    "text": "Some example code\nCreating new variables\ncase_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables.\n\n# Turn quant_var into a Low/Med/High version\ndata &lt;- data %&gt;%\n    mutate(cat_var = case_when(\n            quant_var &lt; 10 ~ \"Low\",\n            quant_var &gt;= 10 & quant_var &lt;= 20 ~ \"Med\",\n            quant_var &gt; 20 ~ \"High\"\n        )\n    )\n\n# Turn cat_var (A, B, C categories) into another categorical variable\n# (collapse A and B into one category)\ndata &lt;- data %&gt;%\n    mutate(new_cat_var = case_when(\n            cat_var %in% c(\"A\", \"B\") ~ \"A or B\"\n            cat_var==\"C\" ~ \"C\"\n        )\n    )\n\n# Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable\n# Doing this for multiple variables allows you to create an index\ndata &lt;- data %&gt;%\n    mutate(x1_score = case_when(\n            x1==0 ~ 10,\n            x1==1 ~ 20,\n            x1==2 ~ 50\n        )\n    )\n\n# Add together multiple variables with mutate\ndata &lt;- data %&gt;%\n    mutate(index = x1_score + x2_score + x3_score)",
    "crumbs": [
      "Appendices",
      "R Resources"
    ]
  },
  {
    "objectID": "stat155.html",
    "href": "stat155.html",
    "title": "STAT 155 Review",
    "section": "",
    "text": "COMPREHENSIVE REVIEW\nA comprehensive STAT 155 review is provided by the Prof. Johnson’s Spring 2022 STAT 155 manual here and the STAT 155 Notes created by Profs. Grinde, Heggeseth, and Myint here.\n\n\n\nQUICK REVIEW\nLet \\(y\\) be a response variable with a set of \\(k\\) explanatory variables \\(x = (x_{1}, x_{2}, ..., x_{k})\\). Then the population linear regression model is\n\\[\\begin{split}\ny & = f(x) + \\varepsilon  = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k} + \\varepsilon \\\\\n\\end{split}\\]\nNOTES:\n\n\\(\\beta\\) is the Greek letter “beta”. \\(\\varepsilon\\) is the Greek letter “epsilon”.\n\n“Linear” regression is so named because it assumes that \\(y\\) is a linear combination of the \\(x\\)’s. It does not mean that the relationship itself is linear!! For example, one of the predictors might be a quadratic term: \\(x_2 = x_1^2\\).\n\\(f(x) = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_k x_{k}\\) captures the trend of the relationship\n\n\\(\\beta_0\\) = intercept coefficient\nthe model value when \\(x_1=x_2=\\cdots=x_k=0\\)\n\\(\\beta_i\\) = \\(x_i\\) coefficient\nhow \\(x_i\\) is related to \\(y\\) when holding constant all other \\(x_i\\)\n\n\\(\\epsilon\\) reflects deviation from the trend (the residual)\n\n\n\n\n\nFitting the Model\nOnce we have a population model in mind, we can “fit the model” (i.e. estimate the \\(\\beta\\) population coefficients) using sample data:\n\\[\\begin{split}\ny & =  \\hat{f}(x) + \\varepsilon \\\\\n& = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1} + \\hat{\\beta}_2 x_{2} + \\cdots + \\hat{\\beta}_k x_{k} + \\varepsilon \\\\\n\\end{split}\\]\n\n\nTo this end, collect a sample of data on \\(n\\) subjects. Use subscripts to denote the data for subject \\(i\\): \\(y_i\\) and \\(x_{ij}\\). Then the predicted response and residual (prediction error) for subject \\(i\\) are\n\nprediction \\[\\hat{y}_i = \\hat{f}(x_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + \\cdots + \\hat{\\beta}_k x_{ik}\\]\nresidual / prediction error \\[y_i - \\hat{y}_i\\]\n\n\n\n\nLeast Squares Criterion\nEstimate (\\(\\beta_0, \\beta_1,..., \\beta_k\\)) by (\\(\\hat{\\beta}_0, \\hat{\\beta}_1,..., \\hat{\\beta}_k)\\) that minimize the sum of squared residuals: \\[\\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = (y_1-\\hat{y}_1)^2 + (y_2-\\hat{y}_2)^2 + \\cdots + (y_n-\\hat{y}_n)^2\\]",
    "crumbs": [
      "Appendices",
      "STAT 155 Review"
    ]
  }
]