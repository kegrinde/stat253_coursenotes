---
title: "LASSO: Shrinkage/Regularization"
logo: "images/mac.png"
---

```{r 06_setup}
#| include: false
#| cache: false
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
library(conflicted)
conflicts_prefer(tidyr::expand)
conflicts_prefer(dplyr::filter)
```



# Settling In {.unnumbered .smaller}

- Sit with the same group as last class. 
  - Last day with these groups!
  - Fill out the Group 1 Feedback Survey on Moodle to help me pick new ones.
- Prepare to take notes.
  - [Locate](schedule.html), download, save, and open the QMD for today's class
  - NOTE: I moved the R code notes to the *end* of the QMD, but kept the 
  `eval = FALSE` chunks. If you don't like these, you can do a find-and-replace 
  to move them, but you won't be able to render your document right away.
- See the #announcements channel on [Slack](https://macstat253.slack.com) for upcoming events.
- Check out this Minnesota Public Radio (MPR) interview on [Can AI replace your doctor?](https://www.mprnews.org/episode/2023/09/19/can-ai-replace-your-doctor)! It's a discussion of AI / machine learning in medicine. NOTE: ML is a subset of AI. (image from Wiki)

![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/AI-ML-DL.svg/440px-AI-ML-DL.svg.png){width="300px"}

- Now that we're on MPR, journalist [David Montgomery](http://dhmontgomery.com/portfolio/) used R for data analysis and visualizations using this [custom ggtheme](https://github.com/dhmontgomery/personal-work/tree/master/theme-mpr) to make visuals for MPR. To create a custom ggtheme, check out <https://themockup.blog/posts/2020-12-26-creating-and-using-custom-ggplot2-themes/>. 
- Interested in the intersection between statistics/data science and journalism? Consider applying for [this internship](https://recruiting2.ultipro.com/STA1013/JobBoard/94aec289-5757-a8f0-d3bb-77f9cd846172/OpportunityDetail?opportunityId=bd90fff9-acf2-4e04-b8b6-497965cde9a7) with the Star Tribune! (**Deadline: Friday, November 1**)





# Learning Goals {.unnumbered .smaller}

- Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection
- Explain why variable scaling is important for the performance of shrinkage methods
- Explain how the lambda tuning parameter affects model performance and how this is related to overfitting
- Describe how output from LASSO models can give a measure of *variable importance*

<!-- next time: add question that shows lasso not greedy. ex: ankle is in parsimonious model, but not "best" bigger model -->


<!-- DETAILS FOR INSTRUCTOR: -->

<!-- LASSO drawback: when large p / small n, (1) LASSO selects at most n predictors, (2) tends to only pick 1 of a group of correlated predictors -->
<!-- Including the quadratic ridge penalty produces a unique minimum -->


<!-- - Why does LASSO set coef to 0?  The idea is that the LASSO penalty constrains coefficients to be within a multidimensional diamond that has a large number of corners (at which coefficients are set to 0) whereas the ridge penalty constrains coefficients to be within a smooth boundary. -->












# Notes: LASSO {-}


## Context {.unnumbered .smaller}

<center>
<img src="images/MLDiagram2.jpg"/>
</center>

- **world = supervised learning**       
    We want to model some output variable $y$ using a set of *potential* predictors ($x_1, x_2, ..., x_p$).

- **task = regression**       
    $y$ is quantitative

- **model = linear regression**       
    We'll assume that the relationship between $y$ and ($x_1, x_2, ..., x_p$) can be represented by
    
    $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \varepsilon$$


- [**estimation algorithm = LASSO**]{style="background-color:lightgray;"} (instead of least squares)

<br><br>


## Least Absolute Shrinkage and Selection Operator {.unnumbered .smaller}

**LASSO:** **L**east **A**bsolute **S**hrinkage and **S**election **O**perator

::: incremental

Dates back to 1996, proposed by Robert Tibshirani (one of the authors of ISLR)

> Robert Tibshirani, Regression Shrinkage and Selection Via the Lasso, Journal of the Royal Statistical Society: Series B (Methodological), Volume 58, Issue 1, January 1996, Pages 267â€“288, [https://doi.org/10.1111/j.2517-6161.1996.tb02080.x](https://doi.org/10.1111/j.2517-6161.1996.tb02080.x)

:::


## Goal {.unnumbered .smaller}

**GOAL: Model Selection**

Use the LASSO algorithm to help us *regularize* and *select* the "best" predictors $x$ to use in a **predictive** linear regression model of $y$:

$$y = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \cdots + \hat{\beta}_p x_p + \varepsilon$$



## Idea  {.unnumbered .smaller}

- *Penalize* a predictor for adding complexity to the model (by penalizing its coefficient). 
- Track whether the predictor's *contribution* to the model (lowering RSS) is enough to offset this penalty. 


## Algorithm Criterion  {.unnumbered .smaller}

Identify the model coefficients $\hat{\beta}_1, \hat{\beta}_2, ...  \hat{\beta}_p$ that *minimize* the **penalized residual sum of squares**:

$$RSS + \lambda \sum_{j=1}^p \vert \hat{\beta}_j\vert = \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p \vert \hat{\beta}_j\vert$$

where 

- residual sum of squares (RSS) measures the overall model prediction error
- the penalty term measures the overall size of the model coefficients
- $\lambda \ge 0$ ("lambda") is a tuning parameter        





# Small Group Discussion {-}

Discuss basic understanding from the video to help each other clear up concepts.


## Questions {.unnumbered .smaller}

**1: LASSO vs other algorithms for building linear regression models**


a. LASSO vs least squares       
    - What's one advantage of LASSO vs least squares?
    - Which algorithm(s) require us (or R) to *scale* the predictors?

b. What is one advantage of LASSO vs backward stepwise selection? 


<details>
<summary>Solution</summary>
a. LASSO helps with model selection, i.e. kicks some predictors out of the model, and preventing overfitting.
b. LASSO isn't greedy and doesn't overestimate the significance of the predictors it retains (its variable selection isn't based on p-values).
</details>
<br>




**2: LASSO tuning**

We have to pick a $\lambda$ penalty tuning parameter for our LASSO model.
What's the impact of $\lambda$?

a. When $\lambda$ is 0, ...
b. As $\lambda$ increases, the predictor coefficients ....
c. **Goldilocks problem**:
    If $\lambda$ is too big, .... 
    If $\lambda$ is too small, ...
d. To decide between a LASSO that uses $\lambda = 0.01$ vs $\lambda = 0.1$ (for example), we can ....



<details>
<summary>Solution</summary>

a. LASSO is equivalent to least squares.
b. shrink toward or to 0.
c. too big: all predictors are kicked out of the model. too small: too few predictors are kicked out, hence the model is complicated and maybe overfit.
d. compare CV MAE of the LASSOs with these $\lambda$
</details>
<br>




**COMMENT: Picking $\lambda$**


We *cannot* know the "best" value for $\lambda$ in advance.
This varies from analysis to analysis.

We must try a reasonable *range* of possible values for $\lambda$.
This also varies from analysis to analysis.

In general, we have to use *trial-and-error* to identify a range that is...

- wide enough that it doesn't miss the best values for $\lambda$
- narrow enough that it focuses on reasonable values for $\lambda$ 






# Exercises {-}

## Instructions {.unnumbered .smaller}

- Open the QMD fo today and scroll down to the *Exercises*
- Work on implementing LASSO to familiar data 
- Become familiar with the new code structures: 
  - instead of `fit_resamples` to run CV, we'll use `tune_grid` 
  to tune the algorithm with CV
  - new engine: `set_engine('glmnet')` 
- Ask me questions as I move around the room.

## Questions {.unnumbered .smaller}

We'll use the LASSO algorithm to help us build a good *predictive* model of `height` using the collection of 12 possible predictors in the `humans` dataset: 

```{r 06-data-setup}
#| eval: false
#| echo: false
# download from here: https://bcheggeseth.github.io/253_spring_2024/data/bodyfat2.csv
# or here: https://ajohns24.github.io/data/bodyfat1.csv
# how were these created??

tmp1 <- read.csv('/Users/kgrinde/Documents/GitHub/STAT253/data/ajohns_bodyfat1.csv')
tmp2 <- read.csv('/Users/kgrinde/Documents/GitHub/STAT253/data/bheggeseth_bodyfat2.csv')

dim(tmp1)
dim(tmp2)

View(tmp1)
View(tmp2)

identical(tmp1, tmp2)

write.csv(tmp1, "data/bodyfat2.csv", row.names = FALSE)
```

